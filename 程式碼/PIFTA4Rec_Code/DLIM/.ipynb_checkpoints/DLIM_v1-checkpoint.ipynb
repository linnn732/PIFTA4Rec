{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9b3a52-cede-4c5c-afc7-b69485b43fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gzip\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d413ef8-39fd-46ce-a396-07e1d1fa77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Dunnhumby\" # \"Tafeng\" or \"Dunnhumby\"\n",
    "k = 30\n",
    "\n",
    "# 隨資料集調整\n",
    "batch_size = 32  # Tafeng = 64 / Dunnhumby = 32\n",
    "learning_rate = 0.00001  # Tafeng = 0.0001 / Dunnhumby = 0.00001\n",
    "vector_size = 3005  # Tafeng = 12087 / Dunnhumby = 3005\n",
    "num_products = 3005  # Tafeng = 12087 / Dunnhumby = 3005\n",
    "\n",
    "#固定參數設置\n",
    "epochs = 80\n",
    "embed_dim = 64\n",
    "ffn_hidden_dim = 256\n",
    "decay_rate = 0.3\n",
    "dropout_rate = 0.3\n",
    "num_heads = 4\n",
    "num_trans_layers = 1\n",
    "max_seq_length = 75\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8bc901-7d94-4a3c-9a96-871065d64822",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f'data/preprocessed_data/{dataset}_training_answer.gz', 'rb') as f:\n",
    "    training_answers = pickle.load(f)\n",
    "\n",
    "with gzip.open(f'data/preprocessed_data/{dataset}_validation_answer.gz', 'rb') as f:\n",
    "    validation_answers = pickle.load(f)\n",
    "\n",
    "with gzip.open(f'data/preprocessed_data/{dataset}_test_answer.gz', 'rb') as f:\n",
    "    test_answers = pickle.load(f)\n",
    "\n",
    "true_training_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in training_answers}\n",
    "true_validation_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in validation_answers }\n",
    "true_test_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in test_answers }\n",
    "\n",
    "training_embedding_file = f'data/{dataset}/basketembedding/training_basketembedding_{embed_dim}.pkl.gz'\n",
    "training_neighbors_file = f'data/{dataset}/training_neighbors_for_dlim.json.gz'\n",
    "\n",
    "validation_embedding_file = f'data/{dataset}/basketembedding/validation_basketembedding_{embed_dim}.pkl.gz'\n",
    "validation_neighbors_file = f'data/{dataset}/validation_neighbors_for_dlim.json.gz'\n",
    "\n",
    "test_embedding_file = f'data/{dataset}/basketembedding/test_basketembedding_{embed_dim}.pkl.gz'\n",
    "test_neighbors_file = f'data/{dataset}/test_neighbors_for_dlim.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d72e703-b22a-48df-a03b-c7e309e1ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketDataset(Dataset):\n",
    "    # 接收訓練集的嵌入向量文件路徑、鄰居信息文件路徑、真實購物籃字典以及最大序列長度作為參數\n",
    "    def __init__(self, training_embedding_file, training_neighbors_file, true_training_basket_dict,max_seq_length=max_seq_length):\n",
    "        with gzip.open(training_embedding_file, 'rb') as f:\n",
    "            self.basket_embeddings = pickle.load(f)\n",
    "        with gzip.open(training_neighbors_file, 'rb') as f:\n",
    "            self.neighbors = json.load(f)\n",
    "        self.true_training_basket_dict = true_training_basket_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    # 返回數據集中的樣本數量\n",
    "    def __len__(self):\n",
    "        return len(self.neighbors)\n",
    "\n",
    "    # 計算與最晚日期的差值，並返回這些差值的列表\n",
    "    def calculate_relative_dates(self, transaction_dates):\n",
    "        #dates = [np.datetime64(date) for date in transaction_dates] # Tafeng 要跑這行\n",
    "        dates = [np.datetime64(f\"{str(date)[:4]}-{str(date)[4:6]}-{str(date)[6:]}\") for date in transaction_dates] # Dunnhumby 要跑這行\n",
    "        max_date = max(dates) + np.timedelta64(1, 'D')\n",
    "        relative_dates = [(max_date - date).astype(int) for date in dates]\n",
    "        return relative_dates\n",
    "\n",
    "\n",
    "    # 按索引獲取數據集中的單個樣本\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, neighbors_ids = self.neighbors[idx]\n",
    "\n",
    "        # 獲取指定用戶的購物籃嵌入向量和交易日期，並計算相對日期\n",
    "        user_data = self.basket_embeddings.get(user_id, [])\n",
    "        user_embeddings = [torch.tensor(embedding[0]) for embedding in user_data]\n",
    "        user_dates = [embedding[1] for embedding in user_data]\n",
    "        user_dates = self.calculate_relative_dates(user_dates)\n",
    "\n",
    "        # 初始化用戶的嵌入向量和交易日期的填充張量。如果用戶的購物籃數據少於最大序列長度，則使用零和 -1 進行填充\n",
    "        user_embeddings_padded = torch.zeros((self.max_seq_length, len(user_embeddings[0])))\n",
    "        user_dates_padded = torch.full((self.max_seq_length,), -1, dtype=torch.int64)  # 使用 -1 填充日期\n",
    "\n",
    "        # pad_sequence 是 PyTorch 中的一個函數，用於將一系列長度不一的序列填充到相同的長度。\n",
    "        if user_embeddings:\n",
    "            user_embeddings_tensor = pad_sequence(user_embeddings, batch_first=True) # 將不同長度的用戶購物籃嵌入向量填充到相同的長度，生成一個統一的張量 \n",
    "            user_dates_tensor = torch.tensor(user_dates, dtype=torch.int64)\n",
    "            user_seq_len = min(self.max_seq_length, len(user_dates)) # 實際要使用的序列長度\n",
    "\n",
    "            user_embeddings_padded[:user_seq_len, :] = user_embeddings_tensor[:user_seq_len, :]\n",
    "            user_dates_padded[:user_seq_len] = user_dates_tensor[:user_seq_len]\n",
    "\n",
    "        # 初始化邻居嵌入向量和交易日期的填充列表\n",
    "        neighbor_embeddings_padded = torch.zeros((300, self.max_seq_length, len(user_embeddings[0])))\n",
    "        neighbor_dates_padded = torch.full((300, self.max_seq_length), -1, dtype=torch.int64)  # 使用 -1 填充日期\n",
    "\n",
    "        # 填充邻居的购物篮嵌入向量和交易日期\n",
    "        for i, neighbor_id in enumerate(neighbors_ids):\n",
    "            n_data = self.basket_embeddings.get(neighbor_id, []) # 獲取該鄰居的購物籃數據。如果找不到對應的數據，則返回一個空列表。\n",
    "\n",
    "            # 分別從鄰居的購物籃數據中提取嵌入向量和交易日期\n",
    "            n_embeddings = [torch.tensor(embedding[0]) for embedding in n_data]\n",
    "            n_dates = [embedding[1] for embedding in n_data]\n",
    "            n_dates = self.calculate_relative_dates(n_dates)\n",
    "            \n",
    "            if n_embeddings:\n",
    "                n_embeddings_tensor = pad_sequence(n_embeddings, batch_first=True)\n",
    "                n_dates_tensor = torch.tensor(n_dates, dtype=torch.int64)\n",
    "                seq_len = min(self.max_seq_length, len(n_dates))\n",
    "\n",
    "                neighbor_embeddings_padded[i, :seq_len, :] = n_embeddings_tensor[:seq_len, :]\n",
    "                neighbor_dates_padded[i, :seq_len] = n_dates_tensor[:seq_len]\n",
    "\n",
    "        true_basket_vector = self.true_training_basket_dict.get(user_id, torch.zeros(vector_size))\n",
    "        return user_embeddings_padded, user_dates_padded, neighbor_embeddings_padded, neighbor_dates_padded, true_basket_vector\n",
    "\n",
    "def create_dataloader(embedding_file, neighbors_file, batch_size, true_basket_dict):\n",
    "    dataset = BasketDataset(embedding_file, neighbors_file, true_basket_dict, max_seq_length=max_seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c68beb8-46da-4fbc-89f1-55f8fd15b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, decay_rate, embedding_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.decay_rate = nn.Parameter(torch.tensor(decay_rate))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, basket_sequence, transaction_dates):\n",
    "        mask = (transaction_dates != -1).float()\n",
    "        decay_weights = torch.exp(-self.decay_rate * transaction_dates)\n",
    "        decay_weights = decay_weights * mask\n",
    "        decay_weights_sum = decay_weights.sum(1, keepdim=True)\n",
    "        normalized_weights = decay_weights / decay_weights_sum\n",
    "        user_embedding = torch.sum(normalized_weights.unsqueeze(-1) * basket_sequence, dim=1)\n",
    "        return user_embedding\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ffn_hidden_dim, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ffn_hidden_dim, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src):\n",
    "        attn_output, _ = self.multihead_attn(src, src, src)\n",
    "        src = self.layer_norm1(src + attn_output)\n",
    "        ffn_output = self.feed_forward(src)\n",
    "        src = self.layer_norm2(src + ffn_output)\n",
    "        return src\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_hidden_dim, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, ffn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(ffn_hidden_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ffn = self.fc2(F.relu(self.fc1(x)))\n",
    "        x = self.layer_norm(x + self.dropout(x_ffn))\n",
    "        return x\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d14ec07-6f3b-46e5-96b2-1d02932e6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, decay_rate, ffn_hidden_dim, num_products, dropout_rate, num_trans_layers=num_trans_layers):\n",
    "        super(RecommendationModel, self).__init__()\n",
    "        self.temporal_attention = TemporalAttention(decay_rate, embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(embedding_dim, num_heads, ffn_hidden_dim, dropout_rate) for _ in range(num_trans_layers)\n",
    "        ])\n",
    "        self.mlp = MLPLayer(embedding_dim, ffn_hidden_dim, num_products)\n",
    "\n",
    "    def forward(self, user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates):\n",
    "        user_embedding = self.temporal_attention(user_basket_sequence, user_transaction_dates)\n",
    "        neighbor_embeddings = torch.stack([     \n",
    "            self.temporal_attention(neighbor_seq, neighbor_dates)\n",
    "            for neighbor_seq, neighbor_dates in zip(neighbor_basket_sequence, neighbor_transaction_dates)\n",
    "        ]).transpose(0, 1)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            neighbor_embeddings = layer(neighbor_embeddings)\n",
    "\n",
    "        neighbor_embedding = neighbor_embeddings[-1]\n",
    "        combined_embedding = user_embedding + neighbor_embedding\n",
    "        output = self.mlp(combined_embedding.squeeze(0))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9eed558-c4f1-4b37-9832-5997e24a86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topk_metrics(predictions, targets, k):\n",
    "    # 将模型输出转换为 top-k 二值向量\n",
    "    _, top_indices = torch.topk(predictions, k, dim=1)\n",
    "    topk_binary_vector = torch.zeros_like(predictions)\n",
    "    topk_binary_vector.scatter_(1, top_indices, 1)\n",
    "\n",
    "    # 计算 true positives, false positives, false negatives\n",
    "    true_positives = torch.sum(topk_binary_vector * targets, dim=1)\n",
    "    false_positives = torch.sum(topk_binary_vector * (1 - targets), dim=1)\n",
    "    false_negatives = torch.sum((1 - topk_binary_vector) * targets, dim=1)\n",
    "\n",
    "    # 计算指标\n",
    "    recall = torch.mean(true_positives / (true_positives + false_negatives))\n",
    "    precision = torch.mean(true_positives / (true_positives + false_positives))\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # 计算 Hit Ratio (HR)\n",
    "    hr = torch.mean((true_positives > 0).float())\n",
    "\n",
    "    return recall.item(), precision.item(), f1.item(), hr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb77500-22c7-4cc0-bfc4-6ff0941467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(predictions, targets, k):\n",
    "\n",
    "    # 获取 top-k 预测项的索引\n",
    "    _, top_indices = torch.topk(predictions, k, dim=1)\n",
    "    \n",
    "    # 生成 DCG 分数\n",
    "    dcg = 0.0\n",
    "    for i in range(1, k + 1):\n",
    "        dcg += ((2 ** targets.gather(1, top_indices[:, i - 1].view(-1, 1)) - 1) / torch.log2(torch.tensor(i + 1).float())).squeeze()\n",
    "\n",
    "    # 生成理想的 DCG 分数 (IDCG)\n",
    "    _, ideal_indices = torch.topk(targets, k, dim=1)\n",
    "    idcg = 0.0\n",
    "    for i in range(1, k + 1):\n",
    "        idcg += ((2 ** targets.gather(1, ideal_indices[:, i - 1].view(-1, 1)) - 1) / torch.log2(torch.tensor(i + 1).float())).squeeze()\n",
    "\n",
    "    # 处理 IDCG 为 0 的情况，防止除以零\n",
    "    idcg[idcg == 0] = 1.0\n",
    "\n",
    "    # 计算 NDCG\n",
    "    ndcg = torch.mean(dcg / idcg)\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b0c48d-2a79-4e65-b45b-b1cc651f34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = create_dataloader(training_embedding_file, training_neighbors_file, batch_size, true_training_basket_dict)\n",
    "validation_loader = create_dataloader(validation_embedding_file, validation_neighbors_file, batch_size, true_validation_basket_dict)\n",
    "test_loader = create_dataloader(test_embedding_file, test_neighbors_file, batch_size, true_test_basket_dict)\n",
    "\n",
    "# 實例化模型\n",
    "recommendation_model =  RecommendationModel(embedding_dim=embed_dim, num_heads=num_heads, decay_rate=decay_rate, ffn_hidden_dim=ffn_hidden_dim, num_products=num_products, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(recommendation_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a496698-32b2-4c36-b388-d6cb8ca10851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(recommendation_model, validation_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k):\n",
    "\n",
    "    recommendation_model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'recall': 0.0, 'precision': 0.0, 'f1': 0.0, 'hr': 0.0, 'ndcg': 0.0}\n",
    "\n",
    "    for batch in validation_loader:\n",
    "        user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates, true_basket_vector = batch\n",
    "        user_embeddings = user_embeddings.to(device)\n",
    "        user_transaction_dates = user_transaction_dates.to(device)\n",
    "        neighbor_embeddings = [ne.to(device) for ne in neighbor_embeddings]\n",
    "        neighbor_transaction_dates = [nt.to(device) for nt in neighbor_transaction_dates]\n",
    "        true_basket_vector = true_basket_vector.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predicted_scores = recommendation_model(user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates)\n",
    "            #normalized_ans = predicted_scores / torch.sum(predicted_scores)\n",
    "\n",
    "            loss = loss_function(predicted_scores, true_basket_vector.float())\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            recall, precision, f1, hr = calculate_topk_metrics(predicted_scores, true_basket_vector, k)\n",
    "            ndcg = ndcg_score(predicted_scores, true_basket_vector, k)\n",
    "\n",
    "            val_metrics['recall'] += recall\n",
    "            val_metrics['precision'] += precision\n",
    "            val_metrics['f1'] += f1\n",
    "            val_metrics['hr'] += hr\n",
    "            val_metrics['ndcg'] += ndcg\n",
    "\n",
    "    avg_loss = val_loss / len(validation_loader)\n",
    "    avg_metrics = {k: val_metrics[k] / len(validation_loader) for k in val_metrics}\n",
    "    \n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7efa28-9144-4af1-b1dc-2dced6bc5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(recommendation_model, test_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k):\n",
    "\n",
    "    recommendation_model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_metrics = {'recall': 0.0, 'precision': 0.0, 'f1': 0.0, 'hr': 0.0, 'ndcg': 0.0}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates, true_basket_vector = batch\n",
    "        user_embeddings = user_embeddings.to(device)\n",
    "        user_transaction_dates = user_transaction_dates.to(device)\n",
    "        neighbor_embeddings = [ne.to(device) for ne in neighbor_embeddings]\n",
    "        neighbor_transaction_dates = [nt.to(device) for nt in neighbor_transaction_dates]\n",
    "        true_basket_vector = true_basket_vector.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predicted_scores = recommendation_model(user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates)\n",
    "            #normalized_ans = predicted_scores / torch.sum(predicted_scores)\n",
    "            \n",
    "            loss = loss_function(predicted_scores, true_basket_vector.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            recall, precision, f1, hr = calculate_topk_metrics(predicted_scores, true_basket_vector, k)\n",
    "            ndcg = ndcg_score(predicted_scores, true_basket_vector, k)\n",
    "        \n",
    "            test_metrics['recall'] += recall\n",
    "            test_metrics['precision'] += precision\n",
    "            test_metrics['f1'] += f1\n",
    "            test_metrics['hr'] += hr\n",
    "            test_metrics['ndcg'] += ndcg\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    avg_metrics = {k: test_metrics[k] / len(test_loader) for k in test_metrics}\n",
    "\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543addd0-e411-4de0-ad8d-7aeba21d8105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 Loss: 0.0019078120640817397: 100%|██████████| 289/289 [13:24<00:00,  2.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5503 | Recall: 0.0051 | Precision: 0.0019 | F1 Score: nan | NDCG: 0.0031 | HR: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 Loss: 0.0009909687776466555: 100%|██████████| 289/289 [13:23<00:00,  2.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2896 | Recall: 0.0066 | Precision: 0.0021 | F1 Score: nan | NDCG: 0.0036 | HR: 0.0578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 Loss: 0.00045248051415677716: 100%|██████████| 289/289 [13:21<00:00,  2.77s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1271 | Recall: 0.0068 | Precision: 0.0021 | F1 Score: nan | NDCG: 0.0037 | HR: 0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 Loss: 0.0002226193522499507: 100%|██████████| 289/289 [13:21<00:00,  2.78s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0628 | Recall: 0.0380 | Precision: 0.0093 | F1 Score: nan | NDCG: 0.0330 | HR: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 Loss: 0.00015028443664415485: 100%|██████████| 289/289 [13:21<00:00,  2.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0379 | Recall: 0.0767 | Precision: 0.0176 | F1 Score: nan | NDCG: 0.0841 | HR: 0.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80 Loss: 0.00010542623750272506: 100%|██████████| 289/289 [13:25<00:00,  2.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0274 | Recall: 0.0959 | Precision: 0.0237 | F1 Score: 0.0374 | NDCG: 0.1078 | HR: 0.4631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 Loss: 8.575282475321351e-05: 100%|██████████| 289/289 [13:23<00:00,  2.78s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0229 | Recall: 0.1165 | Precision: 0.0299 | F1 Score: 0.0465 | NDCG: 0.1198 | HR: 0.5152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80 Loss: 6.510553217676684e-05: 100%|██████████| 289/289 [13:25<00:00,  2.79s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0195 | Recall: 0.1374 | Precision: 0.0344 | F1 Score: 0.0544 | NDCG: 0.1356 | HR: 0.5303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80 Loss: 4.839368976626842e-05: 100%|██████████| 289/289 [13:20<00:00,  2.77s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0180 | Recall: 0.1572 | Precision: 0.0392 | F1 Score: 0.0623 | NDCG: 0.1404 | HR: 0.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 Loss: 5.898452573375306e-05: 100%|██████████| 289/289 [13:21<00:00,  2.77s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0173 | Recall: 0.1683 | Precision: 0.0424 | F1 Score: 0.0672 | NDCG: 0.1494 | HR: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80 Loss: 6.653604365137622e-05: 100%|██████████| 289/289 [13:21<00:00,  2.77s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0167 | Recall: 0.1810 | Precision: 0.0439 | F1 Score: 0.0695 | NDCG: 0.1610 | HR: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 Loss: 6.682421225783735e-05: 100%|██████████| 289/289 [13:21<00:00,  2.77s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0166 | Recall: 0.1612 | Precision: 0.0418 | F1 Score: 0.0658 | NDCG: 0.1434 | HR: 0.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 Loss: 5.606653420157911e-05: 100%|██████████| 289/289 [13:22<00:00,  2.78s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0162 | Recall: 0.1664 | Precision: 0.0434 | F1 Score: 0.0684 | NDCG: 0.1465 | HR: 0.5720\n",
      "Early stopping due to no improvement in validation NDCG.\n",
      "Test Loss: 0.0168 | Recall: 0.1634 | Precision: 0.0433 | F1 Score: 0.0678 | NDCG: 0.1408 | HR: 0.5819\n"
     ]
    }
   ],
   "source": [
    "best_val_ndcg = -float('inf') \n",
    "patience = 2\n",
    "no_improvement_count = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    recommendation_model.train()\n",
    "    training_progress_bar = tqdm(training_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "    \n",
    "    for batch in training_progress_bar:\n",
    "        \n",
    "        user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates, true_basket_vector = batch\n",
    "        user_embeddings = user_embeddings.to(device)\n",
    "        user_transaction_dates = user_transaction_dates.to(device)\n",
    "        neighbor_embeddings = [ne.to(device) for ne in neighbor_embeddings]\n",
    "        neighbor_transaction_dates = [nt.to(device) for nt in neighbor_transaction_dates]\n",
    "        true_basket_vector = true_basket_vector.to(device)\n",
    "\n",
    "        \n",
    "        predicted_scores = recommendation_model(user_embeddings, user_transaction_dates, neighbor_embeddings, neighbor_transaction_dates)\n",
    "        #normalized_ans = predicted_scores / torch.sum(predicted_scores)\n",
    "\n",
    "        loss = loss_function(predicted_scores, true_basket_vector.float())  # 确保 answer_vector 是 float 类型\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_progress_bar.set_description(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item() / len(training_loader)}\")\n",
    "\n",
    "     # 在每个epoch结束后进行验证\n",
    "    val_loss, val_metrics = validate_model(\n",
    "        recommendation_model, validation_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k)\n",
    "    \n",
    "    tqdm.write(f'Validation Loss: {val_loss:.4f} | Recall: {val_metrics[\"recall\"]:.4f} | Precision: {val_metrics[\"precision\"]:.4f} | F1 Score: {val_metrics[\"f1\"]:.4f} | NDCG: {val_metrics[\"ndcg\"]:.4f} | HR: {val_metrics[\"hr\"]:.4f}')\n",
    "\n",
    "    if val_metrics['ndcg'] > best_val_ndcg:\n",
    "        best_val_ndcg = val_metrics['ndcg']\n",
    "        no_improvement_count = 0\n",
    "        best_model_state = {\n",
    "            'recommendation_model': recommendation_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # 如果没有改进的计数达到了 patience，则停止训练\n",
    "    if no_improvement_count >= patience:\n",
    "        print(\"Early stopping due to no improvement in validation NDCG.\")\n",
    "        break\n",
    "\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, 'DLIM_Best_model.pth')\n",
    "\n",
    "# 加载最佳模型状态\n",
    "best_model_state = torch.load('DLIM_Best_model.pth')\n",
    "recommendation_model.load_state_dict(best_model_state['recommendation_model'])\n",
    "optimizer.load_state_dict(best_model_state['optimizer'])\n",
    "\n",
    "# 在所有训练循环结束后调用测试函数\n",
    "test_loss, test_metrics = test_model(\n",
    "    recommendation_model, test_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k)\n",
    "tqdm.write(f'Test Loss: {test_loss:.4f} | Recall: {test_metrics[\"recall\"]:.4f} | Precision: {test_metrics[\"precision\"]:.4f} | F1 Score: {test_metrics[\"f1\"]:.4f} | NDCG: {test_metrics[\"ndcg\"]:.4f} | HR: {test_metrics[\"hr\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pifta",
   "language": "python",
   "name": "pifta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
