{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj0gS7tPsLgF"
   },
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w0UCsEAFpstp"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "import gzip\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGUATq4sRb7"
   },
   "source": [
    "## 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W4U-qVi-rvPQ"
   },
   "outputs": [],
   "source": [
    "activate_codes_num = -1\n",
    "next_k_step = 1\n",
    "training_chunk = 0\n",
    "test_chunk = 1\n",
    "\n",
    "num_nearest_neighbors = 300\n",
    "within_decay_rate = 0.9\n",
    "group_decay_rate = 0.7\n",
    "group_size = 7\n",
    "topk = 10\n",
    "\n",
    "dataset = \"Dunnhumby\" # \"TaFeng\" or \"Dunnhumby\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z05PNhc1yOjU"
   },
   "source": [
    "## 生成字典與計算最終向量維度\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7vjhHs4gqmty"
   },
   "outputs": [],
   "source": [
    "def generate_dictionary_BA(files, attributes_list):\n",
    "    dictionary_table = {}\n",
    "    counter_table = {}\n",
    "\n",
    "    # attributes_list 僅有 MATERIAL_NUMBER\n",
    "    for attr in attributes_list:\n",
    "        dictionary = {}\n",
    "        dictionary_table[attr] = dictionary\n",
    "        counter_table[attr] = 0\n",
    "\n",
    "    csv.field_size_limit(128) # 設置 CSV 讀取的最大長度\n",
    "\n",
    "    for filename in files:\n",
    "        count = 0 #用於追蹤當前文件中處理的行數\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|') # 指定逗號（,）為字段分隔符，並將豎線（|）作為引號字符\n",
    "            for row in reader:\n",
    "                if count == 0: # 跳過第一行（標題行）:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                key = attributes_list[0] # 取出第一個屬性，即 MATERIAL_NUMBER\n",
    "                if row[3] not in dictionary_table[key]: # 如果 row[3] 的值還沒有在字典中，則將其添加到字典中，並賦予一個索引\n",
    "                    dictionary_table[key][row[3]] = counter_table[key]\n",
    "                    counter_table[key] = counter_table[key] + 1 # 更新 counter_table 中對應屬性的計數\n",
    "                    count += 1\n",
    "\n",
    "    total = 0\n",
    "    for key in counter_table.keys():\n",
    "        total = total + counter_table[key]\n",
    "\n",
    "    print('# dimensions of final vector: ' + str(total) + ' | '+str(count-1))\n",
    "\n",
    "    return dictionary_table, total, counter_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJbI4awNyeQr"
   },
   "source": [
    "## 從 CSV 文件中讀取數據並處理，將其轉換為特定格式的數據結構\n",
    "\n",
    "**目的：在歷史與未來中，處理每位用戶的所有購物籃，並於前後加入 -1 標記表示開始與結束**\n",
    "\n",
    "```\n",
    "# data_chunk[0][\"2\"] :[[-1], array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,67]), ... array([ 52, 106, 142, 143, 144, 145, 146, 147]), [-1]]\n",
    "# data_chunk[1][\"2\"]:[[-1], array([   5,    9,   50,  125,  145, 1357]), [-1]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vZm66eV2qgTU"
   },
   "outputs": [],
   "source": [
    "# 從 CSV 文件中讀取數據並處理，將其轉換為特定格式的數據結構\n",
    "def read_claim2vector_embedding_file_no_vector(files): #\n",
    "    attributes_list = ['MATERIAL_NUMBER'] # 一個包含要處理的屬性名稱的列表(存了商品編號）\n",
    "    print('----- Start Dictionary Generation -----')\n",
    "    dictionary_table, num_dim, counter_table = generate_dictionary_BA(files, attributes_list) # 生成字典並返回三個值 #files是./data/TaFang_history_NB.csv 跟 ./data/TaFang_future_NB.csv\n",
    "    print('----- Finish Dictionary Generation -----')\n",
    "    usr_attr = 'CUSTOMER_ID'\n",
    "    ord_attr = 'ORDER_NUMBER'\n",
    "    freq_max = 200\n",
    "    data_chunk = [] # 用於儲存處理後數據的列表\n",
    "    day_gap_counter = []\n",
    "    claims_counter = 0\n",
    "    num_claim = 0\n",
    "\n",
    "    # 迭代處理 files 列表中的每個文件，先重新命名 history 之後再 future\n",
    "    for file_id in range(len(files)):\n",
    "        count = 0\n",
    "        data_chunk.append({})\n",
    "        filename = files[file_id] # 打開文件\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile) # 讀取 CSV 文件\n",
    "\n",
    "            # 用於在下面的循環中跟踪上一行的數據\n",
    "            last_pid_date = '*'\n",
    "            last_pid = '-1'\n",
    "            last_days = -1\n",
    "            # 2 more elements in the end for start and end states\n",
    "            feature_vector = []\n",
    "\n",
    "            # 對 CSV 文件中的每一行進行迭代\n",
    "            for row in reader:\n",
    "                cur_pid_date = row[usr_attr] + '_' + row[ord_attr] # 從當前行生成一個唯一的標識符 (1_14)\n",
    "                cur_pid = row[usr_attr] # 並獲取用戶 ID (1)\n",
    "\n",
    "                # 如果當前用戶 ID 不等於上一個用戶 ID，則在 data_chunk 中為這個新用戶創建一個新條目\n",
    "                if cur_pid != last_pid:\n",
    "                    # start state\n",
    "                    tmp = [-1]\n",
    "                    data_chunk[file_id][cur_pid] = []\n",
    "                    data_chunk[file_id][cur_pid].append(tmp)\n",
    "\n",
    "                # 如果當前的 cur_pid_date 不等於 last_pid_date，則對 feature_vector 進行排序並將其添加到 data_chunk 中的相應位置，然後重置 feature_vector\n",
    "                if cur_pid_date not in last_pid_date:\n",
    "                    if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                        sorted_feature_vector = np.sort(feature_vector)\n",
    "                        data_chunk[file_id][last_pid].append(sorted_feature_vector)\n",
    "                        if len(sorted_feature_vector) > 0:\n",
    "                            count = count + 1\n",
    "                    feature_vector = []\n",
    "\n",
    "                    claims_counter = 0\n",
    "                # 如果當前用戶 ID 不等於上一個用戶 ID，則在 data_chunk 中的上一個用戶 ID 下添加一個結束標記\n",
    "                if cur_pid != last_pid:\n",
    "                    # end state\n",
    "                    if last_pid not in '-1':\n",
    "\n",
    "                        tmp = [-1]\n",
    "                        data_chunk[file_id][last_pid].append(tmp)\n",
    "                        #print(data_chunk[file_id][last_pid])\n",
    "\n",
    "                key = attributes_list[0]\n",
    "                within_idx = dictionary_table[key][row[key]]\n",
    "                previous_idx = 0\n",
    "\n",
    "                for j in range(attributes_list.index(key)):\n",
    "                    previous_idx = previous_idx + counter_table[attributes_list[j]]\n",
    "                idx = within_idx + previous_idx\n",
    "\n",
    "                if idx not in feature_vector:\n",
    "                    feature_vector.append(idx)\n",
    "\n",
    "                # 更新 last_pid_date 和 last_pid 值\n",
    "                last_pid_date = cur_pid_date\n",
    "                last_pid = cur_pid\n",
    "                #last_days = cur_days\n",
    "                if file_id == 1:\n",
    "                    claims_counter = claims_counter + 1\n",
    "\n",
    "            # 在循環結束後，將最後一個 feature_vector 添加到 data_chunk 中\n",
    "            if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                data_chunk[file_id][last_pid].append(np.sort(feature_vector))\n",
    "\n",
    "    # #儲存 data_chunk 資料用於查看\n",
    "    # def save_top_20_to_txt(data_chunk, file_index, file_name):\n",
    "    #   with open(file_name, 'w') as f:\n",
    "    #       for key, value in list(data_chunk[file_index].items())[:20]:\n",
    "    #           f.write(f\"{key}: {value}\\n\")\n",
    "    # # save_top_20_to_txt(data_chunk, 0, 'data/data_chunk_0_top20.txt')\n",
    "    # save_top_20_to_txt(data_chunk, 1, 'data/data_chunk_1_top20.txt')\n",
    "\n",
    "    # 正確答案\n",
    "    #np.savez('data/answer.npz', **data_chunk[1])\n",
    "    #print(\"data_chunk[1]:\", data_chunk[1])\n",
    "    print(\"num_dim+2 :\", num_dim + 2)\n",
    "    return data_chunk, num_dim + 2 # 返回處理後的數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FjWQg0papS-"
   },
   "source": [
    "## 分割資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PGH6BvjBZ3q6"
   },
   "outputs": [],
   "source": [
    "def partition_the_data_validate(data_chunk, key_set, next_k_step):\n",
    "    # key_set = ['1', '2', '3', '4', '5', ...'13972'] （即 Future 中所有用戶的 ID）\n",
    "    # next_k_step = 1\n",
    "    print('----- Start Splitting Data Set -----')\n",
    "\n",
    "    filtered_key_set = [] # 用於儲存經過篩選後的 key\n",
    "    past_chunk = 0\n",
    "    future_chunk = 1\n",
    "\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[past_chunk][key]) <= 3:\n",
    "            continue\n",
    "        if len(data_chunk[future_chunk][key]) < 2 + next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set)*0.9)]\n",
    "    validation_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)*0.9):int(4 / 5 * len(filtered_key_set))]\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    print('Number of validation instances: ' + str(len(validation_key_set)))\n",
    "    print('Number of test instances: ' + str(len(test_key_set)))\n",
    "\n",
    "    # print('Training Key Set: ',training_key_set)\n",
    "\n",
    "    print(\"training_key_set[0]: \", training_key_set[0])\n",
    "    print(\"validation_key_set[0]: \",validation_key_set[0])\n",
    "    print(\"test_key_set[0]: \",test_key_set[0])\n",
    "    print('----- Finish splitting the data set -----')\n",
    "\n",
    "    return training_key_set, validation_key_set, test_key_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr8i_8flEH0_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjLqYlpEeX94"
   },
   "source": [
    "## 分群計算，獲得所有群組的平均向量\n",
    "\n",
    "**將一系列的歷史數據向量按給定的群組大小分組，並計算每個群組中的向量的平均值**\n",
    "\n",
    "假設 his_list 長度為 10，group_size 為 3，\\\n",
    "est_num_vec_each_block = 10 / 3 ≈ 3.33（每個群組中平均應有大約 3.33 個向量）\\\n",
    "base_num_vec_each_block = 3.33 向下取整 = 3（每個群組至少應有 3 個向量）\\\n",
    "residual = 3.33 - 3 = 0.33 （差異）\\\n",
    "num_vec_has_extra_vec = 1（有 1 個群組將比其他群組多一個向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QFdfwk0beT9u"
   },
   "outputs": [],
   "source": [
    "def group_history_list(his_list,group_size):\n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        return grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "        return grouped_vec_list, group_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beTNBmbjA7lY"
   },
   "source": [
    "## 整合所有群組的平均向量，獲得用戶歷史向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IB0bsR8qCBvh"
   },
   "outputs": [],
   "source": [
    "def temporal_decay_sum_history(data_set, key_set, output_size,group_size,within_decay_rate,group_decay_rate):\n",
    "\n",
    "    print('Temporal Decay Sum History ...')\n",
    "\n",
    "    sum_history = {}\n",
    "\n",
    "    for key in key_set:\n",
    "\n",
    "        vec_list = data_set[key]\n",
    "        num_vec = len(vec_list) - 2\n",
    "        his_list = []\n",
    "\n",
    "        for idx in range(1,num_vec+1):\n",
    "            his_vec = np.zeros(output_size)\n",
    "            decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "            for ele in vec_list[idx]:\n",
    "                his_vec[ele] = decayed_val\n",
    "            his_list.append(his_vec) # 包含了用戶所有交易的時間衰減向量\n",
    "\n",
    "        grouped_list,real_group_size = group_history_list(his_list,group_size)\n",
    "        his_vec = np.zeros(output_size)\n",
    "        for idx in range(real_group_size):\n",
    "            decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "            if idx>=len(grouped_list):\n",
    "                print( 'idx: '+ str(idx))\n",
    "                print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "            his_vec += grouped_list[idx]*decayed_val\n",
    "        sum_history[key] = his_vec/real_group_size\n",
    "    return sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y1Lq6_NrRVyR"
   },
   "outputs": [],
   "source": [
    "# data_chunk、測試集用戶向量、測試集、訓練集用戶向量、訓練集、鄰居索引\n",
    "\n",
    "def test_get_neighbors_vectors(data_chunk, temporal_decay_sum_history_test,test_key_set,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"（Test）Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for test_key_id in range(len(test_key_set)):\n",
    "        test_key = test_key_set[test_key_id]\n",
    "        test_history = temporal_decay_sum_history_test[test_key] # 真向量\n",
    "        #target_set.append(data_chunk[test_chunk][test_key_set[test_key_id]]) # [[-1], array([ 270,  669, 1653, 1886, 1987, 4718, 6677]), [-1]]\n",
    "        #target_set.append((test_key, data_chunk[test_chunk][test_key][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][test_key][1]\n",
    "        answer_vector = np.zeros(len(test_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((test_key, data_chunk[test_chunk][test_key][1],answer_tensor))\n",
    "\n",
    "\n",
    "        sum_training_history = np.zeros(len(test_history))\n",
    "\n",
    "        for indecis in index[test_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[test_key_id]) #真向量\n",
    "        user_neighbor_set.append((test_key, test_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "        print(f\"Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "        print(f\"Target Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_test_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "    #   pickle.dump(user_neighbor_set, fp)\n",
    "    #   print(\"The test set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_test_answer.gz', 'wb') as fp:\n",
    "    #  pickle.dump(target_set, fp)\n",
    "    #  print(\"The test set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LbiCmRvbMCze"
   },
   "outputs": [],
   "source": [
    "def validation_get_neighbors_vectors(data_chunk, temporal_decay_sum_history_validation,validation_key_set,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"(Validation) Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for validation_key_id in range(len(validation_key_set)):\n",
    "        validation_key = validation_key_set[validation_key_id]\n",
    "        validation_history = temporal_decay_sum_history_validation[validation_key] # 真向量\n",
    "        #target_set.append(validation_key, data_chunk[test_chunk][validation_key_set[validation_key_id]][1]) # 正確答案[[-1], array([ 270,  669, 1653, 1886, 1987, 4718, 6677]), [-1]]\n",
    "        #target_set.append((validation_key, data_chunk[test_chunk][training_key_set[validation_key_id]][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][validation_key][1]\n",
    "        answer_vector = np.zeros(len(validation_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((validation_key, data_chunk[test_chunk][validation_key][1],answer_tensor))\n",
    "\n",
    "        sum_training_history = np.zeros(len(validation_history))\n",
    "\n",
    "        for indecis in index[validation_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[validation_key_id]) #真向量\n",
    "        user_neighbor_set.append((validation_key, validation_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    # 打印 user_neighbor_set 的前 20 个条目\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "        print(f\"User Neighbor Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    # 打印 target_set 的前 20 个条目\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "        print(f\"Target Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_validation_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "    #   pickle.dump(user_neighbor_set, fp)\n",
    "    #   print(\"The validation set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_validation_answer.gz', 'wb') as fp:\n",
    "    #   pickle.dump(target_set, fp)\n",
    "    #   print(\"The validation set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t8tuCVLwSIpW"
   },
   "outputs": [],
   "source": [
    "def training_get_neighbors_vectors(data_chunk,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"(Training) Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for training_key_id in range(len(training_key_set)):\n",
    "        training_key = training_key_set[training_key_id]\n",
    "        training_history = temporal_decay_sum_history_training[training_key] # 真向量\n",
    "        #target_set.append((training_key, data_chunk[test_chunk][training_key_set[training_key_id]][1]))\n",
    "        #target_set.append((training_key, data_chunk[test_chunk][training_key][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][training_key][1]\n",
    "        answer_vector = np.zeros(len(training_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((training_key, data_chunk[test_chunk][training_key][1],answer_tensor))\n",
    "\n",
    "        sum_training_history = np.zeros(len(training_history))\n",
    "\n",
    "        for indecis in index[training_key_id]:\n",
    "            training_key2 = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key2]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[training_key_id]) #真向量\n",
    "        user_neighbor_set.append((training_key, training_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "      print(f\"Item {i}: {item}\")\n",
    "      print()\n",
    "\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "      print(f\"Target Item {i}: {item}\")\n",
    "      print()\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_training_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "    #   pickle.dump(user_neighbor_set, fp)\n",
    "    #   print(\"The validation set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    # with gzip.GzipFile('preprocessing-data/TaFeng_training_answer.gz', 'wb') as fp:\n",
    "    #   pickle.dump(target_set, fp)\n",
    "    #   print(\"The training set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9qLmeZf02WC"
   },
   "source": [
    "## KNN 尋找相似鄰居\n",
    "\n",
    "對數據集中的數據行最近鄰居搜索，返回每個測試點的最近鄰居索引（indices）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5bpNdSm107E4"
   },
   "outputs": [],
   "source": [
    "# temporal_decay_sum_history_test, temporal_decay_sum_history_training,num_nearest_neighbors,training_key_set\n",
    "\n",
    "def KNN_test(query_set, target_set, k, training_key_set):\n",
    "\n",
    "    print(\"Start Looking for Neighbors ...\")\n",
    "\n",
    "    # Create matrices for target and query sets\n",
    "    history_mat = []\n",
    "    for key in target_set.keys():\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    query_keys = list(query_set.keys())  # Save the keys of the query set\n",
    "\n",
    "    for key in query_keys:\n",
    "        test_mat.append(query_set[key])\n",
    "\n",
    "    # Fit NearestNeighbors and find neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "\n",
    "    print(len(test_mat))\n",
    "    #print(indices)  # Example: [[3305 2654 ... 696][1313 ... 5668 453]]\n",
    "    print(\"indices[0]: \", indices[0])  # Example: [3305 2654 ... 696][1313 ... 5668 453]]\n",
    "    #print(\"training_key_set[3305]: \",training_key_set[3305])\n",
    "\n",
    "    neighbors_info = []\n",
    "    for i, query_key in enumerate(query_keys):\n",
    "        neighbors = [training_key_set[idx] for idx in indices[i]]\n",
    "        neighbors_info.append((query_key, neighbors))\n",
    "\n",
    "    #print(neighbors_info[:20])\n",
    "\n",
    "    print(\"Finish Looking for Neighbors !\")\n",
    "\n",
    "    # 指定 JSON 和 gzip 文件的路径\n",
    "    json_file_path = f'data/{dataset}/test_neighbors_for_dlim.json'\n",
    "    gzip_file_path = f'data/{dataset}/test_neighbors_for_dlim.json.gz'\n",
    "\n",
    "    # 将 neighbors_info 数据保存为 JSON 文件\n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(neighbors_info, file)\n",
    "\n",
    "    # 读取 JSON 文件并以 gzip 格式写入\n",
    "    with open(json_file_path, 'rb') as f_in:\n",
    "        with gzip.open(gzip_file_path, 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "\n",
    "    print(f\"GZIP file saved at {gzip_file_path}\")\n",
    "\n",
    "    return neighbors_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p29f9jtk26wI"
   },
   "outputs": [],
   "source": [
    "# temporal_decay_sum_history_test, temporal_decay_sum_history_training,num_nearest_neighbors,training_key_set\n",
    "\n",
    "def KNN_validation(query_set, target_set, k, training_key_set):\n",
    "\n",
    "    print(\"Start Looking for Neighbors ...\")\n",
    "\n",
    "    # Create matrices for target and query sets\n",
    "    history_mat = []\n",
    "    for key in target_set.keys():\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    query_keys = list(query_set.keys())  # Save the keys of the query set\n",
    "\n",
    "    for key in query_keys:\n",
    "        test_mat.append(query_set[key])\n",
    "\n",
    "    # Fit NearestNeighbors and find neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "\n",
    "    print(len(test_mat))\n",
    "    #print(indices)  # Example: [[3305 2654 ... 696][1313 ... 5668 453]]\n",
    "    print(\"indices[0]: \", indices[0])  # Example: [3305 2654 ... 696][1313 ... 5668 453]]\n",
    "    #print(\"training_key_set[3231]: \",training_key_set[3231])\n",
    "\n",
    "    neighbors_info = []\n",
    "    for i, query_key in enumerate(query_keys):\n",
    "        neighbors = [training_key_set[idx] for idx in indices[i]]\n",
    "        neighbors_info.append((query_key, neighbors))\n",
    "\n",
    "    #print(neighbors_info[:20])\n",
    "\n",
    "    print(\"Finish Looking for Neighbors !\")\n",
    "\n",
    "    # 指定 JSON 和 gzip 文件的路径\n",
    "    json_file_path = f'data/{dataset}/validation_neighbors_for_dlim.json'\n",
    "    gzip_file_path = f'data/{dataset}/validation_neighbors_for_dlim.json.gz'\n",
    "\n",
    "    # 将 neighbors_info 数据保存为 JSON 文件\n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(neighbors_info, file)\n",
    "\n",
    "    # 读取 JSON 文件并以 gzip 格式写入\n",
    "    with open(json_file_path, 'rb') as f_in:\n",
    "        with gzip.open(gzip_file_path, 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "\n",
    "    print(f\"GZIP file saved at {gzip_file_path}\")\n",
    "\n",
    "    return neighbors_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QECZBpXM-y5Y"
   },
   "outputs": [],
   "source": [
    "def KNN_training(query_set, target_set, k):\n",
    "\n",
    "    print(\"Start Looking for Neighbors ...\")\n",
    "    keys_list = list(target_set.keys())\n",
    "    #print(keys_list)\n",
    "\n",
    "    history_mat = []\n",
    "    for key in target_set.keys(): # key 是 User ID\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    query_keys = list(query_set.keys())  # 保存查詢鍵列表\n",
    "    for key in query_set.keys():\n",
    "        test_mat.append(query_set[key])\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "    all_filtered_indices = []  # 用于存储所有查询点的过滤后的邻居索引\n",
    "\n",
    "    for i, key in enumerate(query_keys):\n",
    "        index_to_remove = keys_list.index(key)  # 获取要删除的索引\n",
    "        filtered_indices = [index for index in indices[i] if index != index_to_remove]\n",
    "        all_filtered_indices.append(filtered_indices)\n",
    "\n",
    "    print('all_filtered_indices: ', all_filtered_indices[0])\n",
    "    print('all_filtered_indices[0][0] :',all_filtered_indices[0][0])\n",
    "    print(\"training_key_set[2346]: \",training_key_set[2346])\n",
    "\n",
    "    neighbors_info = []\n",
    "    for i, query_key in enumerate(query_keys):\n",
    "        neighbors = [training_key_set[idx] for idx in all_filtered_indices[i]]\n",
    "        neighbors_info.append((query_key, neighbors))\n",
    "\n",
    "    print(neighbors_info[:1])\n",
    "    print(\"Finish Looking for Neighbors !\")\n",
    "\n",
    "    # 指定 JSON 和 gzip 文件的路径\n",
    "    json_file_path = f'data/{dataset}/training_neighbors_for_dlim.json'\n",
    "    gzip_file_path = f'data/{dataset}/training_neighbors_for_dlim.json.gz'\n",
    "\n",
    "    # 将 neighbors_info 数据保存为 JSON 文件\n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(neighbors_info, file)\n",
    "\n",
    "    # 读取 JSON 文件并以 gzip 格式写入\n",
    "    with open(json_file_path, 'rb') as f_in:\n",
    "        with gzip.open(gzip_file_path, 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "\n",
    "    print(f\"GZIP file saved at {gzip_file_path}\")\n",
    "\n",
    "    return neighbors_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z69F5fpj5v7q"
   },
   "source": [
    "## 取得用戶向量與對應的鄰居"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ROKy64mYBB-w"
   },
   "outputs": [],
   "source": [
    "def get_user_neighbor(data_chunk, training_key_set, test_key_set, validation_key_set, num_nearest_neighbors,temporal_decay_sum_history_test, temporal_decay_sum_history_training,temporal_decay_sum_history_validation):\n",
    "\n",
    "    # 獲得每位用戶的鄰居索引\n",
    "    index_test = KNN_test(temporal_decay_sum_history_test, temporal_decay_sum_history_training,num_nearest_neighbors,training_key_set)\n",
    "    index_validation = KNN_validation(temporal_decay_sum_history_validation, temporal_decay_sum_history_training,num_nearest_neighbors,training_key_set)\n",
    "    index_training = KNN_training(temporal_decay_sum_history_training, temporal_decay_sum_history_training,num_nearest_neighbors)\n",
    "\n",
    "    status = \"OK\"\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4RxXu8pHjTJI"
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_Fscore(groundtruth,pred):\n",
    "    a = groundtruth\n",
    "    b = pred\n",
    "    correct = 0\n",
    "    truth = 0\n",
    "    positive = 0\n",
    "\n",
    "    for idx in range(len(a)):\n",
    "        if a[idx] == 1:\n",
    "            truth += 1\n",
    "            if b[idx] == 1:\n",
    "                correct += 1\n",
    "        if b[idx] == 1:\n",
    "            positive += 1\n",
    "\n",
    "    flag = 0\n",
    "    if 0 == positive:\n",
    "        precision = 0\n",
    "        flag = 1\n",
    "        #print('postivie is 0')\n",
    "    else:\n",
    "        precision = correct/positive\n",
    "    if 0 == truth:\n",
    "        recall = 0\n",
    "        flag = 1\n",
    "        #print('recall is 0')\n",
    "    else:\n",
    "        recall = correct/truth\n",
    "\n",
    "    if flag == 0 and precision + recall > 0:\n",
    "        F = 2*precision*recall/(precision+recall)\n",
    "    else:\n",
    "        F = 0\n",
    "    return precision, recall, F, correct\n",
    "\n",
    "def get_HT(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            return 1\n",
    "        count += 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_NDCG1(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1)/math.log2(count+1+1)\n",
    "        count += 1\n",
    "    idcg = 0\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "    for i in range(num_item):\n",
    "        idcg += (1) / math.log2(i + 1 + 1)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY0Eqecl-VZJ"
   },
   "source": [
    "# 主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzBIvnv_pt_P",
    "outputId": "86e23180-7f26-4dcc-b531-e5c6b001d9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Start Dictionary Generation -----\n",
      "# dimensions of final vector: 3003 | 0\n",
      "----- Finish Dictionary Generation -----\n",
      "num_dim+2 : 3005\n",
      "----- Start Splitting Data Set -----\n",
      "Number of training instances: 9234\n",
      "Number of validation instances: 1026\n",
      "Number of test instances: 2565\n",
      "training_key_set[0]:  31\n",
      "validation_key_set[0]:  726826\n",
      "test_key_set[0]:  804580\n",
      "----- Finish splitting the data set -----\n",
      "Temporal Decay Sum History ...\n",
      "Temporal Decay Sum History ...\n",
      "Temporal Decay Sum History ...\n",
      "Start Looking for Neighbors ...\n",
      "2565\n",
      "indices[0]:  [1372 5379 5081 7989 4144  686  535 2916 6810 4243 4417 1855 1997 6136\n",
      " 1177 2445 6227 7972 7600 3770  688 7224 7322 2101 4655 6124 2542 6186\n",
      " 6445 4374 8241 7874 8757 3719 6574 2656 6450 8476 2207 1701 3378 6627\n",
      "  131 1816 5561  325 3245 2769  615 6154 3454 8820 1065 4277  530 8527\n",
      " 5533 6421 3077 3810 7399  797 6603 1156 1263 6470 2322 3714 6317 3732\n",
      " 5097  817 4329  440  658  540 2856 4648 3364 8349 7747 7429 6120 8398\n",
      "  914 8473 6254 5971 1710 7330 2236 5995 9178 2159 8732 2703 4613 1046\n",
      "  378 5026  379 2004 1230 6448 1028  614 6833 4307  142 3165 2165 5966\n",
      "  621 6740  596 4185 7873 1066 2871 2387 6848 1080 1756 6503 5208 6845\n",
      " 3665 5362 4906 9000 2517 6171 1627 7037 1173 2418 7881 4825 8023  789\n",
      " 3084 6982 7892 4599 8084 5212 3907 7145 1690 4331 3660 3185 6463 5659\n",
      " 8375 7387 8225 8978 3217 7389 6607 8708 2227 4827 4808 1578  550 7111\n",
      " 1651 6697 8497 4244 5680 2552 5070 5892 3394 1276 7462 5517 5931  837\n",
      " 4261 6190 5405 5035 1049 6162 2561 4181 4209 3886  130 7805 8894 4741\n",
      "  493  671 2145 3246 8368 3502 1463 7468  685 1514 2511 5179  377 6303\n",
      " 2876 6364 6396  281  507  410 6660 1007  944 8410 6093 6104 8008 1494\n",
      " 5195 4931 8116 8326 3602 5364 3315 2909 1226 6258 2353 5919 4257 7243\n",
      " 8112 7278 1105 1853 5760 3360 4056 4396 4161 8147 8640 5922 1903 8711\n",
      " 2043 3124 8523 4583 6225 6344 9053 3803 6401 1153 8550 5187 3639 7053\n",
      " 5769 3107 6056 6811  909 7151 1352 3402 7895 3346 6024 5309 1294  326\n",
      " 1736 2569 2504 4357  983 3102 3711 2163 4551 6651 6061 4440 3494 3379\n",
      " 2270 2498 2654 1704 9191 2147]\n",
      "Finish Looking for Neighbors !\n",
      "GZIP file saved at data/Dunnhumby/test_neighbors_for_dlim.json.gz\n",
      "Start Looking for Neighbors ...\n",
      "1026\n",
      "indices[0]:  [2769 5379 4613 3770 8241 1997 6124 6186 6154 3378 6421 6136 8527 1372\n",
      "  378 8757 5533  686 9178  615  658 5097 6450  817 2207 8473 8349  795\n",
      " 4648 7387 1066 1816  440 2004 4329 2916  614 6227 2165 7881 3165 2101\n",
      " 3719 4825  142 6848 7145 2163 7892 5035 5561 6982 1230 5212 6810 3907\n",
      " 4599  789 1756 6445 3454 8497 2445 6463 8708  530 7462  685 2542 6364\n",
      " 3246 1651 5070  596 7600  493 1710 7111 8978 6317 3660 9211 6574 7989\n",
      " 5362 7322 8894  281 5966 4741 6740 4827  377 1701 3217 4261  410  837\n",
      " 6660 4060  621 6162 6607 2871 6503 4277 9043  944 5306 2387 2418 4056\n",
      " 1853 5364 6697 4174 2857 4307 3077 8640 6303 1080 2856 5187 3803  507\n",
      " 5769 7468 4808 4931 8023 7151 8084 5977 6056 1667 6159 8368 7895 6833\n",
      " 5931 6396  983 1071 4231 8008 4417 4257 3402 5919 6521 5517 1263 9120\n",
      " 7331 8147 4954 1156 8398 4331 5081 6214 4286 7972 2909 5709  319 1046\n",
      " 7075 4440 9053 8615 7859 3886 2997 2513 8323 3084 2630 3417 2270 2228\n",
      " 4304 4144 1884 8112 4108 4726 8820 1578 3597 3379 8522  648  797 7978\n",
      " 4851 7263 2511 6061 7278  535 7874 3364 5405 7742 2876 6190 1065 4789\n",
      " 8402 1105 1725  379 3625  131 5208 7663 2703 3837 6189 6104 3346 3602\n",
      " 1153 8718 2366 7820 5227 7936 5044 4333 4906 7399 2353 4868 1736  916\n",
      " 7885 5042 1173 1049 6559 6344 2561 7467 6315 5973 6394 6401 2834 4059\n",
      "  914 4033 7818  631 7037 5906 7126 2355 6120 6821 5031 6024 8193 4848\n",
      " 1143 7768 3593 3956  171 7726 5876  888 9147 2879 8514 2034 7747 6603\n",
      " 8722 1142  961 3826 6745  688 3282 7849 2618 2697 1514 4090 6093 8868\n",
      " 1690 2227   75 6225 4057 2147]\n",
      "Finish Looking for Neighbors !\n",
      "GZIP file saved at data/Dunnhumby/validation_neighbors_for_dlim.json.gz\n",
      "Start Looking for Neighbors ...\n",
      "all_filtered_indices:  [2703, 5379, 3454, 3770, 1372, 6186, 7663, 2769, 6124, 8241, 6227, 3165, 3084, 6136, 686, 1997, 2207, 5362, 2916, 5533, 8640, 6982, 4648, 8757, 6450, 1816, 6445, 3378, 5097, 4144, 6154, 2542, 6810, 4825, 7322, 2445, 5561, 5195, 6421, 817, 615, 7600, 8349, 5680, 3077, 1701, 658, 9178, 614, 4243, 8473, 6303, 2004, 1756, 4056, 1066, 7881, 685, 2101, 4417, 3719, 440, 7989, 378, 3660, 4613, 530, 4329, 4851, 377, 8711, 7972, 2856, 8497, 5081, 8375, 6463, 2165, 2370, 6848, 6833, 142, 7892, 4931, 6740, 4370, 5212, 1710, 1230, 797, 8718, 3364, 3907, 3837, 8398, 493, 4277, 837, 7387, 8894, 5966, 8153, 550, 8527, 1853, 8820, 789, 5070, 535, 8708, 944, 788, 621, 4954, 6821, 7462, 8368, 1651, 7747, 7111, 7399, 8978, 5035, 6607, 3502, 3360, 4307, 6120, 4827, 6190, 2418, 6344, 1065, 1163, 7849, 7895, 3246, 6317, 8023, 6660, 711, 131, 4257, 9120, 648, 6162, 8147, 3346, 281, 6364, 5187, 983, 6401, 914, 4741, 4742, 8314, 2876, 7151, 410, 6503, 379, 4261, 3217, 8084, 4808, 2236, 4655, 8615, 5364, 3607, 7874, 688, 8008, 2935, 5620, 9147, 1156, 6396, 4331, 631, 6225, 7037, 596, 5769, 4599, 8732, 4374, 1810, 8402, 5931, 7429, 7389, 8719, 3185, 5227, 2353, 7468, 7145, 9000, 1322, 1049, 2579, 6697, 5892, 3315, 6061, 6521, 3803, 507, 1870, 6056, 8323, 1690, 2387, 4170, 6470, 5517, 7224, 7467, 2871, 5026, 3102, 2909, 2997, 1046, 3602, 6055, 8225, 1578, 8089, 4231, 171, 1514, 6603, 5977, 2561, 2511, 7126, 6214, 1263, 6574, 2513, 4095, 5919, 9053, 1080, 123, 6760, 1667, 44, 9020, 2054, 1177, 4286, 5906, 2163, 2879, 8868, 4161, 3886, 2630, 7331, 4174, 3245, 9058, 2228, 5574, 1954, 3597, 6254, 2517, 8522, 319, 1202, 6845, 9191, 5745, 2270, 3200, 7075, 4440, 325, 7742, 5871, 1725, 7223, 7988, 4848, 1105, 3402, 6118, 6229, 2656, 5794, 2316, 8822, 4124, 1276, 6562]\n",
      "all_filtered_indices[0][0] : 2703\n",
      "training_key_set[2346]:  185320\n",
      "[('31', ['211052', '428025', '266989', '293802', '107077', '491839', '605802', '215512', '486759', '651259', '495345', '244650', '238158', '487479', '52783', '154266', '172346', '425722', '226027', '441867', '683525', '551172', '365721', '691843', '510937', '140492', '510368', '261575', '403281', '323812', '489192', '198783', '537635', '380801', '579253', '191377', '444051', '412206', '508807', '62328', '47760', '600406', '659359', '453093', '237637', '130456', '51022', '722116', '47742', '332270', '669592', '500478', '155225', '136130', '317474', '82736', '622732', '52749', '162857', '346608', '288706', '34878', '630499', '29945', '283556', '362775', '41099', '339747', '383016', '29934', '688560', '629767', '221167', '671602', '402253', '661642', '511680', '168928', '187135', '540205', '538986', '11216', '623887', '390128', '531790', '343886', '413620', '131435', '94689', '60968', '688967', '259899', '305179', '299442', '663696', '38017', '334736', '63600', '584197', '701102', '474396', '644348', '42399', '673586', '143318', '695739', '59994', '401746', '41465', '688311', '72163', '59645', '48205', '392422', '538320', '589494', '661259', '127088', '611415', '561751', '585322', '708114', '398826', '521912', '271178', '259539', '338285', '486488', '380952', '492314', '190065', '503218', '82732', '89338', '620211', '624115', '250900', '501481', '633307', '526245', '54066', '10665', '333070', '717730', '50286', '489604', '643956', '258356', '22778', '505040', '411829', '75136', '507340', '69549', '373941', '374183', '657124', '222978', '564651', '33027', '514532', '30144', '333590', '247986', '638196', '379625', '175075', '366454', '681437', '425851', '280293', '622257', '52863', '631846', '227244', '448778', '719795', '88691', '507001', '339788', '48846', '495160', '555114', '46247', '459757', '361919', '689700', '344042', '139919', '664117', '471872', '587584', '584316', '688985', '245601', '415453', '185941', '589912', '564240', '709995', '102717', '81186', '202015', '528833', '469080', '256306', '482532', '515785', '297312', '39175', '145253', '482336', '657588', '129259', '188167', '327081', '512027', '440521', '570839', '589838', '222547', '397955', '239807', '225523', '232415', '80962', '280144', '481944', '649449', '121439', '639199', '331299', '13583', '116567', '521610', '475246', '200260', '196853', '562712', '494231', '97938', '519963', '196896', '320694', '471379', '713886', '83948', '10193', '533358', '128209', '4239', '711694', '159474', '90354', '335425', '469890', '168687', '223047', '698713', '326297', '303022', '205695', '580102', '327357', '250864', '714086', '174532', '445050', '151450', '279989', '497226', '197036', '673243', '25648', '92317', '539951', '722750', '457660', '178892', '246869', '558265', '348767', '25882', '611301', '467785', '132947', '570609', '630475', '382901', '85139', '262921', '486457', '495659', '207064', '461647', '182563', '695983', '322740', '99255', '519141'])]\n",
      "Finish Looking for Neighbors !\n",
      "GZIP file saved at data/Dunnhumby/training_neighbors_for_dlim.json.gz\n"
     ]
    }
   ],
   "source": [
    "files = [f'data/{dataset}_history.csv', f'data/{dataset}_future.csv'] # 歷史與未來的兩個檔案\n",
    "\n",
    "# 讀取和處理兩個檔案\n",
    "# data_chunk 長度為 2，其保存在歷史與未來中，對於每位用戶的所有購物籃。[0] 即為 history，data_chunk[1] 即為 future\n",
    "data_chunk, input_size = read_claim2vector_embedding_file_no_vector(files)\n",
    "training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[test_chunk]), 1) # 將數據分為訓練、驗證和測試集\n",
    "\n",
    "# # 取得用戶向量\n",
    "temporal_decay_sum_history_training = temporal_decay_sum_history(data_chunk[training_chunk],training_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "temporal_decay_sum_history_test = temporal_decay_sum_history(data_chunk[training_chunk],test_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "temporal_decay_sum_history_validation = temporal_decay_sum_history(data_chunk[training_chunk],validation_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "\n",
    "# # # 取得所有用戶向量與其對應的鄰居\n",
    "status = get_user_neighbor(data_chunk, training_key_set, test_key_set,validation_key_set,num_nearest_neighbors,temporal_decay_sum_history_test, temporal_decay_sum_history_training,temporal_decay_sum_history_validation)\n",
    "\n",
    "# print(status)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pifta",
   "language": "python",
   "name": "pifta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
