{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e240d7e-8229-4263-8eb4-69e6cc645bee",
   "metadata": {},
   "source": [
    "# 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec67a0fd-bf74-4f81-8bae-bf551e42b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32abe0-816b-4830-a72a-159155616a2e",
   "metadata": {},
   "source": [
    "# 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4546d202-fb74-4047-8331-f119b2f2c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Dunnhumby\" # \"Tafeng\" or \"Dunnhumby\"\n",
    "\n",
    "\n",
    "# 隨資料集調整\n",
    "k = 30\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "\n",
    "\n",
    "#固定參數設置\n",
    "epochs = 80\n",
    "embed_dim = 64\n",
    "ffn_hidden_dim = 256\n",
    "decay_rate = 0.3\n",
    "dropout_rate = 0.3\n",
    "num_heads = 4\n",
    "num_trans_layers = 1\n",
    "max_seq_length = 75\n",
    "vector_size = 3005  # Tafeng = 12087 / Dunnhumby = 3005\n",
    "num_products = 3005 # Tafeng = 12087 / Dunnhumby = 3005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048032a-24e1-424b-8105-078364e1c795",
   "metadata": {},
   "source": [
    "# 載入數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d8668c-a2ff-42b5-a346-bfd12f4c2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_training_answer.gz\", \"rb\") as fp:\n",
    "    TaFeng_training_answer = pickle.load(fp)\n",
    "\n",
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_validation_answer.gz\", \"rb\") as fp:\n",
    "    TaFeng_validation_answer = pickle.load(fp)\n",
    "\n",
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_test_answer.gz\", \"rb\") as fp:\n",
    "    TaFeng_test_answer = pickle.load(fp)\n",
    "\n",
    "\n",
    "# 將答案轉換為字典\n",
    "true_training_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in TaFeng_training_answer}\n",
    "true_validation_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in TaFeng_validation_answer}\n",
    "true_test_basket_dict = {item[0]: item[2].float() if not isinstance(item[2], torch.Tensor) else item[2].float() for item in TaFeng_test_answer}\n",
    "\n",
    "# KIM \n",
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_training_user_and_neighbor_set.gz\", \"rb\") as fp:\n",
    "    TaFeng_training_user_and_neighbor_set = pickle.load(fp)\n",
    "\n",
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_validation_user_and_neighbor_set.gz\", \"rb\") as fp:\n",
    "    TaFeng_validation_user_and_neighbor_set = pickle.load(fp)\n",
    "\n",
    "with gzip.open(f\"data/{dataset}/preprocessed_data/{dataset}_test_user_and_neighbor_set.gz\", \"rb\") as fp:\n",
    "    TaFeng_test_user_and_neighbor_set = pickle.load(fp)\n",
    "\n",
    "# DLIM\n",
    "training_embedding_file = f'data/{dataset}/basketembedding/training_basketembedding_{embed_dim}.pkl.gz'\n",
    "training_neighbors_file = f'data/{dataset}/training_neighbors_for_dlim.json.gz'\n",
    "\n",
    "validation_embedding_file = f'data/{dataset}/basketembedding/validation_basketembedding_{embed_dim}.pkl.gz'\n",
    "validation_neighbors_file = f'data/{dataset}/validation_neighbors_for_dlim.json.gz'\n",
    "\n",
    "test_embedding_file = f'data/{dataset}/basketembedding/test_basketembedding_{embed_dim}.pkl.gz'\n",
    "test_neighbors_file = f'data/{dataset}/test_neighbors_for_dlim.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662b5b8-69c0-45ef-8b91-0b87a7def2d9",
   "metadata": {},
   "source": [
    "# 定義 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3382dd30-86b5-4bf1-97cd-bc78858d1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, user_neighbor_data, answer_data, basket_embedding_file, basket_neighbors_file, true_basket_dict, max_seq_length=max_seq_length):\n",
    "        # TaFeng 数据\n",
    "        self.user_neighbor_data = user_neighbor_data\n",
    "        self.answer_data = answer_data\n",
    "\n",
    "        # Basket 数据\n",
    "        with gzip.open(basket_embedding_file, 'rb') as f:\n",
    "            self.basket_embeddings = pickle.load(f)\n",
    "        with gzip.open(basket_neighbors_file, 'rb') as f:\n",
    "            self.neighbors = json.load(f)\n",
    "        self.true_basket_dict = true_basket_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def calculate_relative_dates(self, transaction_dates):\n",
    "        #dates = [np.datetime64(date) for date in transaction_dates] # Tafeng 要跑這行\n",
    "        dates = [np.datetime64(f\"{str(date)[:4]}-{str(date)[4:6]}-{str(date)[6:]}\") for date in transaction_dates] # Dunnhumby 要跑這行\n",
    "        max_date = max(dates) + np.timedelta64(1, 'D')\n",
    "        relative_dates = [(max_date - date).astype(int) for date in dates]\n",
    "        return relative_dates\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_neighbor_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # TaFeng 数据\n",
    "        user_id, user_vector, neighbor_vector = self.user_neighbor_data[idx]\n",
    "        _, _, answer_vector = self.answer_data[idx]\n",
    "        ta_feng_data = (torch.tensor(user_vector, dtype=torch.float32), \n",
    "                        torch.tensor(neighbor_vector, dtype=torch.float32), \n",
    "                        answer_vector.clone().detach().to(dtype=torch.float32))\n",
    "\n",
    "        # DLIM\n",
    "        # Basket 数据\n",
    "        _, neighbors_ids = self.neighbors[idx]\n",
    "        user_data = self.basket_embeddings.get(user_id, [])\n",
    "        user_embeddings = [torch.tensor(embedding[0]) for embedding in user_data]\n",
    "        user_dates = [embedding[1] for embedding in user_data]\n",
    "        user_dates = self.calculate_relative_dates(user_dates)\n",
    "\n",
    "        # 填充用户的购物篮嵌入向量和交易日期\n",
    "        user_embeddings_padded = torch.zeros((self.max_seq_length, len(user_embeddings[0])))\n",
    "        user_dates_padded = torch.full((self.max_seq_length,), -1, dtype=torch.int64)  # 使用 -1 填充日期\n",
    "\n",
    "        if user_embeddings:\n",
    "            user_embeddings_tensor = pad_sequence(user_embeddings, batch_first=True)\n",
    "            user_dates_tensor = torch.tensor(user_dates, dtype=torch.int64)\n",
    "            user_seq_len = min(self.max_seq_length, len(user_dates))\n",
    "\n",
    "            user_embeddings_padded[:user_seq_len, :] = user_embeddings_tensor[:user_seq_len, :]\n",
    "            user_dates_padded[:user_seq_len] = user_dates_tensor[:user_seq_len]\n",
    "\n",
    "        # 初始化邻居嵌入向量和交易日期的填充列表\n",
    "        neighbor_embeddings_padded = torch.zeros((300, self.max_seq_length, len(user_embeddings[0])))\n",
    "        neighbor_dates_padded = torch.full((300, self.max_seq_length), -1, dtype=torch.int64)  # 使用 -1 填充日期\n",
    "\n",
    "        # 填充邻居的购物篮嵌入向量和交易日期\n",
    "        for i, neighbor_id in enumerate(neighbors_ids):\n",
    "            n_data = self.basket_embeddings.get(neighbor_id, [])\n",
    "            n_embeddings = [torch.tensor(embedding[0]) for embedding in n_data]\n",
    "            n_dates = [embedding[1] for embedding in n_data]\n",
    "            n_dates = self.calculate_relative_dates(n_dates)\n",
    "            \n",
    "            if n_embeddings:\n",
    "                n_embeddings_tensor = pad_sequence(n_embeddings, batch_first=True)\n",
    "                n_dates_tensor = torch.tensor(n_dates, dtype=torch.int64)\n",
    "                seq_len = min(self.max_seq_length, len(n_dates))\n",
    "\n",
    "                neighbor_embeddings_padded[i, :seq_len, :] = n_embeddings_tensor[:seq_len, :]\n",
    "                neighbor_dates_padded[i, :seq_len] = n_dates_tensor[:seq_len]\n",
    "\n",
    "        true_basket_vector = self.true_basket_dict.get(user_id, torch.zeros(vector_size))\n",
    "        basket_data = (user_embeddings_padded, user_dates_padded, neighbor_embeddings_padded, neighbor_dates_padded, true_basket_vector)\n",
    "\n",
    "        return ta_feng_data, basket_data  # 返回一个包含两部分数据的元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e1531c-deb7-4bf6-a2f3-2c8a40fdef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集 DataLoader\n",
    "training_dataset = CombinedDataset(TaFeng_training_user_and_neighbor_set, TaFeng_training_answer,\n",
    "                                             training_embedding_file, training_neighbors_file,\n",
    "                                             true_training_basket_dict, max_seq_length=max_seq_length)\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 验证集 DataLoader\n",
    "validation_dataset = CombinedDataset(TaFeng_validation_user_and_neighbor_set, TaFeng_validation_answer,\n",
    "                                               validation_embedding_file, validation_neighbors_file,\n",
    "                                               true_validation_basket_dict, max_seq_length=max_seq_length)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 测试集 DataLoader\n",
    "test_dataset = CombinedDataset(TaFeng_test_user_and_neighbor_set, TaFeng_test_answer,\n",
    "                                         test_embedding_file, test_neighbors_file,\n",
    "                                         true_test_basket_dict, max_seq_length=max_seq_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1aca6c-4a53-483c-8e41-40f9c55fb460",
   "metadata": {},
   "source": [
    "# KIM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729bd8ab-e186-44c3-9846-bafca2308ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, vector_size):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.7))\n",
    "\n",
    "    def forward(self, user_vector, neighbor_vector):\n",
    "\n",
    "        weighted_neighbor_vector = self.alpha * neighbor_vector\n",
    "        weighted_user_vector = (1 - self.alpha) * user_vector\n",
    "        Ans_1 = weighted_user_vector + weighted_neighbor_vector        \n",
    "        return Ans_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9bf71-ae83-49b3-8e8d-fa6df1c4e68c",
   "metadata": {},
   "source": [
    "# DLIM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73348d6a-6347-4428-bf36-38f6be87b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, decay_rate, embedding_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.decay_rate = nn.Parameter(torch.tensor(decay_rate))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, basket_sequence, transaction_dates):\n",
    "        mask = (transaction_dates != -1).float()\n",
    "        decay_weights = torch.exp(-self.decay_rate * transaction_dates)\n",
    "        decay_weights = decay_weights * mask\n",
    "        decay_weights_sum = decay_weights.sum(1, keepdim=True)\n",
    "        normalized_weights = decay_weights / decay_weights_sum\n",
    "        user_embedding = torch.sum(normalized_weights.unsqueeze(-1) * basket_sequence, dim=1)\n",
    "        return user_embedding\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ffn_hidden_dim, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ffn_hidden_dim, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src):\n",
    "        attn_output, _ = self.multihead_attn(src, src, src)\n",
    "        src = self.layer_norm1(src + attn_output)\n",
    "        ffn_output = self.feed_forward(src)\n",
    "        src = self.layer_norm2(src + ffn_output)\n",
    "        return src\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_hidden_dim, dropout_rate):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, ffn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(ffn_hidden_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ffn = self.fc2(F.relu(self.fc1(x)))\n",
    "        x = self.layer_norm(x + self.dropout(x_ffn))\n",
    "        return x\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b864ad3a-1f8e-42b2-91fc-8c86375c0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, decay_rate, ffn_hidden_dim, num_products, dropout_rate, num_trans_layers=num_trans_layers):\n",
    "        super(RecommendationModel, self).__init__()\n",
    "        self.temporal_attention = TemporalAttention(decay_rate, embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(embedding_dim, num_heads, ffn_hidden_dim, dropout_rate) for _ in range(num_trans_layers)\n",
    "        ])\n",
    "        self.mlp = MLPLayer(embedding_dim, ffn_hidden_dim, num_products)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates):\n",
    "        user_embedding = self.temporal_attention(user_basket_sequence, user_transaction_dates)\n",
    "        neighbor_embeddings = torch.stack([     \n",
    "            self.temporal_attention(neighbor_seq, neighbor_dates)\n",
    "            for neighbor_seq, neighbor_dates in zip(neighbor_basket_sequence, neighbor_transaction_dates)\n",
    "        ]).transpose(0, 1)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            neighbor_embeddings = layer(neighbor_embeddings)\n",
    "\n",
    "        neighbor_embedding = neighbor_embeddings[-1]\n",
    "        combined_embedding = user_embedding + neighbor_embedding\n",
    "        #output = self.relu(self.mlp(combined_embedding.squeeze(0)))\n",
    "        output = self.mlp(combined_embedding.squeeze(0))\n",
    "\n",
    "         # 通过MLP层进行预测\n",
    "        #recommendation_output = self.mlp(combined_embedding)\n",
    "        #output = self.softmax(recommendation_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24880d6-41b4-4a91-b8bb-31dc7c004fd1",
   "metadata": {},
   "source": [
    "# 損失函數與優化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1b8723-eeef-42b7-bc86-ef472f7a2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = AttentionMechanism(vector_size).to(device)\n",
    "recommendation_model = RecommendationModel(embedding_dim=embed_dim, num_heads=num_heads, decay_rate=decay_rate, ffn_hidden_dim=ffn_hidden_dim, num_products=num_products, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(list(attention_model.parameters()) + list(recommendation_model.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360e0f9-fc42-42c3-a08c-cbfb12acf831",
   "metadata": {},
   "source": [
    "# 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8b2d04-57ee-4c9e-b8ed-e1ec01341150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topk_metrics(predictions, targets, k):\n",
    "    # 将模型输出转换为 top-k 二值向量\n",
    "    _, top_indices = torch.topk(predictions, k, dim=1)\n",
    "    topk_binary_vector = torch.zeros_like(predictions)\n",
    "    topk_binary_vector.scatter_(1, top_indices, 1)\n",
    "\n",
    "    # 计算 true positives, false positives, false negatives\n",
    "    true_positives = torch.sum(topk_binary_vector * targets, dim=1)\n",
    "    false_positives = torch.sum(topk_binary_vector * (1 - targets), dim=1)\n",
    "    false_negatives = torch.sum((1 - topk_binary_vector) * targets, dim=1)\n",
    "\n",
    "    # 计算指标\n",
    "    recall = torch.mean(true_positives / (true_positives + false_negatives))\n",
    "    precision = torch.mean(true_positives / (true_positives + false_positives))\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # 计算 Hit Ratio (HR)\n",
    "    hr = torch.mean((true_positives > 0).float())\n",
    "\n",
    "    return recall.item(), precision.item(), f1.item(), hr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e215d1-db59-478f-84ec-fa4779c81cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_score(predictions, targets, k):\n",
    "\n",
    "    # 获取 top-k 预测项的索引\n",
    "    _, top_indices = torch.topk(predictions, k, dim=1)\n",
    "    \n",
    "    # 生成 DCG 分数\n",
    "    dcg = 0.0\n",
    "    for i in range(1, k + 1):\n",
    "        dcg += ((2 ** targets.gather(1, top_indices[:, i - 1].view(-1, 1)) - 1) / torch.log2(torch.tensor(i + 1).float())).squeeze()\n",
    "\n",
    "    # 生成理想的 DCG 分数 (IDCG)\n",
    "    _, ideal_indices = torch.topk(targets, k, dim=1)\n",
    "    idcg = 0.0\n",
    "    for i in range(1, k + 1):\n",
    "        idcg += ((2 ** targets.gather(1, ideal_indices[:, i - 1].view(-1, 1)) - 1) / torch.log2(torch.tensor(i + 1).float())).squeeze()\n",
    "\n",
    "    # 处理 IDCG 为 0 的情况，防止除以零\n",
    "    idcg[idcg == 0] = 1.0\n",
    "\n",
    "    # 计算 NDCG\n",
    "    ndcg = torch.mean(dcg / idcg)\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf31943-80b3-4306-9c19-4f3fb64b050f",
   "metadata": {},
   "source": [
    "# 驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b074b2-8084-4662-8e86-7abf65af856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(attention_model, recommendation_model, validation_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k):\n",
    "    attention_model.eval()\n",
    "    recommendation_model.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'recall': 0.0, 'precision': 0.0, 'f1': 0.0, 'hr': 0.0, 'ndcg': 0.0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in validation_loader:\n",
    "            # 解包 CombinedDataset 返回的数据\n",
    "            ta_feng_data, basket_data = val_batch\n",
    "            user_vector, neighbor_vector, answer_vector = ta_feng_data\n",
    "            user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = basket_data\n",
    "\n",
    "            # 移动数据到设备\n",
    "            user_vector, neighbor_vector, answer_vector = user_vector.to(device), neighbor_vector.to(device), answer_vector.to(device)\n",
    "            user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = user_basket_sequence.to(device), user_transaction_dates.to(device), neighbor_basket_sequence.to(device), neighbor_transaction_dates.to(device), true_basket_vector.to(device)\n",
    "\n",
    "            # 模型前向传播\n",
    "            output_attention = attention_model(user_vector, neighbor_vector)\n",
    "            output_recommendation = recommendation_model(user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates)\n",
    "            \n",
    "            # 对模型的输出进行组合和处理\n",
    "            normalized_ans_2 = output_recommendation / torch.sum(output_recommendation)\n",
    "            combined_output = output_attention + normalized_ans_2\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = loss_function(combined_output, answer_vector.float())\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "            # 计算评估指标\n",
    "            recall, precision, f1, hr = calculate_topk_metrics(combined_output, answer_vector, k)\n",
    "            ndcg = ndcg_score(combined_output, answer_vector, k)\n",
    "            \n",
    "            val_metrics['recall'] += recall\n",
    "            val_metrics['precision'] += precision\n",
    "            val_metrics['f1'] += f1\n",
    "            val_metrics['hr'] += hr\n",
    "            val_metrics['ndcg'] += ndcg\n",
    "            \n",
    "    avg_loss = val_loss / len(validation_loader)\n",
    "    avg_metrics = {k: val_metrics[k] / len(validation_loader) for k in val_metrics}\n",
    "    \n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ee969-f080-4669-9d68-383c7929ac78",
   "metadata": {},
   "source": [
    "# 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f16c0463-1f68-4520-95af-ae2b26da8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(attention_model, recommendation_model, test_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k):\n",
    "    attention_model.eval()\n",
    "    recommendation_model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_metrics = {'recall': 0.0, 'precision': 0.0, 'f1': 0.0, 'hr': 0.0, 'ndcg': 0.0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            # 解包 CombinedDataset 返回的数据\n",
    "            ta_feng_data, basket_data = test_batch\n",
    "            user_vector, neighbor_vector, answer_vector = ta_feng_data\n",
    "            user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = basket_data\n",
    "\n",
    "            # 移动数据到设备\n",
    "            user_vector, neighbor_vector, answer_vector = user_vector.to(device), neighbor_vector.to(device), answer_vector.to(device)\n",
    "            user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = user_basket_sequence.to(device), user_transaction_dates.to(device), neighbor_basket_sequence.to(device), neighbor_transaction_dates.to(device), true_basket_vector.to(device)\n",
    "\n",
    "            # 模型前向传播\n",
    "            output_attention = attention_model(user_vector, neighbor_vector)\n",
    "            output_recommendation = recommendation_model(user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates)\n",
    "\n",
    "            normalized_ans_2 = output_recommendation / torch.sum(output_recommendation)\n",
    "            combined_output = output_attention + normalized_ans_2\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = loss_function(combined_output, answer_vector.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # 计算评估指标\n",
    "            recall, precision, f1, hr = calculate_topk_metrics(combined_output, answer_vector, k)\n",
    "            ndcg = ndcg_score(combined_output, answer_vector, k)\n",
    "        \n",
    "            test_metrics['recall'] += recall\n",
    "            test_metrics['precision'] += precision\n",
    "            test_metrics['f1'] += f1\n",
    "            test_metrics['hr'] += hr\n",
    "            test_metrics['ndcg'] += ndcg\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    avg_metrics = {k: test_metrics[k] / len(test_loader) for k in test_metrics}\n",
    "\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba886bb-2fb1-4b54-b55a-fd840069e9f9",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00255ce4-afa2-40b7-bf3b-4c096a67d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 Loss: 0.0023989413436308863: 100%|██████████| 289/289 [13:26<00:00,  2.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.69329527 | Recall: 0.42896044 | Precision: 0.10565025 | F1 Score: 0.16825886 | NDCG: 0.34592104 | HR: 0.81060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 Loss: 0.0023990277600535884: 100%|██████████| 289/289 [13:24<00:00,  2.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.69329481 | Recall: 0.41514676 | Precision: 0.10359849 | F1 Score: 0.16465010 | NDCG: 0.33960685 | HR: 0.80303030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 Loss: 0.0023988353339858535: 100%|██████████| 289/289 [13:24<00:00,  2.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.69329449 | Recall: 0.43037363 | Precision: 0.10587121 | F1 Score: 0.16865488 | NDCG: 0.34688883 | HR: 0.81060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 Loss: 0.002398922575386338: 100%|██████████| 289/289 [13:23<00:00,  2.78s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.69329415 | Recall: 0.43013313 | Precision: 0.10580808 | F1 Score: 0.16855489 | NDCG: 0.34671186 | HR: 0.81060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 Loss: 0.002398948974675373: 100%|██████████| 289/289 [13:22<00:00,  2.78s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.69329381 | Recall: 0.43000161 | Precision: 0.10580808 | F1 Score: 0.16854073 | NDCG: 0.34654420 | HR: 0.81060606\n",
      "Early stopping due to no improvement in validation NDCG.\n",
      "Test Loss: 0.69329337 | Recall: 0.41767001 | Precision: 0.10769033 | F1 Score: 0.17018250 | NDCG: 0.34140455 | HR: 0.81658951\n"
     ]
    }
   ],
   "source": [
    "# 初始化早停機制相關變數\n",
    "best_val_ndcg = -float('inf') \n",
    "patience = 2\n",
    "no_improvement_count = 0\n",
    "best_model_state = None\n",
    "\n",
    "# 訓練循環\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    attention_model.train()\n",
    "    recommendation_model.train()\n",
    "\n",
    "    training_progress_bar = tqdm(training_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    for batch in training_progress_bar:\n",
    "        # 解包 CombinedDataset 返回的数据\n",
    "        ta_feng_data, basket_data = batch\n",
    "        user_vector, neighbor_vector, answer_vector = ta_feng_data\n",
    "        user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = basket_data\n",
    "\n",
    "        # 移动数据到设备\n",
    "        user_vector, neighbor_vector, answer_vector = user_vector.to(device), neighbor_vector.to(device), answer_vector.to(device)\n",
    "        user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates, true_basket_vector = user_basket_sequence.to(device), user_transaction_dates.to(device), neighbor_basket_sequence.to(device), neighbor_transaction_dates.to(device), true_basket_vector.to(device)\n",
    "\n",
    "        # 模型前向传播\n",
    "        output_attention = attention_model(user_vector, neighbor_vector)\n",
    "        output_recommendation = recommendation_model(user_basket_sequence, user_transaction_dates, neighbor_basket_sequence, neighbor_transaction_dates)\n",
    "  \n",
    "        normalized_ans_2 = output_recommendation / torch.sum(output_recommendation)\n",
    "        combined_output = output_attention + normalized_ans_2\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = loss_function(combined_output, answer_vector.float())  # 确保 answer_vector 是 float 类型\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_progress_bar.set_description(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item() / len(training_loader)}\")\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}, Alpha Value: {attention_model.alpha.item()}')\n",
    "\n",
    "    # 在每个epoch结束后进行验证\n",
    "    val_loss, val_metrics = validate_model(\n",
    "        attention_model, recommendation_model, validation_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k)\n",
    "    \n",
    "    tqdm.write(f'Validation Loss: {val_loss:.8f} | Recall: {val_metrics[\"recall\"]:.8f} | Precision: {val_metrics[\"precision\"]:.8f} | F1 Score: {val_metrics[\"f1\"]:.8f} | NDCG: {val_metrics[\"ndcg\"]:.8f} | HR: {val_metrics[\"hr\"]:.8f}')\n",
    "    \n",
    "    if val_metrics['ndcg'] > best_val_ndcg:\n",
    "        best_val_ndcg = val_metrics['ndcg']\n",
    "        no_improvement_count = 0\n",
    "        best_model_state = {\n",
    "            'attention_model': attention_model.state_dict(),\n",
    "            'recommendation_model': recommendation_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # 如果没有改进的计数达到了 patience，则停止训练\n",
    "    if no_improvement_count >= patience:\n",
    "        print(\"Early stopping due to no improvement in validation NDCG.\")\n",
    "        break\n",
    "\n",
    "# 保存最佳模型状态\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, 'Best_Model_PIFTA4Rec.pth')\n",
    "\n",
    "# 加载最佳模型状态\n",
    "best_model_state = torch.load('Best_Model_PIFTA4Rec.pth')\n",
    "attention_model.load_state_dict(best_model_state['attention_model'])\n",
    "recommendation_model.load_state_dict(best_model_state['recommendation_model'])\n",
    "optimizer.load_state_dict(best_model_state['optimizer'])\n",
    "\n",
    "# 在所有训练循环结束后调用测试函数\n",
    "test_loss, test_metrics = test_model(\n",
    "    attention_model, recommendation_model, test_loader, device, loss_function, calculate_topk_metrics, ndcg_score, k)\n",
    "tqdm.write(f'Test Loss: {test_loss:.8f} | Recall: {test_metrics[\"recall\"]:.8f} | Precision: {test_metrics[\"precision\"]:.8f} | F1 Score: {test_metrics[\"f1\"]:.8f} | NDCG: {test_metrics[\"ndcg\"]:.8f} | HR: {test_metrics[\"hr\"]:.8f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pifta",
   "language": "python",
   "name": "pifta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
