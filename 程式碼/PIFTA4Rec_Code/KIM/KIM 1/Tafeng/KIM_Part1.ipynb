{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj0gS7tPsLgF"
   },
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w0UCsEAFpstp"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "import gzip\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGUATq4sRb7"
   },
   "source": [
    "## 參數設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W4U-qVi-rvPQ"
   },
   "outputs": [],
   "source": [
    "activate_codes_num = -1\n",
    "next_k_step = 1\n",
    "training_chunk = 0\n",
    "test_chunk = 1\n",
    "\n",
    "num_nearest_neighbors = 300\n",
    "within_decay_rate = 0.9\n",
    "group_decay_rate = 0.7\n",
    "group_size = 7\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z05PNhc1yOjU"
   },
   "source": [
    "## 生成字典與計算最終向量維度\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7vjhHs4gqmty"
   },
   "outputs": [],
   "source": [
    "def generate_dictionary_BA(files, attributes_list):\n",
    "    dictionary_table = {}\n",
    "    counter_table = {}\n",
    "\n",
    "    # attributes_list 僅有 MATERIAL_NUMBER\n",
    "    for attr in attributes_list:\n",
    "        dictionary = {}\n",
    "        dictionary_table[attr] = dictionary\n",
    "        counter_table[attr] = 0\n",
    "\n",
    "    csv.field_size_limit(128) # 設置 CSV 讀取的最大長度\n",
    "\n",
    "    for filename in files:\n",
    "        count = 0 #用於追蹤當前文件中處理的行數\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|') # 指定逗號（,）為字段分隔符，並將豎線（|）作為引號字符\n",
    "            for row in reader:\n",
    "                if count == 0: # 跳過第一行（標題行）:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                key = attributes_list[0] # 取出第一個屬性，即 MATERIAL_NUMBER\n",
    "                if row[3] not in dictionary_table[key]: # 如果 row[3] 的值還沒有在字典中，則將其添加到字典中，並賦予一個索引\n",
    "                    dictionary_table[key][row[3]] = counter_table[key]\n",
    "                    counter_table[key] = counter_table[key] + 1 # 更新 counter_table 中對應屬性的計數\n",
    "                    count += 1\n",
    "\n",
    "    total = 0\n",
    "    for key in counter_table.keys():\n",
    "        total = total + counter_table[key]\n",
    "\n",
    "    print('# dimensions of final vector: ' + str(total) + ' | '+str(count-1))\n",
    "\n",
    "    return dictionary_table, total, counter_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJbI4awNyeQr"
   },
   "source": [
    "## 從 CSV 文件中讀取數據並處理，將其轉換為特定格式的數據結構\n",
    "\n",
    "**目的：在歷史與未來中，處理每位用戶的所有購物籃，並於前後加入 -1 標記表示開始與結束**\n",
    "\n",
    "```\n",
    "# data_chunk[0][\"2\"] :[[-1], array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,67]), ... array([ 52, 106, 142, 143, 144, 145, 146, 147]), [-1]]\n",
    "# data_chunk[1][\"2\"]:[[-1], array([   5,    9,   50,  125,  145, 1357]), [-1]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vZm66eV2qgTU"
   },
   "outputs": [],
   "source": [
    "# 從 CSV 文件中讀取數據並處理，將其轉換為特定格式的數據結構\n",
    "def read_claim2vector_embedding_file_no_vector(files): #\n",
    "    attributes_list = ['MATERIAL_NUMBER'] # 一個包含要處理的屬性名稱的列表(存了商品編號）\n",
    "    print('----- Start Dictionary Generation -----')\n",
    "    dictionary_table, num_dim, counter_table = generate_dictionary_BA(files, attributes_list) # 生成字典並返回三個值 #files是./data/TaFang_history_NB.csv 跟 ./data/TaFang_future_NB.csv\n",
    "    print('----- Finish Dictionary Generation -----')\n",
    "    usr_attr = 'CUSTOMER_ID'\n",
    "    ord_attr = 'ORDER_NUMBER'\n",
    "    freq_max = 200\n",
    "    data_chunk = [] # 用於儲存處理後數據的列表\n",
    "    day_gap_counter = []\n",
    "    claims_counter = 0\n",
    "    num_claim = 0\n",
    "\n",
    "    # 迭代處理 files 列表中的每個文件，先重新命名 history 之後再 future\n",
    "    for file_id in range(len(files)):\n",
    "        count = 0\n",
    "        data_chunk.append({})\n",
    "        filename = files[file_id] # 打開文件\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile) # 讀取 CSV 文件\n",
    "\n",
    "            # 用於在下面的循環中跟踪上一行的數據\n",
    "            last_pid_date = '*'\n",
    "            last_pid = '-1'\n",
    "            last_days = -1\n",
    "            # 2 more elements in the end for start and end states\n",
    "            feature_vector = []\n",
    "\n",
    "            # 對 CSV 文件中的每一行進行迭代\n",
    "            for row in reader:\n",
    "                cur_pid_date = row[usr_attr] + '_' + row[ord_attr] # 從當前行生成一個唯一的標識符 (1_14)\n",
    "                cur_pid = row[usr_attr] # 並獲取用戶 ID (1)\n",
    "\n",
    "                # 如果當前用戶 ID 不等於上一個用戶 ID，則在 data_chunk 中為這個新用戶創建一個新條目\n",
    "                if cur_pid != last_pid:\n",
    "                    # start state\n",
    "                    tmp = [-1]\n",
    "                    data_chunk[file_id][cur_pid] = []\n",
    "                    data_chunk[file_id][cur_pid].append(tmp)\n",
    "\n",
    "                # 如果當前的 cur_pid_date 不等於 last_pid_date，則對 feature_vector 進行排序並將其添加到 data_chunk 中的相應位置，然後重置 feature_vector\n",
    "                if cur_pid_date not in last_pid_date:\n",
    "                    if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                        sorted_feature_vector = np.sort(feature_vector)\n",
    "                        data_chunk[file_id][last_pid].append(sorted_feature_vector)\n",
    "                        if len(sorted_feature_vector) > 0:\n",
    "                            count = count + 1\n",
    "                    feature_vector = []\n",
    "\n",
    "                    claims_counter = 0\n",
    "                # 如果當前用戶 ID 不等於上一個用戶 ID，則在 data_chunk 中的上一個用戶 ID 下添加一個結束標記\n",
    "                if cur_pid != last_pid:\n",
    "                    # end state\n",
    "                    if last_pid not in '-1':\n",
    "\n",
    "                        tmp = [-1]\n",
    "                        data_chunk[file_id][last_pid].append(tmp)\n",
    "                        #print(data_chunk[file_id][last_pid])\n",
    "\n",
    "                key = attributes_list[0]\n",
    "                within_idx = dictionary_table[key][row[key]]\n",
    "                previous_idx = 0\n",
    "\n",
    "                for j in range(attributes_list.index(key)):\n",
    "                    previous_idx = previous_idx + counter_table[attributes_list[j]]\n",
    "                idx = within_idx + previous_idx\n",
    "\n",
    "                if idx not in feature_vector:\n",
    "                    feature_vector.append(idx)\n",
    "\n",
    "                # 更新 last_pid_date 和 last_pid 值\n",
    "                last_pid_date = cur_pid_date\n",
    "                last_pid = cur_pid\n",
    "                #last_days = cur_days\n",
    "                if file_id == 1:\n",
    "                    claims_counter = claims_counter + 1\n",
    "\n",
    "            # 在循環結束後，將最後一個 feature_vector 添加到 data_chunk 中\n",
    "            if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                data_chunk[file_id][last_pid].append(np.sort(feature_vector))\n",
    "\n",
    "    # #儲存 data_chunk 資料用於查看\n",
    "    # def save_top_20_to_txt(data_chunk, file_index, file_name):\n",
    "    #   with open(file_name, 'w') as f:\n",
    "    #       for key, value in list(data_chunk[file_index].items())[:20]:\n",
    "    #           f.write(f\"{key}: {value}\\n\")\n",
    "    # # save_top_20_to_txt(data_chunk, 0, 'data/data_chunk_0_top20.txt')\n",
    "    # save_top_20_to_txt(data_chunk, 1, 'data/data_chunk_1_top20.txt')\n",
    "\n",
    "    # 正確答案\n",
    "    #np.savez('data/answer.npz', **data_chunk[1])\n",
    "    #print(\"data_chunk[1]:\", data_chunk[1])\n",
    "    print(\"num_dim+2 :\", num_dim + 2)\n",
    "    return data_chunk, num_dim + 2 # 返回處理後的數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FjWQg0papS-"
   },
   "source": [
    "## 分割資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PGH6BvjBZ3q6"
   },
   "outputs": [],
   "source": [
    "def partition_the_data_validate(data_chunk, key_set, next_k_step):\n",
    "    # key_set = ['1', '2', '3', '4', '5', ...'13972'] （即 Future 中所有用戶的 ID）\n",
    "    # next_k_step = 1\n",
    "    print('----- Start Splitting Data Set -----')\n",
    "\n",
    "    filtered_key_set = [] # 用於儲存經過篩選後的 key\n",
    "    past_chunk = 0\n",
    "    future_chunk = 1\n",
    "\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[past_chunk][key]) <= 3:\n",
    "            continue\n",
    "        if len(data_chunk[future_chunk][key]) < 2 + next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set)*0.9)]\n",
    "    validation_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)*0.9):int(4 / 5 * len(filtered_key_set))]\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    print('Number of validation instances: ' + str(len(validation_key_set)))\n",
    "    print('Number of test instances: ' + str(len(test_key_set)))\n",
    "\n",
    "    # print('Training Key Set: ',training_key_set)\n",
    "\n",
    "    print(\"training_key_set[0]: \", training_key_set[0])\n",
    "    print(\"validation_key_set[0]: \",validation_key_set[0])\n",
    "    print(\"test_key_set[0]: \",test_key_set[0])\n",
    "    print('----- Finish splitting the data set -----')\n",
    "\n",
    "    return training_key_set, validation_key_set, test_key_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjLqYlpEeX94"
   },
   "source": [
    "## 分群計算，獲得所有群組的平均向量\n",
    "\n",
    "**將一系列的歷史數據向量按給定的群組大小分組，並計算每個群組中的向量的平均值**\n",
    "\n",
    "假設 his_list 長度為 10，group_size 為 3，\\\n",
    "est_num_vec_each_block = 10 / 3 ≈ 3.33（每個群組中平均應有大約 3.33 個向量）\\\n",
    "base_num_vec_each_block = 3.33 向下取整 = 3（每個群組至少應有 3 個向量）\\\n",
    "residual = 3.33 - 3 = 0.33 （差異）\\\n",
    "num_vec_has_extra_vec = 1（有 1 個群組將比其他群組多一個向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QFdfwk0beT9u"
   },
   "outputs": [],
   "source": [
    "def group_history_list(his_list,group_size):\n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        return grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "        return grouped_vec_list, group_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beTNBmbjA7lY"
   },
   "source": [
    "## 整合所有群組的平均向量，獲得用戶歷史向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IB0bsR8qCBvh"
   },
   "outputs": [],
   "source": [
    "def temporal_decay_sum_history(data_set, key_set, output_size,group_size,within_decay_rate,group_decay_rate):\n",
    "\n",
    "    print('Temporal Decay Sum History ...')\n",
    "\n",
    "    sum_history = {}\n",
    "\n",
    "    for key in key_set:\n",
    "\n",
    "        vec_list = data_set[key]\n",
    "        num_vec = len(vec_list) - 2\n",
    "        his_list = []\n",
    "\n",
    "        for idx in range(1,num_vec+1):\n",
    "            his_vec = np.zeros(output_size)\n",
    "            decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "            for ele in vec_list[idx]:\n",
    "                his_vec[ele] = decayed_val\n",
    "            his_list.append(his_vec) # 包含了用戶所有交易的時間衰減向量\n",
    "\n",
    "        grouped_list,real_group_size = group_history_list(his_list,group_size)\n",
    "        his_vec = np.zeros(output_size)\n",
    "        for idx in range(real_group_size):\n",
    "            decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "            if idx>=len(grouped_list):\n",
    "                print( 'idx: '+ str(idx))\n",
    "                print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "            his_vec += grouped_list[idx]*decayed_val\n",
    "        sum_history[key] = his_vec/real_group_size\n",
    "    return sum_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkx98pbXQ8fD"
   },
   "source": [
    "## 取得鄰居向量\n",
    "\n",
    "測試集、訓練集、驗證集分開處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y1Lq6_NrRVyR"
   },
   "outputs": [],
   "source": [
    "# data_chunk、測試集用戶向量、測試集、訓練集用戶向量、訓練集、鄰居索引\n",
    "\n",
    "def test_get_neighbors_vectors(data_chunk, temporal_decay_sum_history_test,test_key_set,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"（Test）Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for test_key_id in range(len(test_key_set)):\n",
    "        test_key = test_key_set[test_key_id]\n",
    "        test_history = temporal_decay_sum_history_test[test_key] # 真向量\n",
    "        #target_set.append(data_chunk[test_chunk][test_key_set[test_key_id]]) # [[-1], array([ 270,  669, 1653, 1886, 1987, 4718, 6677]), [-1]]\n",
    "        #target_set.append((test_key, data_chunk[test_chunk][test_key][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][test_key][1]\n",
    "        answer_vector = np.zeros(len(test_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((test_key, data_chunk[test_chunk][test_key][1],answer_tensor))\n",
    "\n",
    "\n",
    "        sum_training_history = np.zeros(len(test_history))\n",
    "\n",
    "        for indecis in index[test_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[test_key_id]) #真向量\n",
    "        user_neighbor_set.append((test_key, test_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "        print(f\"Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "        print(f\"Target Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_test_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "      pickle.dump(user_neighbor_set, fp)\n",
    "      print(\"The test set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_test_answer.gz', 'wb') as fp:\n",
    "      pickle.dump(target_set, fp)\n",
    "      print(\"The test set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LbiCmRvbMCze"
   },
   "outputs": [],
   "source": [
    "def validation_get_neighbors_vectors(data_chunk, temporal_decay_sum_history_validation,validation_key_set,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"(Validation) Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for validation_key_id in range(len(validation_key_set)):\n",
    "        validation_key = validation_key_set[validation_key_id]\n",
    "        validation_history = temporal_decay_sum_history_validation[validation_key] # 真向量\n",
    "        #target_set.append(validation_key, data_chunk[test_chunk][validation_key_set[validation_key_id]][1]) # 正確答案[[-1], array([ 270,  669, 1653, 1886, 1987, 4718, 6677]), [-1]]\n",
    "        #target_set.append((validation_key, data_chunk[test_chunk][training_key_set[validation_key_id]][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][validation_key][1]\n",
    "        answer_vector = np.zeros(len(validation_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((validation_key, data_chunk[test_chunk][validation_key][1],answer_tensor))\n",
    "\n",
    "        sum_training_history = np.zeros(len(validation_history))\n",
    "\n",
    "        for indecis in index[validation_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[validation_key_id]) #真向量\n",
    "        user_neighbor_set.append((validation_key, validation_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    # 打印 user_neighbor_set 的前 20 个条目\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "        print(f\"User Neighbor Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    # 打印 target_set 的前 20 个条目\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "        print(f\"Target Item {i}: {item}\")\n",
    "        print()\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_validation_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "      pickle.dump(user_neighbor_set, fp)\n",
    "      print(\"The validation set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_validation_answer.gz', 'wb') as fp:\n",
    "      pickle.dump(target_set, fp)\n",
    "      print(\"The validation set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t8tuCVLwSIpW"
   },
   "outputs": [],
   "source": [
    "def training_get_neighbors_vectors(data_chunk,temporal_decay_sum_history_training,training_key_set,index):\n",
    "\n",
    "    print(\"(Training) Get the neighbor vector of each user ...\")\n",
    "    # history_dict = {}  # 用於保存 test_history 和 sum_training_history 的關聯\n",
    "    user_neighbor_set = []  # 初始化一個空列表來存儲數據\n",
    "    target_set = []\n",
    "\n",
    "    for training_key_id in range(len(training_key_set)):\n",
    "        training_key = training_key_set[training_key_id]\n",
    "        training_history = temporal_decay_sum_history_training[training_key] # 真向量\n",
    "        #target_set.append((training_key, data_chunk[test_chunk][training_key_set[training_key_id]][1]))\n",
    "        #target_set.append((training_key, data_chunk[test_chunk][training_key][1]))\n",
    "\n",
    "        answer = data_chunk[test_chunk][training_key][1]\n",
    "        answer_vector = np.zeros(len(training_history))\n",
    "\n",
    "        # 將目標變量中的元素對應的位置設為 1\n",
    "        for ii in answer:\n",
    "            answer_vector[ii] = 1\n",
    "        answer_tensor = torch.from_numpy(answer_vector)\n",
    "        target_set.append((training_key, data_chunk[test_chunk][training_key][1],answer_tensor))\n",
    "\n",
    "        sum_training_history = np.zeros(len(training_history))\n",
    "\n",
    "        for indecis in index[training_key_id]:\n",
    "            training_key2 = training_key_set[indecis]\n",
    "            sum_training_history += temporal_decay_sum_history_training[training_key2]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[training_key_id]) #真向量\n",
    "        user_neighbor_set.append((training_key, training_history, sum_training_history)) # 特定用戶向量與其對應的鄰居向量，Ex. ([0.5, 0.2, 0.8], [0.3, 0.37, 0.37])\n",
    "\n",
    "    for i, item in enumerate(user_neighbor_set[:20]):\n",
    "      print(f\"Item {i}: {item}\")\n",
    "      print()\n",
    "\n",
    "    for i, item in enumerate(target_set[:20]):\n",
    "      print(f\"Target Item {i}: {item}\")\n",
    "      print()\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_training_user_and_neighbor_set.gz', 'wb') as fp:\n",
    "      pickle.dump(user_neighbor_set, fp)\n",
    "      print(\"The Training set user vectors and neighbor vectors have been saved !\")\n",
    "\n",
    "    with gzip.GzipFile('preprocessing-data/TaFeng_training_answer.gz', 'wb') as fp:\n",
    "      pickle.dump(target_set, fp)\n",
    "      print(\"The Training set answer have been saved !\")\n",
    "\n",
    "    return user_neighbor_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9qLmeZf02WC"
   },
   "source": [
    "## KNN 尋找相似鄰居\n",
    "\n",
    "對數據集中的數據行最近鄰居搜索，返回每個測試點的最近鄰居索引（indices）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5bpNdSm107E4"
   },
   "outputs": [],
   "source": [
    "def KNN(query_set, target_set, k):\n",
    "\n",
    "    print(\"Start Looking for Neighbors ...\")\n",
    "\n",
    "    history_mat = []\n",
    "    for key in target_set.keys():\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    query_keys = list(query_set.keys())  # 保存查詢鍵列表\n",
    "    for key in query_set.keys():\n",
    "        test_mat.append(query_set[key])\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "\n",
    "    print(len(test_mat))\n",
    "    #print(indices[0]) #[[3305 2654 ... 696][1313 ... 5668 453]]\n",
    "    #print('Index :',indices[0][0])\n",
    "    #print('Distances:',distances)\n",
    "    print(\"Finish Looking for Neighbors !\")\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QECZBpXM-y5Y"
   },
   "outputs": [],
   "source": [
    "def KNN_training(query_set, target_set, k):\n",
    "\n",
    "    print(\"Start Looking for Neighbors ...\")\n",
    "    keys_list = list(target_set.keys())\n",
    "    #print(keys_list)\n",
    "\n",
    "    history_mat = []\n",
    "    for key in target_set.keys(): # key 是 User ID\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    query_keys = list(query_set.keys())  # 保存查詢鍵列表\n",
    "    for key in query_set.keys():\n",
    "        test_mat.append(query_set[key])\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "    all_filtered_indices = []  # 用于存储所有查询点的过滤后的邻居索引\n",
    "\n",
    "    for i, key in enumerate(query_keys):\n",
    "        index_to_remove = keys_list.index(key)  # 获取要删除的索引\n",
    "        filtered_indices = [index for index in indices[i] if index != index_to_remove]\n",
    "        all_filtered_indices.append(filtered_indices)\n",
    "\n",
    "    print( all_filtered_indices[0])\n",
    "    print('Index :',all_filtered_indices[0][0])\n",
    "\n",
    "    print(\"Finish Looking for Neighbors !\")\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z69F5fpj5v7q"
   },
   "source": [
    "## 取得用戶向量與對應的鄰居向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ROKy64mYBB-w"
   },
   "outputs": [],
   "source": [
    "def get_user_neighbor_vectors(data_chunk, training_key_set, test_key_set, validation_key_set, num_nearest_neighbors,temporal_decay_sum_history_test, temporal_decay_sum_history_training,temporal_decay_sum_history_validation):\n",
    "\n",
    "    # 獲得每位用戶的鄰居索引\n",
    "    index_test = KNN(temporal_decay_sum_history_test, temporal_decay_sum_history_training,num_nearest_neighbors)\n",
    "    index_validation = KNN(temporal_decay_sum_history_validation, temporal_decay_sum_history_training,num_nearest_neighbors)\n",
    "    index_training = KNN_training(temporal_decay_sum_history_training, temporal_decay_sum_history_training,num_nearest_neighbors)\n",
    "\n",
    "    # 取得每位用戶與鄰居向量\n",
    "    test_user_neighbor_set = test_get_neighbors_vectors(data_chunk,temporal_decay_sum_history_test, test_key_set, temporal_decay_sum_history_training, training_key_set, index_test)\n",
    "    validation_user_neighbor_set = validation_get_neighbors_vectors(data_chunk,temporal_decay_sum_history_validation, validation_key_set, temporal_decay_sum_history_training, training_key_set, index_validation)\n",
    "    training_user_neighbor_set = training_get_neighbors_vectors(data_chunk, temporal_decay_sum_history_training, training_key_set, index_training)\n",
    "\n",
    "    status = \"OK\"\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVuhDzRl-L-m"
   },
   "source": [
    "# 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4RxXu8pHjTJI"
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_Fscore(groundtruth,pred):\n",
    "    a = groundtruth\n",
    "    b = pred\n",
    "    correct = 0\n",
    "    truth = 0\n",
    "    positive = 0\n",
    "\n",
    "    for idx in range(len(a)):\n",
    "        if a[idx] == 1:\n",
    "            truth += 1\n",
    "            if b[idx] == 1:\n",
    "                correct += 1\n",
    "        if b[idx] == 1:\n",
    "            positive += 1\n",
    "\n",
    "    flag = 0\n",
    "    if 0 == positive:\n",
    "        precision = 0\n",
    "        flag = 1\n",
    "        #print('postivie is 0')\n",
    "    else:\n",
    "        precision = correct/positive\n",
    "    if 0 == truth:\n",
    "        recall = 0\n",
    "        flag = 1\n",
    "        #print('recall is 0')\n",
    "    else:\n",
    "        recall = correct/truth\n",
    "\n",
    "    if flag == 0 and precision + recall > 0:\n",
    "        F = 2*precision*recall/(precision+recall)\n",
    "    else:\n",
    "        F = 0\n",
    "    return precision, recall, F, correct\n",
    "\n",
    "def get_HT(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            return 1\n",
    "        count += 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_NDCG1(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1)/math.log2(count+1+1)\n",
    "        count += 1\n",
    "    idcg = 0\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "    for i in range(num_item):\n",
    "        idcg += (1) / math.log2(i + 1 + 1)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY0Eqecl-VZJ"
   },
   "source": [
    "# 主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzBIvnv_pt_P",
    "outputId": "5047e5a4-19b4-4ef9-fbf8-e36e1f6ac09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Start Dictionary Generation -----\n",
      "# dimensions of final vector: 12085 | 2\n",
      "----- Finish Dictionary Generation -----\n",
      "num_dim+2 : 12087\n",
      "----- Start Splitting Data Set -----\n",
      "Number of training instances: 10059\n",
      "Number of validation instances: 1117\n",
      "Number of test instances: 2795\n",
      "training_key_set[0]:  1069\n",
      "validation_key_set[0]:  1858204\n",
      "test_key_set[0]:  1974492\n",
      "----- Finish splitting the data set -----\n",
      "Temporal Decay Sum History ...\n",
      "Temporal Decay Sum History ...\n",
      "Temporal Decay Sum History ...\n",
      "Start Looking for Neighbors ...\n",
      "2795\n",
      "Finish Looking for Neighbors !\n",
      "Start Looking for Neighbors ...\n",
      "1117\n",
      "Finish Looking for Neighbors !\n",
      "Start Looking for Neighbors ...\n",
      "[2346, 9895, 8336, 8693, 3156, 2033, 9877, 9934, 7369, 1063, 7266, 131, 9854, 742, 6687, 9853, 9922, 8283, 9883, 1196, 2351, 7985, 9892, 1577, 9940, 3911, 9800, 2894, 9930, 9033, 9842, 7557, 6484, 4925, 2213, 1964, 6610, 3386, 7818, 5420, 9900, 243, 9885, 9929, 9927, 2519, 8385, 3113, 1416, 7551, 5246, 5310, 9928, 9905, 9882, 9856, 349, 9850, 9943, 1473, 9872, 8197, 6634, 9312, 2438, 9846, 4223, 8536, 65, 9869, 8670, 9858, 9897, 2720, 5811, 373, 5999, 316, 3655, 8705, 8619, 3229, 2707, 5395, 357, 2739, 1802, 1197, 9829, 9741, 4183, 4120, 1172, 224, 2312, 2567, 219, 7688, 1091, 5995, 4155, 9429, 3463, 5826, 3960, 95, 3291, 3656, 3329, 6386, 1746, 9196, 4467, 807, 5053, 4466, 4827, 6686, 1136, 746, 4895, 7372, 8508, 5348, 2995, 10056, 4916, 5621, 6301, 4851, 6664, 4548, 7532, 9374, 5505, 404, 1594, 717, 1293, 1818, 3252, 2278, 4452, 8800, 838, 762, 3100, 1609, 2505, 8905, 1249, 1770, 4893, 8546, 1592, 8712, 9913, 645, 3717, 2093, 3298, 2003, 1856, 7706, 3679, 2357, 2328, 8691, 1945, 6431, 9649, 8245, 808, 9715, 9932, 6306, 4231, 6636, 2977, 7817, 2236, 2393, 9079, 431, 6768, 5727, 1087, 7665, 5259, 4549, 8328, 6260, 9572, 2311, 7276, 3410, 1760, 6268, 1294, 3286, 2350, 2203, 1375, 1559, 9848, 6378, 3138, 9942, 7392, 9901, 358, 1633, 3255, 2391, 2753, 7617, 9590, 8379, 796, 371, 1792, 5176, 4403, 5679, 4997, 3801, 9921, 1513, 1175, 2895, 4736, 2192, 30, 4632, 8403, 5392, 36, 6964, 426, 5567, 3296, 9870, 9845, 9145, 7866, 9936, 4371, 3757, 3658, 608, 6475, 5013, 947, 9263, 7535, 8092, 531, 9657, 588, 3301, 413, 9962, 204, 3697, 6283, 9886, 4070, 9935, 9506, 3619, 9457, 1480, 7794, 9021, 9645, 2583, 9387, 8428, 9348, 1432, 9131, 429, 3016, 8734, 2111, 8220, 9568, 195, 5129, 6968, 6140, 9115, 8133, 2763, 4856, 8792, 6981, 3369, 56, 3319]\n",
      "Index : 2346\n",
      "Finish Looking for Neighbors !\n",
      "（Test）Get the neighbor vector of each user ...\n",
      "Item 0: ('1974492', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 1.06850142e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 1: ('1974584', array([0., 0., 0., ..., 0., 0., 0.]), array([4.86558909e-05, 2.47509577e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 2: ('1974690', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 3: ('1974843', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 4: ('1974867', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 5: ('1974881', array([0., 0., 0., ..., 0., 0., 0.]), array([1.77897531e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 6: ('1974973', array([0., 0., 0., ..., 0., 0., 0.]), array([1.85579283e-04, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 7: ('1974980', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 8: ('1975024', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 9: ('1975048', array([0., 0., 0., ..., 0., 0., 0.]), array([1.85859753e-04, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 10: ('1975413', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 11: ('1975420', array([0., 0., 0., ..., 0., 0., 0.]), array([3.03154639e-05, 1.89836040e-05, 1.06850142e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 12: ('1975543', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 13: ('1975567', array([0., 0., 0., ..., 0., 0., 0.]), array([4.37636354e-05, 1.89836040e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 14: ('1975765', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 15: ('1975833', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 16: ('1975895', array([0., 0., 0., ..., 0., 0., 0.]), array([1.77897531e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 17: ('1976014', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 1.98045000e-04, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 18: ('1976144', array([0., 0., 0., ..., 0., 0., 0.]), array([1.77897531e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 19: ('1976236', array([0., 0., 0., ..., 0., 0., 0.]), array([1.77897531e-05, 2.68303438e-05, 1.06850142e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Target Item 0: ('1974492', array([195]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 1: ('1974584', array([  17,  328, 2083]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 2: ('1974690', array([  257,   658,  2133,  3755,  5897, 11252]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 3: ('1974843', array([670]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 4: ('1974867', array([8331]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 5: ('1974881', array([ 333, 1467, 3676, 3700, 6265]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 6: ('1974973', array([  10,  129, 2635, 3656]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 7: ('1974980', array([  129,   161,   202,   587,   642,   708,   957,  3449,  4698,\n",
      "        6615,  7573,  8408,  9448, 10140]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 8: ('1975024', array([ 887, 2139, 3164, 4140, 5897, 7859]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 9: ('1975048', array([1095]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 10: ('1975413', array([  16,  247,  254,  534,  583,  893, 6865]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 11: ('1975420', array([ 327, 3373, 6849, 8511]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 12: ('1975543', array([ 185,  497, 4718]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 13: ('1975567', array([  76,  709, 1317, 1447, 2157, 2232, 2496, 2856, 3408, 4138, 4140,\n",
      "       4698, 7647, 7864]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 14: ('1975765', array([ 678, 5365]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 15: ('1975833', array([ 238,  541, 5911]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 16: ('1975895', array([  190,   250,   413,   434,   451,   741,  1178,  1302,  1656,\n",
      "        1952,  2133,  2144,  2255,  2391,  2424,  3469,  3685,  3689,\n",
      "        3704,  9182, 10794, 11808]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 17: ('1976014', array([  419,   533,  1230,  2039,  2166,  3176,  4360,  4790,  6213,\n",
      "        7834,  9838, 11975]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 18: ('1976144', array([  51,  117,  254,  412,  433,  434,  437,  452, 1069, 1390, 1444,\n",
      "       1687, 1693, 1776, 2024, 2029, 2030, 2084, 2322, 2466, 2731, 3015,\n",
      "       4698, 5258, 5975, 6440, 7283, 8330]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 19: ('1976236', array([1031, 5065]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "The test set user vectors and neighbor vectors have been saved !\n",
      "The test set answer have been saved !\n",
      "(Validation) Get the neighbor vector of each user ...\n",
      "User Neighbor Item 0: ('1858204', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 1.06850142e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 1: ('1858211', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 2: ('1858273', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 3: ('1858303', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 4: ('1858358', array([0., 0., 0., ..., 0., 0., 0.]), array([2.62278827e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 5: ('1858372', array([0., 0., 0., ..., 0., 0., 0.]), array([5.74348982e-05, 2.47509577e-05, 1.98045000e-04, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 6: ('1858440', array([0., 0., 0., ..., 0., 0., 0.]), array([6.83924208e-05, 1.89836040e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 7: ('1858457', array([0., 0., 0., ..., 0., 0., 0.]), array([3.90340637e-05, 1.89836040e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 8: ('1858471', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 9: ('1858501', array([0., 0., 0., ..., 0., 0., 0.]), array([1.89090527e-04, 1.89836040e-05, 1.99113501e-04, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 10: ('1858556', array([0., 0., 0., ..., 0., 0., 0.]), array([0.00020298, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ]))\n",
      "\n",
      "User Neighbor Item 11: ('1858761', array([0., 0., 0., ..., 0., 0., 0.]), array([4.80096586e-04, 7.84673981e-06, 1.06850142e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 12: ('1859263', array([0., 0., 0., ..., 0., 0., 0.]), array([3.03154639e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 13: ('1859270', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 14: ('1859300', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 15: ('1859324', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 16: ('1859539', array([0., 0., 0., ..., 0., 0., 0.]), array([5.74361141e-05, 1.89836040e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 17: ('1859546', array([0., 0., 0., ..., 0., 0., 0.]), array([1.85859753e-04, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 18: ('1859560', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "User Neighbor Item 19: ('1859607', array([0., 0., 0., ..., 0., 0., 0.]), array([1.75092829e-05, 7.84673981e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Target Item 0: ('1858204', array([ 980, 1648, 2427, 7354]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 1: ('1858211', array([  862,  1263,  4339, 11369]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 2: ('1858273', array([1315, 2377, 2580, 4244, 4887, 6035, 6524]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 3: ('1858303', array([  140, 11355, 11448]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 4: ('1858358', array([1886, 3108, 3254, 3331, 3713]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 5: ('1858372', array([5630]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 6: ('1858440', array([  595,   911,  3151,  3936, 11797]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 7: ('1858457', array([ 416,  506,  601,  905, 1341, 2854, 4064]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 8: ('1858471', array([ 1172,  1203,  1231,  1471,  1641,  2749,  4137,  4515, 10479,\n",
      "       10873]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 9: ('1858501', array([ 398,  418,  549,  856,  987, 1272, 1474, 3009, 3175, 4674]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 10: ('1858556', array([  51,   68,  660,  788,  969, 1090, 1559, 3346, 4093, 4096, 5881,\n",
      "       7254]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 11: ('1858761', array([ 810, 1245, 1871, 2540, 2542, 6958, 8591, 8872, 9503]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 12: ('1859263', array([ 129,  627,  660, 2253]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 13: ('1859270', array([2712, 4259, 8378]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 14: ('1859300', array([  17,  873, 1169, 1227, 1818, 2469, 6254, 7058]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 15: ('1859324', array([ 153,  795,  865, 1339, 3600, 3864, 4674]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 16: ('1859539', array([ 328, 1391]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 17: ('1859546', array([  145,   434,   450,   576,  1178,  1522,  1607,  1735,  4812,\n",
      "       10597]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 18: ('1859560', array([2029, 2401, 3363, 8476]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 19: ('1859607', array([  73,  931, 2101, 9820]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "The validation set user vectors and neighbor vectors have been saved !\n",
      "The validation set answer have been saved !\n",
      "(Training) Get the neighbor vector of each user ...\n",
      "Item 0: ('1069', array([0.03176523, 0.11179856, 0.050421  , ..., 0.        , 0.        ,\n",
      "       0.        ]), array([0.00013167, 0.00039817, 0.00016751, ..., 0.        , 0.        ,\n",
      "       0.        ]))\n",
      "\n",
      "Item 1: ('1113', array([0., 0., 0., ..., 0., 0., 0.]), array([2.18854329e-05, 7.82067091e-06, 1.06495158e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 2: ('1823', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 3: ('3667', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 4: ('5241', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 2.67412064e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 5: ('5517', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 1.98451995e-04, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 6: ('6668', array([0., 0., 0., ..., 0., 0., 0.]), array([2.61407469e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 7: ('7795', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 1.06495158e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 8: ('10801', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 0.00000000e+00, 1.06495158e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 9: ('11235', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 10: ('11914', array([0., 0., 0., ..., 0., 0., 0.]), array([1.68304244e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 11: ('12249', array([0., 0., 0., ..., 0., 0., 0.]), array([1.77306510e-05, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 12: ('13697', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 1.06495158e-06, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 13: ('15424', array([0., 0., 0., ..., 0., 0., 0.]), array([3.02334833e-04, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 14: ('16766', array([0., 0., 0., ..., 0., 0., 0.]), array([1.71099628e-05, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 15: ('18173', array([0., 0., 0., ..., 0., 0., 0.]), array([2.61407469e-05, 1.89205355e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 16: ('18524', array([0., 0., 0., ..., 0., 0., 0.]), array([8.14079002e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 17: ('19545', array([0., 0., 0., ..., 0., 0., 0.]), array([1.74511125e-05, 7.82067091e-06, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 18: ('19750', array([0., 0., 0., ..., 0., 0., 0.]), array([8.14079002e-06, 1.89205355e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))\n",
      "\n",
      "Item 19: ('19873', array([0.03403417, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ]), array([0.00041868, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ]))\n",
      "\n",
      "Target Item 0: ('1069', array([445]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 1: ('1113', array([ 434, 1541, 1552, 2232, 3458, 3912]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 2: ('1823', array([ 254, 8930, 9500]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 3: ('3667', array([   5, 8045]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 4: ('5241', array([ 359,  780,  796, 1772, 7183]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 5: ('5517', array([   10,    16,    51,    83,    85,    88,    91,    95,    96,\n",
      "         109,   192,   320,   546,  1014,  1245,  2133,  2232,  2393,\n",
      "        2456,  2797,  3151,  3373,  3469,  3644,  3656,  4491,  4579,\n",
      "        4685,  4774,  5549,  5851,  7644,  8495,  8568,  9823, 10714,\n",
      "       11185, 11308]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 6: ('6668', array([ 136,  158,  202, 1455, 3311, 4102]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 7: ('7795', array([ 162,  190, 1505, 1698, 2073, 2510, 3745, 4742, 9304]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 8: ('10801', array([ 244,  272, 4737]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 9: ('11235', array([  492,  1910,  2085,  9069, 10563]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 10: ('11914', array([ 887, 1603]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 11: ('12249', array([ 183,  190,  419,  486,  488,  813, 1587, 1974, 2926, 4057, 4203,\n",
      "       4418, 4495, 4688, 5536, 5930]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 12: ('13697', array([  22,  286,  294,  314,  325,  328,  387,  557,  771, 1003, 1015,\n",
      "       1119, 1667, 2004, 2257, 2280, 2496, 2666, 2698, 3043, 3152, 3340,\n",
      "       4143, 5240, 5647, 6406, 8193]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 13: ('15424', array([   23,   130,   189,  1179,  1974,  4688,  5770,  9706, 10277]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 14: ('16766', array([129]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 15: ('18173', array([1414, 6849]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 16: ('18524', array([ 848, 2167, 6589]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 17: ('19545', array([ 542, 2040, 2977]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 18: ('19750', array([ 280, 1186, 2870, 3278, 3392, 4887, 5232]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "Target Item 19: ('19873', array([ 494,  842, 1536, 3245, 4260, 7015, 7463]), tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64))\n",
      "\n",
      "The Training set user vectors and neighbor vectors have been saved !\n",
      "The Training set answer have been saved !\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "files = ['./cleaned_dataset/TaFeng_history.csv', './cleaned_dataset/TaFeng_future.csv'] # 歷史與未來的兩個檔案\n",
    "\n",
    "# 讀取和處理兩個檔案\n",
    "# data_chunk 長度為 2，其保存在歷史與未來中，對於每位用戶的所有購物籃。[0] 即為 history，data_chunk[1] 即為 future\n",
    "data_chunk, input_size = read_claim2vector_embedding_file_no_vector(files)\n",
    "training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[test_chunk]), 1) # 將數據分為訓練、驗證和測試集\n",
    "\n",
    "# 取得用戶向量\n",
    "temporal_decay_sum_history_training = temporal_decay_sum_history(data_chunk[training_chunk],training_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "temporal_decay_sum_history_test = temporal_decay_sum_history(data_chunk[training_chunk],test_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "temporal_decay_sum_history_validation = temporal_decay_sum_history(data_chunk[training_chunk],validation_key_set, input_size,group_size, within_decay_rate,group_decay_rate)\n",
    "\n",
    "# # 取得所有用戶向量與其對應的鄰居向量\n",
    "status = get_user_neighbor_vectors(data_chunk, training_key_set, test_key_set,validation_key_set,num_nearest_neighbors,temporal_decay_sum_history_test, temporal_decay_sum_history_training,temporal_decay_sum_history_validation)\n",
    "\n",
    "print(status)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iCGUATq4sRb7",
    "z05PNhc1yOjU",
    "mjLqYlpEeX94",
    "beTNBmbjA7lY",
    "SVuhDzRl-L-m"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pifta",
   "language": "python",
   "name": "pifta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
