{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fa94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "\n",
    "\n",
    "# 忽略特定類型的警告\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"ChainedAssignmentError\")\n",
    "warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da673c2d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfeb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUS = \"0,1,5,3,2,4\"\n",
    "REORDER = False\n",
    "RAW_DATA_DIR = '../../cleaned_dataset/'\n",
    "FEAT_DATA_DIR = './tmp/feat/'\n",
    "DREAM_MODEL_DIR = './tmp/dream/'\n",
    "\n",
    "if not os.path.exists(FEAT_DATA_DIR):\n",
    "    os.makedirs(FEAT_DATA_DIR)\n",
    "if not os.path.exists(DREAM_MODEL_DIR):\n",
    "    os.makedirs(DREAM_MODEL_DIR)\n",
    "\n",
    "DREAM_CONFIG = {'basket_pool_type': 'max', # 'avg'\n",
    "                'rnn_layers': 2,  # TaFeng:2, Dunnhumby:2\n",
    "                'rnn_type': 'RNN_RELU',  #'RNN_TANH',#'GRU',#'LSTM',# 'RNN_RELU', TaFeng: RNN_RELU, Dunnhumby: ,Instacart:\n",
    "                'dropout': 0.5,\n",
    "                'num_product': 3003+1, # TaFeng:12085+1, Dunnhumby:3003+1\n",
    "                'none_idx':  3003, # TaFeng:12085, Dunnhumby:3003\n",
    "                'embedding_dim': 128, # TaFeng: 128 \n",
    "                'cuda': torch.cuda.is_available(), # True,\n",
    "                'clip': 200,  # 0.25\n",
    "                'epochs': 40,   # 40\n",
    "                'batch_size': 32, # TaFeng: 32, Dunnhumby: 32\n",
    "                'learning_rate': 0.001, # TaFeng: 0.001, Dunnhumby: 0.001\n",
    "                'log_interval': 2, # num of batchs between two logging\n",
    "                'data_name': \"Dunnhumby\", # 資料集名稱  TaFeng, Dunnhumby\n",
    "                'train_times': 1, # 訓練次數\n",
    "                # 'checkpoint_dir': DREAM_MODEL_DIR + '/{data_name}/' + '\\dream-{epoch:02d}-{loss:.4f}.model',\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592dbf2",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c809072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " configure DREAM\n",
    " \n",
    " @TODO: REFACTOR config class\n",
    "\"\"\"\n",
    "       \n",
    "class Config(object):\n",
    "    def __init__(self, config):\n",
    "        self.cuda = config['cuda']\n",
    "        self.clip = config['clip']\n",
    "        self.epochs = config['epochs']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.learning_rate = config['learning_rate'] # Initial Learning Rate\n",
    "        self.log_interval = config['log_interval']\n",
    "        \n",
    "        self.none_idx = config['none_idx']\n",
    "        # self.pn_pair_num = config['pn_pair_num']\n",
    "        self.basket_pool_type = config['basket_pool_type']\n",
    "        self.rnn_type = config['rnn_type']\n",
    "        self.rnn_layer_num = config['rnn_layers']\n",
    "        self.dropout = config['dropout']\n",
    "        self.num_product = config['num_product'] # 商品数目，用于定义Embedding Layer\n",
    "        self.embedding_dim = config['embedding_dim'] # 商品表述维数， 用于定义Embedding Layer\n",
    "        self.data_name = config['data_name']\n",
    "        self.train_times = config['train_times']\n",
    "        # self.checkpoint_dir = config['checkpoint_dir']\n",
    "\n",
    "        self.alias = '{}{}_{}_{}_{}_clip{}'.format(self.rnn_layer_num,\n",
    "                                                   self.rnn_type,\n",
    "                                                   self.embedding_dim,\n",
    "                                                   self.learning_rate,\n",
    "                                                   self.batch_size,\n",
    "                                                   self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72dea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bced53",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a7bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    Useful Functions\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import constants\n",
    "# from config import Config\n",
    "# dr_config = Config(constants.DREAM_CONFIG)\n",
    "dr_config = Config(DREAM_CONFIG)\n",
    "\n",
    "################### PADDING & BATCH\n",
    "def pad(tensor, length):\n",
    "    '''\n",
    "        pad 1st dim\n",
    "        remain 0th 2nd dim\n",
    "    '''\n",
    "    #pdb.set_trace()\n",
    "    return torch.cat([tensor, tensor.new(tensor.size(0), length - tensor.size(1), *tensor.size()[2:]).zero_()], 1)\n",
    "\n",
    "def sort_batch_of_lists(batch_of_lists, lens, uids, rbks = None, ihis = None):\n",
    "    '''\n",
    "        sort batch of lists according to len(list)\n",
    "        descending\n",
    "    '''\n",
    "    sorted_idx = [i[0] for i in sorted(enumerate(lens), key = lambda x : x[1], reverse = True)]\n",
    "    uids = [uids[i] for i in sorted_idx]\n",
    "    lens = [lens[i] for i in sorted_idx]\n",
    "    batch_of_lists = [batch_of_lists[i] for i in sorted_idx]\n",
    "    if rbks is not None and ihis is not None:\n",
    "        rbks = [rbks[i] for i in sorted_idx]\n",
    "        ihis = [ihis[i] for i in sorted_idx]\n",
    "        return batch_of_lists, lens, uids, rbks, ihis\n",
    "    else:\n",
    "        return batch_of_lists, lens, uids\n",
    "\n",
    "def pad_batch_of_lists(batch_of_lists, max_len):\n",
    "    '''\n",
    "        pad batch of lists\n",
    "    '''\n",
    "    padded = [l + [[dr_config.none_idx]] * (max_len - len(l)) for l in batch_of_lists]\n",
    "    return padded\n",
    "    \n",
    "def batchify(data, batch_size, is_reordered = False):\n",
    "    '''\n",
    "        turn dataset into iterable batches\n",
    "    '''\n",
    "    if is_reordered is True:\n",
    "        num_batch = len(data) // batch_size\n",
    "        for i in range(num_batch):\n",
    "            baskets, lens, uids, rbks, ihis = data[i * batch_size : (i + 1) * batch_size] # load\n",
    "            baskets, lens, uids, rbks, ihis = sort_batch_of_lists(baskets, lens, uids, rbks, ihis) # sort, max_len = lens[0]\n",
    "            padded_baskets = pad_batch_of_lists(baskets, lens[0]) # pad\n",
    "            padded_rbks = pad_batch_of_lists(rbks, lens[0])\n",
    "            padded_ihis = pad_batch_of_lists(ihis, lens[0])\n",
    "            yield padded_baskets, lens, uids, padded_rbks, padded_ihis\n",
    "        \n",
    "        if len(data) % batch_size != 0:\n",
    "            residual = [i for i in range(num_batch * batch_size, len(data))] + list(np.random.choice(len(data), batch_size - len(data) % batch_size))    \n",
    "            print(len(residual))\n",
    "            baskets, lens, uids, rbks, ihis = map(list, zip(*[data[idx] for idx in residual]))\n",
    "            baskets, lens, uids, rbks, ihis = sort_batch_of_lists(baskets, lens, uids, rbks, ihis)\n",
    "            padded_baskets = pad_batch_of_lists(baskets, lens[0])\n",
    "            padded_rbks = pad_batch_of_lists(rbks, lens[0])\n",
    "            padded_ihis = pad_batch_of_lists(ihis, lens[0])\n",
    "            yield padded_baskets, lens, uids, padded_rbks, padded_ihis\n",
    "\n",
    "    else: \n",
    "        num_batch = len(data) // batch_size\n",
    "        for i in range(num_batch):\n",
    "            baskets, lens, uids = data[i * batch_size : (i + 1) * batch_size] # load\n",
    "            baskets, lens, uids = sort_batch_of_lists(baskets, lens, uids) # sort, max_len = lens[0]\n",
    "            padded_baskets = pad_batch_of_lists(baskets, lens[0]) # pad\n",
    "            yield padded_baskets, lens, uids\n",
    "        \n",
    "        if len(data) % batch_size != 0:\n",
    "            residual = [i for i in range(num_batch * batch_size, len(data))] + list(np.random.choice(len(data), batch_size - len(data) % batch_size))    \n",
    "            print(len(residual))\n",
    "            baskets, lens, uids = map(list, zip(*[data[idx] for idx in residual]))\n",
    "            baskets, lens, uids = sort_batch_of_lists(baskets, lens, uids)\n",
    "            padded_baskets = pad_batch_of_lists(baskets, lens[0])\n",
    "            yield padded_baskets, lens, uids\n",
    "\n",
    "####################### Model Related\n",
    "\n",
    "def pool_max(tensor, dim):\n",
    "    return torch.max(tensor, dim)[0]\n",
    "\n",
    "def pool_avg(tensor, dim):\n",
    "    return torch.mean(tensor, dim)\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "###################### Summary\n",
    "def get_grad_norm(model):\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.data.view(-1, 1))\n",
    "    assert len(grads) > 0\n",
    "    return torch.norm(torch.cat(grads))\n",
    "\n",
    "\n",
    "def get_loss(loss):\n",
    "    \"\"\"Convert variable into float\"\"\"\n",
    "    loss = loss.data\n",
    "    if isinstance(loss, torch.cuda.FloatTensor):\n",
    "        loss = loss.cpu()\n",
    "    assert len(loss.numpy()) == 1, 'length of loss is large than 1'\n",
    "    return float(loss.numpy()[0])\n",
    "\n",
    "\n",
    "def get_ratio_update(delta, weight):\n",
    "    delta = torch.cat([d.data.view(-1, 1) for d in delta])\n",
    "    weight = torch.cat([w.data.view(-1, 1) for w in weight])\n",
    "    return torch.norm(delta) / torch.norm(weight)\n",
    "\n",
    "\n",
    "def get_weight_update(previous_weight, weight):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        previous_weight: list\n",
    "    \"\"\"\n",
    "    deltas = []\n",
    "    for i, w in enumerate(weight):\n",
    "        deltas.append(w - previous_weight[i])\n",
    "    return deltas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce99609",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21c732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    Prepare Input for DREAM\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "class BasketConstructor(object):\n",
    "    '''\n",
    "        Group products into baskets(type: list)\n",
    "    '''\n",
    "    def __init__(self, raw_data_dir, cache_dir):\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self.cache_dir = cache_dir\n",
    "    \n",
    "    def get_users_products(self, ub_basket, df):\n",
    "        '''\n",
    "            get users' all purchased products\n",
    "        '''\n",
    "        flatten_list = lambda y:[x for a in y for x in flatten_list(a)] if type(y) is list else [y]\n",
    "        users_products = ub_basket[['user_id','basket']]\n",
    "        users_products.columns = ['CUSTOMER_ID','NEW_ITEM_ID']\n",
    "        for i, ub in enumerate(ub_basket['basket']):\n",
    "            print(\"flatten\", len(set(flatten_list(ub))))\n",
    "            print(\"basket\", users_products.loc[ i, \"NEW_ITEM_ID\"])\n",
    "            users_products.loc[ i, \"NEW_ITEM_ID\"] =  list( set(flatten_list(ub)) )\n",
    "            # users_products['NEW_ITEM_ID'][i] =  list( set(flatten_list(ub)) )\n",
    "        return users_products\n",
    "    \n",
    "    def get_baskets(self, data_name, reconstruct = False, reordered = False, none_idx = 49689):\n",
    "        '''\n",
    "            get users' baskets\n",
    "\n",
    "        '''\n",
    "        if not os.path.exists(f'{self.cache_dir}/{data_name}/'):\n",
    "             os.mkdir(f'{self.cache_dir}/{data_name}/')\n",
    "        filepath = f'{self.cache_dir}/{data_name}/basket_{data_name}.pkl'\n",
    "        print(\"filepath=\",filepath)\n",
    "\n",
    "        if (not reconstruct) and os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                up_basket = pickle.load(f)\n",
    "        else:\n",
    "            df = pd.read_csv(self.raw_data_dir + f'{data_name}_clean.csv') \n",
    "            up = df.sort_values(['CUSTOMER_ID', 'CART_ID', 'NEW_ITEM_ID'], ascending = True)\n",
    "            up_basket = up.groupby(['CUSTOMER_ID', 'CART_ID'])['NEW_ITEM_ID'].apply(list).reset_index()\n",
    "            up_basket = up_basket.sort_values(['CUSTOMER_ID', 'CART_ID'], ascending = True).groupby(['CUSTOMER_ID'])['NEW_ITEM_ID'].apply(list).reset_index()\n",
    "            up_basket.columns = ['user_id', 'basket']\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(up_basket, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return up_basket\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    '''\n",
    "        Dataset prepare from user-basket\n",
    "    '''\n",
    "    def __init__(self, up_basket, up_r_basket = None, up_his = None):\n",
    "        if (up_r_basket is not None) and (up_his is not None):\n",
    "            self.is_reordered_included = True\n",
    "        else:\n",
    "            self.is_reordered_included = False\n",
    "\n",
    "        up_basket['num_baskets'] = up_basket.basket.apply(len)\n",
    "        self.user_id = list(up_basket.user_id)\n",
    "        self.num_baskets = [int(n) for n in list(up_basket.num_baskets)]    \n",
    "        self.basket = [[[int(p) for p in b]for b in u] for u in list(up_basket.basket)]\n",
    "\n",
    "        if self.is_reordered_included is True:\n",
    "            up_basket = pd.merge(up_basket, up_r_basket, on = ['user_id'], how = 'left')\n",
    "            up_basket = pd.merge(up_basket, up_his, on = ['user_id'], how = 'left')\n",
    "            self.reorder_basket = [[[int(p) for p in b]for b in u] for u in list(up_basket.reorder_basket)]\n",
    "            self.history_item = [[[int(p) for p in b]for b in u] for u in list(up_basket.history_items)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "            return baskets & num_baskets\n",
    "        '''\n",
    "        if self.is_reordered_included is True:\n",
    "            return self.basket[index], self.num_baskets[index], self.user_id[index], self.reorder_basket[index], self.history_item[index]\n",
    "        else:\n",
    "            return self.basket[index], self.num_baskets[index], self.user_id[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_id)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6196f0",
   "metadata": {},
   "source": [
    "# Dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22685044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    DREAM: Dynamic Recurrent Network for Next Basket Prediction\n",
    "    Based on PyTorch\n",
    "    \n",
    "    @TODO: \n",
    "        - Efficiency Variabel Usage\n",
    "        - Other initilizations for item/product embedding, for example LDA\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# from utils import pool_max, pool_avg\n",
    "\n",
    "\n",
    "class DreamModel(torch.nn.Module):\n",
    "    \"\"\"       Input Data: b_1, ... b_i ..., b_t\n",
    "                   b_i stands for user u's ith basket\n",
    "                   b_i = [p_1,..p_j...,p_n]\n",
    "                   p_j stands for the  jth product in user u's ith basket\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DreamModel, self).__init__()\n",
    "        # Model configuration\n",
    "        self.config = config\n",
    "        # Layer definitons\n",
    "        self.encode = torch.nn.Embedding(config.num_product, \n",
    "                                         config.embedding_dim,\n",
    "                                         padding_idx = config.none_idx) # Item embedding layer, 商品编码\n",
    "        print(\"self.encode=\",self.encode )\n",
    "        self.pool = {'avg':pool_avg, 'max':pool_max}[config.basket_pool_type] # Pooling of basket\n",
    "        # RNN type specify\n",
    "        if config.rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(torch.nn, config.rnn_type)(config.embedding_dim, \n",
    "                                                          config.embedding_dim, \n",
    "                                                          config.rnn_layer_num, \n",
    "                                                          batch_first=True, \n",
    "                                                          dropout=config.dropout)\n",
    "        else:\n",
    "            nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[config.rnn_type]\n",
    "            self.rnn = torch.nn.RNN(config.embedding_dim, \n",
    "                                    config.embedding_dim, \n",
    "                                    config.rnn_layer_num, \n",
    "                                    nonlinearity=nonlinearity, \n",
    "                                    batch_first=True, \n",
    "                                    dropout=config.dropout)\n",
    "    \n",
    "    def forward(self, x, lengths, hidden):\n",
    "        # Basket Encoding \n",
    "        # print(\"x=\\n\",x)\n",
    "        ub_seqs = [] # users' basket sequence\n",
    "        for user in x: # x shape (batch of user, time_step, indice of product) nested lists\n",
    "            embed_baskets = []\n",
    "            for basket in user:\n",
    "                basket = torch.LongTensor(basket).resize_(1, len(basket))\n",
    "                basket = basket.cuda() if self.config.cuda else basket # use cuda for acceleration\n",
    "                basket = self.encode(torch.autograd.Variable(basket)) # shape: 1, len(basket), embedding_dim\n",
    "                embed_baskets.append(self.pool(basket, dim=1))\n",
    "            # concat current user's all baskets and append it to users' basket sequence\n",
    "            ub_seqs.append(torch.cat(embed_baskets, 0).unsqueeze(0))  # shape: 1, num_basket, embedding_dim\n",
    "        # Input for rnn \n",
    "        ub_seqs = torch.cat(ub_seqs, 0).cuda() if self.config.cuda else torch.cat(ub_seqs, 0) # shape: batch_size, max_len, embedding_dim\n",
    "        packed_ub_seqs = torch.nn.utils.rnn.pack_padded_sequence(ub_seqs, lengths, batch_first=True) # packed sequence as required by pytorch\n",
    "        \n",
    "        # RNN\n",
    "        output, h_u = self.rnn(packed_ub_seqs, hidden)\n",
    "        dynamic_user, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True) # shape: batch_size, max_len, embedding_dim\n",
    "        return dynamic_user, h_u\n",
    "        \n",
    "    def init_weight(self):\n",
    "        # Init item embedding\n",
    "        initrange = 0.1\n",
    "        self.encode.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Init hidden states for rnn\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.config.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.config.rnn_layer_num, batch_size, self.config.embedding_dim).zero_()),\n",
    "                    Variable(weight.new(self.config.rnn_layer_num, batch_size, self.config.embedding_dim).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.config.rnn_layer_num, batch_size, self.config.embedding_dim).zero_())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461a58a",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c35d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Evaluation of DREAM\n",
    "\"\"\"\n",
    "import torch\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "# from data import Dataset, BasketConstructor\n",
    "# from utils import repackage_hidden, batchify\n",
    "\n",
    "import pdb\n",
    "\n",
    "def eval_pred(dr_model, ub):\n",
    "    '''\n",
    "        evaluate dream model for predicting next basket on all training users\n",
    "        in batches\n",
    "    '''\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    dr_model.eval()\n",
    "    dr_hidden = dr_model.init_hidden(dr_model.config.batch_size)\n",
    "    start_time = time()\n",
    "    id_u, score_u = [], [] # user's id, user's score\n",
    "    num_batchs = ceil(len(ub) / dr_model.config.batch_size)\n",
    "    for i,x in enumerate(batchify(ub, dr_model.config.batch_size)):\n",
    "        print(i)\n",
    "        baskets, lens, uids = x\n",
    "        _, dynamic_user, _ = dr_model(baskets, lens, dr_hidden)# shape: batch_size, max_len, embedding_size\n",
    "        dr_hidden = repackage_hidden(dr_hidden)\n",
    "        for i,l,du in zip(uids, lens, dynamic_user):\n",
    "            du_latest = du[l - 1].unsqueeze(0) # shape: 1, embedding_size\n",
    "            score_up = torch.mm(du_latest, item_embedding.t()) # shape: 1, num_item\n",
    "            score_u.append(score_up.cpu().data.numpy())\n",
    "            id_u.append(i)\n",
    "    elapsed = time() - start_time \n",
    "    print('[Predicting] Elapsed: {02.2f}'.format(elapsed))\n",
    "    return score_u, id_u\n",
    "\n",
    "def eval_batch(dr_model, ub, batch_size , is_reordered = False):\n",
    "    '''\n",
    "        Using dr_model to predict (u,p) score in batch\n",
    "        Parameters:\n",
    "        - ub: users' baskets\n",
    "        - up: users' history purchases\n",
    "        - batch_size\n",
    "    '''\n",
    "    # turn on evaluation mode\n",
    "    dr_model.eval()\n",
    "    is_cuda =  dr_model.config.cuda\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    dr_hidden = dr_model.init_hidden(batch_size)\n",
    "\n",
    "    id_u, item_u, score_u, dynam_u = [], [], [], []\n",
    "    start_time = time()\n",
    "    num_batchs = ceil(len(ub) / batch_size)\n",
    "    for i, x in enumerate(batchify(ub, batch_size, is_reordered)):\n",
    "        if is_reordered is True:\n",
    "            baskets, lens, uids, r_baskets, h_baskets = x\n",
    "        else:\n",
    "            baskets, lens, uids = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, dr_hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            du_latest =  du[l - 1].unsqueeze(0)\n",
    "            ### 計算所有項目分數\n",
    "            all_item = torch.tensor([i for i in range( dr_model.config.none_idx )], dtype=torch.long).to(device)\n",
    "            # all_item = torch.cuda.LongTensor([i for i in range( dr_model.config.none_idx )])\n",
    "            score_up = torch.mm(du_latest, item_embedding.t()).cpu().data.numpy()[0]\n",
    "            id_u.append(uid), dynam_u.append(du_latest.cpu().data.numpy()[0]), item_u.append(all_item.cpu().numpy()),score_u.append(score_up)\n",
    "        # Logging\n",
    "        elapsed = time() - start_time; start_time = time()\n",
    "        print('[Predicting]| Batch {:5d} / {:5d} | seconds/batch {:02.02f}'.format(i, num_batchs, elapsed))\n",
    "    return id_u, item_u, score_u, dynam_u\n",
    "\n",
    "\n",
    "def eval_up(uid, pid, dr_model, ub, dr_hidden):\n",
    "    '''\n",
    "        calculate latest score for (u,p) pair\n",
    "        ub for single user\n",
    "        dr_hidden: should be initialized and repackage after each batch\n",
    "    '''\n",
    "    dynamic_user = get_dynamic_u(uid, dr_model, ub, dr_hidden)\n",
    "    item_embedding = get_item_embedding(pid, dr_model)\n",
    "    score_up = torch.mm(dynamic_user, item_embedding.t())\n",
    "    return score_up\n",
    "\n",
    "def get_dynamic_u(uid, dr_model, ub, dr_hidden):\n",
    "    '''\n",
    "        get latest dynamic representation of user uid\n",
    "        dr_hidden must be provided as global variable\n",
    "    '''\n",
    "    for i,x in enumerate(batchify(ub, 1)):\n",
    "        baskets, lens, uids = x\n",
    "        _, dynamic_user, _ = dr_model(baskets, lens, dr_hidden)\n",
    "    return dynamic_user[0][lens[0] - 1].unsqueeze(0)\n",
    "\n",
    "def get_item_embedding(pid, dr_model):\n",
    "    '''\n",
    "        get item's embedding\n",
    "        pid can be a integer or a torch.cuda.LongTensor\n",
    "    '''\n",
    "    if isinstance(pid, torch.cuda.LongTensor) or isinstance(pid, torch.LongTensor):\n",
    "        return dr_model.encode.weight[pid]\n",
    "    elif isinstance(pid, int):\n",
    "        return dr_model.encode.weight[pid].unsqueeze(0)\n",
    "    else:\n",
    "        print('Unsupported Index Type %s'%type(pid))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806fb920",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a30821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath= ./tmp/feat//Dunnhumby/basket_Dunnhumby.pkl\n",
      "ub_basket=\n",
      "        user_id                                             basket\n",
      "0           31  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, ...\n",
      "1           68  [[51, 52, 53, 54], [55, 56, 57, 58], [56, 59, ...\n",
      "2          180  [[11, 62, 122, 151, 152, 153, 154, 155, 156, 1...\n",
      "3          324  [[0, 41, 110, 139, 231, 242, 243, 244, 245, 24...\n",
      "4          358  [[0, 11, 31, 63, 158, 172, 302, 307, 308, 309,...\n",
      "...        ...                                                ...\n",
      "12821   999696  [[130, 172], [130, 1668, 1925, 2679], [19, 130...\n",
      "12822   999698  [[162, 708, 778, 1123, 1502, 1516, 1829], [56,...\n",
      "12823   999718  [[16, 107, 138, 158, 274, 280, 290, 558, 568, ...\n",
      "12824   999934  [[0, 11, 62, 67, 108, 109, 140, 240, 245, 379,...\n",
      "12825   999976  [[11, 62, 903, 1172, 1977, 2228, 2934], [290, ...\n",
      "\n",
      "[12826 rows x 2 columns]\n",
      "test_ub_label=       user_id                                              label\n",
      "0      584429                                         [62, 1539]\n",
      "1      681396                      [62, 138, 176, 207, 238, 413]\n",
      "2       43813                                  [904, 1857, 2143]\n",
      "3      371084                [117, 231, 290, 640, 695, 749, 774]\n",
      "4      509796                                             [2370]\n",
      "...       ...                                                ...\n",
      "2561   112808      [363, 366, 475, 1099, 1286, 1554, 2405, 2427]\n",
      "2562   711170                                             [2083]\n",
      "2563   931223  [2, 11, 56, 98, 108, 163, 318, 626, 798, 868, ...\n",
      "2564   709963    [11, 36, 62, 558, 1416, 1775, 1796, 1895, 1932]\n",
      "2565   924918                                         [400, 878]\n",
      "\n",
      "[2566 rows x 2 columns]\n",
      "ub_basket=\n",
      "    user_id                                             basket\n",
      "0   584429  [[36, 100, 495], [11, 293, 495, 2049, 2105, 25...\n",
      "1   681396  [[62, 349, 400, 652, 753, 975, 982, 1577, 1853...\n",
      "2    43813  [[1108], [903], [2324], [2324], [2220], [586, ...\n",
      "3   371084  [[738], [526, 614], [40, 102, 1157, 1393], [10...\n",
      "4   509796  [[11, 36, 39, 224, 343, 524, 826, 1428, 1437, ...\n",
      "self.encode= Embedding(3004, 128, padding_idx=3003)\n",
      "basket_pool_type max\n",
      "rnn_layers 2\n",
      "rnn_type RNN_RELU\n",
      "dropout 0.5\n",
      "num_product 3004\n",
      "none_idx 3003\n",
      "embedding_dim 128\n",
      "cuda True\n",
      "clip 200\n",
      "epochs 40\n",
      "batch_size 32\n",
      "learning_rate 0.001\n",
      "log_interval 2\n",
      "data_name Dunnhumby\n",
      "train_times 1\n",
      "[Training]| Epochs   0 | Batch     2 /   321 | ms/batch 14234.48 | Loss 02.46 |\n",
      "[Training]| Epochs   0 | Batch     4 /   321 | ms/batch 9041.15 | Loss 01.08 |\n",
      "[Training]| Epochs   0 | Batch     6 /   321 | ms/batch 12057.77 | Loss 00.90 |\n",
      "[Training]| Epochs   0 | Batch     8 /   321 | ms/batch 9190.58 | Loss 00.86 |\n",
      "[Training]| Epochs   0 | Batch    10 /   321 | ms/batch 11218.24 | Loss 00.79 |\n",
      "[Training]| Epochs   0 | Batch    12 /   321 | ms/batch 11107.72 | Loss 00.77 |\n",
      "[Training]| Epochs   0 | Batch    14 /   321 | ms/batch 7637.67 | Loss 00.77 |\n",
      "[Training]| Epochs   0 | Batch    16 /   321 | ms/batch 8070.31 | Loss 00.77 |\n",
      "[Training]| Epochs   0 | Batch    18 /   321 | ms/batch 9529.63 | Loss 00.74 |\n",
      "[Training]| Epochs   0 | Batch    20 /   321 | ms/batch 9439.07 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    22 /   321 | ms/batch 11216.74 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    24 /   321 | ms/batch 9875.34 | Loss 00.73 |\n",
      "[Training]| Epochs   0 | Batch    26 /   321 | ms/batch 9529.71 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    28 /   321 | ms/batch 9096.65 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    30 /   321 | ms/batch 8079.78 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    32 /   321 | ms/batch 7648.89 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    34 /   321 | ms/batch 7765.29 | Loss 00.72 |\n",
      "[Training]| Epochs   0 | Batch    36 /   321 | ms/batch 10181.62 | Loss 00.71 |\n",
      "[Training]| Epochs   0 | Batch    38 /   321 | ms/batch 9772.61 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    40 /   321 | ms/batch 9923.10 | Loss 00.71 |\n",
      "[Training]| Epochs   0 | Batch    42 /   321 | ms/batch 7240.01 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    44 /   321 | ms/batch 12849.38 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    46 /   321 | ms/batch 11039.82 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    48 /   321 | ms/batch 10770.76 | Loss 00.71 |\n",
      "[Training]| Epochs   0 | Batch    50 /   321 | ms/batch 11301.08 | Loss 00.71 |\n",
      "[Training]| Epochs   0 | Batch    52 /   321 | ms/batch 10483.94 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    54 /   321 | ms/batch 8860.01 | Loss 00.71 |\n",
      "[Training]| Epochs   0 | Batch    56 /   321 | ms/batch 11528.97 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    58 /   321 | ms/batch 10268.41 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    60 /   321 | ms/batch 9432.41 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    62 /   321 | ms/batch 10047.84 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    64 /   321 | ms/batch 9676.33 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    66 /   321 | ms/batch 7666.88 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    68 /   321 | ms/batch 10697.86 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    70 /   321 | ms/batch 9729.49 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    72 /   321 | ms/batch 8006.80 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    74 /   321 | ms/batch 7761.79 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    76 /   321 | ms/batch 7384.22 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    78 /   321 | ms/batch 11915.47 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    80 /   321 | ms/batch 10120.81 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    82 /   321 | ms/batch 10885.94 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    84 /   321 | ms/batch 8160.77 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    86 /   321 | ms/batch 12764.97 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    88 /   321 | ms/batch 10037.95 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    90 /   321 | ms/batch 9527.36 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    92 /   321 | ms/batch 13201.92 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    94 /   321 | ms/batch 10002.60 | Loss 00.70 |\n",
      "[Training]| Epochs   0 | Batch    96 /   321 | ms/batch 7464.47 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch    98 /   321 | ms/batch 12012.59 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   100 /   321 | ms/batch 11118.17 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   102 /   321 | ms/batch 8352.11 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   104 /   321 | ms/batch 9830.47 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   106 /   321 | ms/batch 7112.04 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   108 /   321 | ms/batch 7501.49 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   110 /   321 | ms/batch 12709.05 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   112 /   321 | ms/batch 7087.19 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   114 /   321 | ms/batch 10133.14 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   116 /   321 | ms/batch 13153.44 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   118 /   321 | ms/batch 11385.84 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   120 /   321 | ms/batch 9534.90 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   122 /   321 | ms/batch 11512.67 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   124 /   321 | ms/batch 10355.45 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   126 /   321 | ms/batch 9705.06 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   128 /   321 | ms/batch 10658.29 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   130 /   321 | ms/batch 11525.21 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   132 /   321 | ms/batch 5524.42 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   134 /   321 | ms/batch 11646.10 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   136 /   321 | ms/batch 9213.55 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   138 /   321 | ms/batch 9090.33 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   140 /   321 | ms/batch 9553.30 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   142 /   321 | ms/batch 13758.89 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   144 /   321 | ms/batch 10841.75 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   146 /   321 | ms/batch 10733.49 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   148 /   321 | ms/batch 12916.26 | Loss 00.69 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   0 | Batch   150 /   321 | ms/batch 9437.37 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   152 /   321 | ms/batch 10175.41 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   154 /   321 | ms/batch 6266.12 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   156 /   321 | ms/batch 11920.95 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   158 /   321 | ms/batch 11724.85 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   160 /   321 | ms/batch 10747.18 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   162 /   321 | ms/batch 8254.67 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   164 /   321 | ms/batch 8342.36 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   166 /   321 | ms/batch 10326.36 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   168 /   321 | ms/batch 14446.86 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   170 /   321 | ms/batch 8519.05 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   172 /   321 | ms/batch 8951.11 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   174 /   321 | ms/batch 8872.34 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   176 /   321 | ms/batch 7645.00 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   178 /   321 | ms/batch 9414.87 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   180 /   321 | ms/batch 10590.45 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   182 /   321 | ms/batch 9158.26 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   184 /   321 | ms/batch 9594.74 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   186 /   321 | ms/batch 10060.50 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   188 /   321 | ms/batch 9044.41 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   190 /   321 | ms/batch 10273.91 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   192 /   321 | ms/batch 9896.65 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   194 /   321 | ms/batch 9895.05 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   196 /   321 | ms/batch 11583.66 | Loss 00.69 |\n",
      "[Training]| Epochs   0 | Batch   198 /   321 | ms/batch 9575.40 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   200 /   321 | ms/batch 6548.00 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   202 /   321 | ms/batch 8502.30 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   204 /   321 | ms/batch 7454.36 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   206 /   321 | ms/batch 10434.79 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   208 /   321 | ms/batch 10100.33 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   210 /   321 | ms/batch 8437.57 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   212 /   321 | ms/batch 7612.05 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   214 /   321 | ms/batch 8969.24 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   216 /   321 | ms/batch 8981.56 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   218 /   321 | ms/batch 11878.39 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   220 /   321 | ms/batch 12529.05 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   222 /   321 | ms/batch 8746.39 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   224 /   321 | ms/batch 10791.71 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   226 /   321 | ms/batch 9848.71 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   228 /   321 | ms/batch 10333.01 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   230 /   321 | ms/batch 11559.10 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   232 /   321 | ms/batch 10271.17 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   234 /   321 | ms/batch 10904.53 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   236 /   321 | ms/batch 11903.23 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   238 /   321 | ms/batch 10328.01 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   240 /   321 | ms/batch 7265.13 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   242 /   321 | ms/batch 7157.70 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   244 /   321 | ms/batch 11524.03 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   246 /   321 | ms/batch 9722.06 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   248 /   321 | ms/batch 11345.02 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   250 /   321 | ms/batch 9647.40 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   252 /   321 | ms/batch 9449.50 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   254 /   321 | ms/batch 10407.12 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   256 /   321 | ms/batch 9584.44 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   258 /   321 | ms/batch 10183.94 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   260 /   321 | ms/batch 10748.25 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   262 /   321 | ms/batch 12553.10 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   264 /   321 | ms/batch 10261.86 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   266 /   321 | ms/batch 8330.75 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   268 /   321 | ms/batch 9914.02 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   270 /   321 | ms/batch 9975.05 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   272 /   321 | ms/batch 13568.18 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   274 /   321 | ms/batch 10166.40 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   276 /   321 | ms/batch 8038.46 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   278 /   321 | ms/batch 10667.85 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   280 /   321 | ms/batch 11372.32 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   282 /   321 | ms/batch 9312.90 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   284 /   321 | ms/batch 10752.27 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   286 /   321 | ms/batch 10697.04 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   288 /   321 | ms/batch 11344.51 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   290 /   321 | ms/batch 13157.31 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   292 /   321 | ms/batch 7597.01 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   294 /   321 | ms/batch 7490.42 | Loss 00.68 |\n",
      "[Training]| Epochs   0 | Batch   296 /   321 | ms/batch 7406.45 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   298 /   321 | ms/batch 10194.42 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   300 /   321 | ms/batch 9461.31 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   302 /   321 | ms/batch 7280.37 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   304 /   321 | ms/batch 11746.84 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   306 /   321 | ms/batch 12604.93 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   308 /   321 | ms/batch 10226.73 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   310 /   321 | ms/batch 7907.46 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   312 /   321 | ms/batch 14201.81 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   314 /   321 | ms/batch 12473.66 | Loss 00.67 |\n",
      "[Training]| Epochs   0 | Batch   316 /   321 | ms/batch 10611.74 | Loss 00.66 |\n",
      "[Training]| Epochs   0 | Batch   318 /   321 | ms/batch 11471.63 | Loss 00.67 |\n",
      "32\n",
      "[Training]| Epochs   0 | Batch   320 /   321 | ms/batch 9334.75 | Loss 00.66 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   0 | Elapsed 3627.29 | Loss 00.66 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 1.34\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 1.44\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.91\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 1.52\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 1.52\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 1.20\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 1.40\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 1.38\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.91\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 1.39\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    21 /    81 | seconds/batch 1.43\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.82\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 1.13\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 1.28\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 2.03\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 1.17\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 1.10\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 1.20\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 1.33\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 1.43\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 1.26\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 1.14\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 1.23\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 1.08\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 1.28\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.92\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 1.58\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 1.27\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 1.05\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 1.50\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.96\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 1.02\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 1.66\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 1.30\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 1.47\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 1.19\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 1.51\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 1.36\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 1.18\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 1.07\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 1.66\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 1.30\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 1.22\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 1.32\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 1.35\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 1.45\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 1.56\n",
      "result= [1, 0.05430245046400039, 0.07036601279921023, 0.08778412864250962, 0.10495119994260887, 0.12138574827674396, 0.07232997237195843, 0.04660915752360343, 0.019823242330216654, 0.014411701788125517, 0.01312433214551847, 0.05052065533394936, 0.04606190753559163, 0.028505543852261242, 0.023102041528453514, 0.021926176705958783, 0.0492941771001669, 0.05236331690233477, 0.05413713530979946, 0.05519304595835397, 0.056078329887233866, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.6635599099559548]\n",
      "[Training]| Epochs   1 | Batch     2 /   321 | ms/batch 13393.05 | Loss 00.99 |\n",
      "[Training]| Epochs   1 | Batch     4 /   321 | ms/batch 8221.96 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch     6 /   321 | ms/batch 13989.99 | Loss 00.67 |\n",
      "[Training]| Epochs   1 | Batch     8 /   321 | ms/batch 14030.51 | Loss 00.67 |\n",
      "[Training]| Epochs   1 | Batch    10 /   321 | ms/batch 11497.69 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    12 /   321 | ms/batch 11697.45 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    14 /   321 | ms/batch 6377.74 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    16 /   321 | ms/batch 8670.61 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    18 /   321 | ms/batch 8654.60 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    20 /   321 | ms/batch 8867.31 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    22 /   321 | ms/batch 12013.55 | Loss 00.67 |\n",
      "[Training]| Epochs   1 | Batch    24 /   321 | ms/batch 8221.25 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    26 /   321 | ms/batch 8956.70 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    28 /   321 | ms/batch 9070.65 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    30 /   321 | ms/batch 9046.26 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    32 /   321 | ms/batch 7402.01 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    34 /   321 | ms/batch 7545.22 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    36 /   321 | ms/batch 9484.92 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    38 /   321 | ms/batch 9106.94 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    40 /   321 | ms/batch 9766.76 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    42 /   321 | ms/batch 7290.69 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    44 /   321 | ms/batch 9127.52 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    46 /   321 | ms/batch 8819.29 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    48 /   321 | ms/batch 8065.62 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    50 /   321 | ms/batch 3576.04 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    52 /   321 | ms/batch 4946.64 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    54 /   321 | ms/batch 5427.84 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    56 /   321 | ms/batch 6409.66 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    58 /   321 | ms/batch 4142.22 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    60 /   321 | ms/batch 3650.91 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch    62 /   321 | ms/batch 4250.09 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    64 /   321 | ms/batch 3519.34 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    66 /   321 | ms/batch 4103.68 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    68 /   321 | ms/batch 5940.13 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    70 /   321 | ms/batch 5720.64 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    72 /   321 | ms/batch 3870.02 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    74 /   321 | ms/batch 4205.77 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    76 /   321 | ms/batch 4557.49 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch    78 /   321 | ms/batch 5205.14 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    80 /   321 | ms/batch 5002.22 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    82 /   321 | ms/batch 4951.02 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    84 /   321 | ms/batch 3743.40 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch    86 /   321 | ms/batch 5388.05 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    88 /   321 | ms/batch 6381.23 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    90 /   321 | ms/batch 6116.58 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    92 /   321 | ms/batch 6546.73 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch    94 /   321 | ms/batch 6102.20 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch    96 /   321 | ms/batch 5407.73 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch    98 /   321 | ms/batch 5041.24 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   100 /   321 | ms/batch 5681.34 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   102 /   321 | ms/batch 3768.46 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   104 /   321 | ms/batch 5737.69 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   106 /   321 | ms/batch 3213.73 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   108 /   321 | ms/batch 3975.50 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   110 /   321 | ms/batch 6453.67 | Loss 00.65 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   1 | Batch   112 /   321 | ms/batch 3939.73 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   114 /   321 | ms/batch 5094.70 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   116 /   321 | ms/batch 7065.99 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   118 /   321 | ms/batch 4726.23 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch   120 /   321 | ms/batch 5311.50 | Loss 00.66 |\n",
      "[Training]| Epochs   1 | Batch   122 /   321 | ms/batch 6216.62 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   124 /   321 | ms/batch 5066.72 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   126 /   321 | ms/batch 4883.07 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   128 /   321 | ms/batch 6202.71 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   130 /   321 | ms/batch 6002.09 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   132 /   321 | ms/batch 4627.66 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   134 /   321 | ms/batch 5544.89 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   136 /   321 | ms/batch 4017.37 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   138 /   321 | ms/batch 5227.39 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   140 /   321 | ms/batch 5714.37 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   142 /   321 | ms/batch 6070.83 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   144 /   321 | ms/batch 4441.87 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   146 /   321 | ms/batch 5687.25 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   148 /   321 | ms/batch 7713.32 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   150 /   321 | ms/batch 4038.74 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   152 /   321 | ms/batch 4220.01 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   154 /   321 | ms/batch 3153.26 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   156 /   321 | ms/batch 5480.87 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   158 /   321 | ms/batch 6306.15 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   160 /   321 | ms/batch 4720.49 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   162 /   321 | ms/batch 5917.61 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   164 /   321 | ms/batch 3248.24 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   166 /   321 | ms/batch 3842.19 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   168 /   321 | ms/batch 7605.57 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   170 /   321 | ms/batch 6054.81 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   172 /   321 | ms/batch 6130.99 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   174 /   321 | ms/batch 3927.00 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   176 /   321 | ms/batch 4161.18 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   178 /   321 | ms/batch 7407.09 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   180 /   321 | ms/batch 4920.24 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   182 /   321 | ms/batch 5999.67 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   184 /   321 | ms/batch 5000.14 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   186 /   321 | ms/batch 6564.67 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   188 /   321 | ms/batch 4573.16 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   190 /   321 | ms/batch 4331.03 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   192 /   321 | ms/batch 6057.10 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   194 /   321 | ms/batch 5679.71 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   196 /   321 | ms/batch 5687.08 | Loss 00.65 |\n",
      "[Training]| Epochs   1 | Batch   198 /   321 | ms/batch 4266.89 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   200 /   321 | ms/batch 3804.41 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   202 /   321 | ms/batch 4117.38 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   204 /   321 | ms/batch 4301.49 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   206 /   321 | ms/batch 4307.07 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   208 /   321 | ms/batch 4014.62 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   210 /   321 | ms/batch 3634.55 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   212 /   321 | ms/batch 6133.59 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   214 /   321 | ms/batch 6904.21 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   216 /   321 | ms/batch 5538.42 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   218 /   321 | ms/batch 6916.67 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   220 /   321 | ms/batch 4753.36 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   222 /   321 | ms/batch 4383.31 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   224 /   321 | ms/batch 4636.64 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   226 /   321 | ms/batch 4844.63 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   228 /   321 | ms/batch 3795.02 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   230 /   321 | ms/batch 6350.42 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   232 /   321 | ms/batch 5274.04 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   234 /   321 | ms/batch 6292.89 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   236 /   321 | ms/batch 5424.45 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   238 /   321 | ms/batch 6117.26 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   240 /   321 | ms/batch 4344.09 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   242 /   321 | ms/batch 3494.47 | Loss 00.59 |\n",
      "[Training]| Epochs   1 | Batch   244 /   321 | ms/batch 6951.73 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   246 /   321 | ms/batch 4938.92 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   248 /   321 | ms/batch 6432.89 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   250 /   321 | ms/batch 4387.39 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   252 /   321 | ms/batch 6086.17 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   254 /   321 | ms/batch 4225.26 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   256 /   321 | ms/batch 4733.15 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   258 /   321 | ms/batch 3459.53 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   260 /   321 | ms/batch 4772.34 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   262 /   321 | ms/batch 5266.35 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   264 /   321 | ms/batch 4008.49 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   266 /   321 | ms/batch 5653.72 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   268 /   321 | ms/batch 4855.29 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   270 /   321 | ms/batch 4649.99 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   272 /   321 | ms/batch 7214.57 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   274 /   321 | ms/batch 5464.73 | Loss 00.64 |\n",
      "[Training]| Epochs   1 | Batch   276 /   321 | ms/batch 4743.05 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   278 /   321 | ms/batch 4818.74 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   280 /   321 | ms/batch 4845.37 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   282 /   321 | ms/batch 4402.41 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   284 /   321 | ms/batch 4049.60 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   286 /   321 | ms/batch 6657.19 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   288 /   321 | ms/batch 5367.48 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   290 /   321 | ms/batch 4867.85 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   292 /   321 | ms/batch 4679.15 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   294 /   321 | ms/batch 4555.93 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   296 /   321 | ms/batch 5130.29 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   298 /   321 | ms/batch 5252.83 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   300 /   321 | ms/batch 5249.96 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   302 /   321 | ms/batch 4530.96 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   304 /   321 | ms/batch 6592.66 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   306 /   321 | ms/batch 5828.12 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   308 /   321 | ms/batch 4960.70 | Loss 00.61 |\n",
      "[Training]| Epochs   1 | Batch   310 /   321 | ms/batch 3609.39 | Loss 00.60 |\n",
      "[Training]| Epochs   1 | Batch   312 /   321 | ms/batch 4460.91 | Loss 00.63 |\n",
      "[Training]| Epochs   1 | Batch   314 /   321 | ms/batch 5558.45 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   316 /   321 | ms/batch 6223.05 | Loss 00.62 |\n",
      "[Training]| Epochs   1 | Batch   318 /   321 | ms/batch 4929.49 | Loss 00.63 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[Training]| Epochs   1 | Batch   320 /   321 | ms/batch 5796.86 | Loss 00.63 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   1 | Elapsed 1861.14 | Loss 00.62 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.91\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.27\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.29\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.97\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.85\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.75\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.70\n",
      "result= [2, 0.08113852164193419, 0.10367663317328771, 0.1428872611684346, 0.17428835053417893, 0.19871884923564764, 0.10506555166266807, 0.07061585614389493, 0.03329413362712667, 0.02494183166456334, 0.021949833478206594, 0.07400915275210992, 0.06899476738897786, 0.04756622730710036, 0.039762367611273323, 0.03662578853330664, 0.06963100086168814, 0.07421387116688238, 0.0780858240823101, 0.08009171742994545, 0.08136538266005039, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.6197207180070289]\n",
      "[Training]| Epochs   2 | Batch     2 /   321 | ms/batch 8196.67 | Loss 00.92 |\n",
      "[Training]| Epochs   2 | Batch     4 /   321 | ms/batch 6292.70 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch     6 /   321 | ms/batch 6859.28 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch     8 /   321 | ms/batch 6703.21 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch    10 /   321 | ms/batch 5733.53 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    12 /   321 | ms/batch 6484.42 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    14 /   321 | ms/batch 3567.48 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    16 /   321 | ms/batch 3583.40 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    18 /   321 | ms/batch 4368.92 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    20 /   321 | ms/batch 3963.89 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    22 /   321 | ms/batch 7329.33 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch    24 /   321 | ms/batch 4825.15 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    26 /   321 | ms/batch 6111.92 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    28 /   321 | ms/batch 5075.30 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    30 /   321 | ms/batch 3950.44 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    32 /   321 | ms/batch 4951.55 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    34 /   321 | ms/batch 4984.58 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    36 /   321 | ms/batch 5046.65 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch    38 /   321 | ms/batch 5112.02 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    40 /   321 | ms/batch 5705.19 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    42 /   321 | ms/batch 6068.05 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    44 /   321 | ms/batch 4286.48 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    46 /   321 | ms/batch 5558.92 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    48 /   321 | ms/batch 5394.14 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    50 /   321 | ms/batch 5445.47 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    52 /   321 | ms/batch 5267.98 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    54 /   321 | ms/batch 3373.99 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    56 /   321 | ms/batch 5941.52 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    58 /   321 | ms/batch 5055.11 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    60 /   321 | ms/batch 4290.97 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    62 /   321 | ms/batch 4023.63 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    64 /   321 | ms/batch 3915.54 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    66 /   321 | ms/batch 4052.46 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    68 /   321 | ms/batch 4185.55 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    70 /   321 | ms/batch 4352.63 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    72 /   321 | ms/batch 3365.47 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    74 /   321 | ms/batch 4078.57 | Loss 00.61 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   2 | Batch    76 /   321 | ms/batch 4365.42 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    78 /   321 | ms/batch 5208.09 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    80 /   321 | ms/batch 4798.83 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    82 /   321 | ms/batch 7310.06 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    84 /   321 | ms/batch 5020.78 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch    86 /   321 | ms/batch 7494.78 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    88 /   321 | ms/batch 6142.08 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    90 /   321 | ms/batch 4300.94 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    92 /   321 | ms/batch 5152.51 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch    94 /   321 | ms/batch 4002.68 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch    96 /   321 | ms/batch 5335.15 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch    98 /   321 | ms/batch 6102.61 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   100 /   321 | ms/batch 6722.38 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   102 /   321 | ms/batch 4663.75 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   104 /   321 | ms/batch 6768.91 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   106 /   321 | ms/batch 4132.26 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   108 /   321 | ms/batch 4712.21 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   110 /   321 | ms/batch 5306.49 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   112 /   321 | ms/batch 4307.68 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   114 /   321 | ms/batch 3671.72 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   116 /   321 | ms/batch 5389.98 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   118 /   321 | ms/batch 5215.91 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   120 /   321 | ms/batch 4168.67 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   122 /   321 | ms/batch 6456.47 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   124 /   321 | ms/batch 6565.01 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   126 /   321 | ms/batch 5969.76 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   128 /   321 | ms/batch 5628.53 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   130 /   321 | ms/batch 7041.10 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   132 /   321 | ms/batch 2401.40 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   134 /   321 | ms/batch 5316.11 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   136 /   321 | ms/batch 4857.05 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   138 /   321 | ms/batch 5340.93 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   140 /   321 | ms/batch 4847.39 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   142 /   321 | ms/batch 7428.86 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   144 /   321 | ms/batch 4587.08 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   146 /   321 | ms/batch 5163.54 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   148 /   321 | ms/batch 6002.99 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch   150 /   321 | ms/batch 5068.56 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   152 /   321 | ms/batch 5038.77 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   154 /   321 | ms/batch 5259.59 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   156 /   321 | ms/batch 6008.61 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   158 /   321 | ms/batch 5057.17 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   160 /   321 | ms/batch 4647.45 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   162 /   321 | ms/batch 5076.46 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   164 /   321 | ms/batch 3517.23 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   166 /   321 | ms/batch 4382.03 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   168 /   321 | ms/batch 6179.70 | Loss 00.63 |\n",
      "[Training]| Epochs   2 | Batch   170 /   321 | ms/batch 5975.74 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   172 /   321 | ms/batch 4258.70 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   174 /   321 | ms/batch 4624.91 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   176 /   321 | ms/batch 5210.99 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   178 /   321 | ms/batch 6945.28 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   180 /   321 | ms/batch 4539.01 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   182 /   321 | ms/batch 4898.61 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   184 /   321 | ms/batch 4274.74 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   186 /   321 | ms/batch 6338.67 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   188 /   321 | ms/batch 4171.25 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   190 /   321 | ms/batch 4668.95 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   192 /   321 | ms/batch 8423.79 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   194 /   321 | ms/batch 13559.95 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   196 /   321 | ms/batch 15390.68 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   198 /   321 | ms/batch 12854.82 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   200 /   321 | ms/batch 8551.39 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   202 /   321 | ms/batch 9250.76 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   204 /   321 | ms/batch 8924.30 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   206 /   321 | ms/batch 9187.12 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   208 /   321 | ms/batch 9801.26 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   210 /   321 | ms/batch 8554.29 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   212 /   321 | ms/batch 6782.33 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   214 /   321 | ms/batch 7242.18 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   216 /   321 | ms/batch 7119.78 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   218 /   321 | ms/batch 8741.89 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   220 /   321 | ms/batch 10419.56 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   222 /   321 | ms/batch 8060.27 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   224 /   321 | ms/batch 11297.16 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   226 /   321 | ms/batch 11446.26 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   228 /   321 | ms/batch 9474.27 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   230 /   321 | ms/batch 10978.26 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   232 /   321 | ms/batch 8724.51 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   234 /   321 | ms/batch 10004.71 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   236 /   321 | ms/batch 10439.09 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   238 /   321 | ms/batch 8172.79 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   240 /   321 | ms/batch 9008.31 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   242 /   321 | ms/batch 6325.97 | Loss 00.56 |\n",
      "[Training]| Epochs   2 | Batch   244 /   321 | ms/batch 10214.10 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   246 /   321 | ms/batch 9970.82 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   248 /   321 | ms/batch 10858.69 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   250 /   321 | ms/batch 13262.87 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   252 /   321 | ms/batch 9469.75 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   254 /   321 | ms/batch 10818.77 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   256 /   321 | ms/batch 12713.29 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   258 /   321 | ms/batch 7360.41 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   260 /   321 | ms/batch 12170.08 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   262 /   321 | ms/batch 18204.31 | Loss 00.62 |\n",
      "[Training]| Epochs   2 | Batch   264 /   321 | ms/batch 7612.68 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   266 /   321 | ms/batch 7089.44 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   268 /   321 | ms/batch 8920.49 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   270 /   321 | ms/batch 12277.19 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   272 /   321 | ms/batch 13447.74 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   274 /   321 | ms/batch 15264.66 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   276 /   321 | ms/batch 12490.74 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   278 /   321 | ms/batch 12592.82 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   280 /   321 | ms/batch 12885.10 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   282 /   321 | ms/batch 16944.22 | Loss 00.60 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   2 | Batch   284 /   321 | ms/batch 12940.77 | Loss 00.57 |\n",
      "[Training]| Epochs   2 | Batch   286 /   321 | ms/batch 11895.22 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   288 /   321 | ms/batch 14895.44 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   290 /   321 | ms/batch 20037.59 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   292 /   321 | ms/batch 12436.50 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   294 /   321 | ms/batch 15341.18 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   296 /   321 | ms/batch 12587.15 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   298 /   321 | ms/batch 14812.79 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   300 /   321 | ms/batch 12643.62 | Loss 00.59 |\n",
      "[Training]| Epochs   2 | Batch   302 /   321 | ms/batch 12364.80 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   304 /   321 | ms/batch 11094.65 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   306 /   321 | ms/batch 10148.57 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   308 /   321 | ms/batch 8060.58 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   310 /   321 | ms/batch 9479.07 | Loss 00.58 |\n",
      "[Training]| Epochs   2 | Batch   312 /   321 | ms/batch 14573.46 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   314 /   321 | ms/batch 12718.89 | Loss 00.61 |\n",
      "[Training]| Epochs   2 | Batch   316 /   321 | ms/batch 14861.97 | Loss 00.60 |\n",
      "[Training]| Epochs   2 | Batch   318 /   321 | ms/batch 12121.29 | Loss 00.60 |\n",
      "32\n",
      "[Training]| Epochs   2 | Batch   320 /   321 | ms/batch 15089.08 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   2 | Elapsed 3437.66 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 1.63\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 1.27\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 1.06\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 1.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 1.01\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 1.18\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 1.62\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 1.85\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 2.14\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 1.42\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 1.84\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 1.14\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 1.02\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.94\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.94\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.83\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.91\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.82\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 1.43\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 1.30\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 1.00\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 1.31\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 1.32\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 1.49\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 1.50\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 1.30\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 1.02\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 1.52\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 1.11\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.97\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 1.19\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 1.15\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.85\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 1.15\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 1.46\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 1.05\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.99\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 1.33\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 1.08\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 1.09\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.85\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 1.08\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 1.01\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 1.07\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.92\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.74\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 1.69\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.93\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 1.09\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 1.50\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 1.24\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 1.28\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 1.38\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.99\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.84\n",
      "result= [3, 0.0911110976851739, 0.1132110809778108, 0.1725836357657833, 0.20233515845735092, 0.22377978686884986, 0.12143392704096234, 0.07521447221097403, 0.040932385510796226, 0.02896349186648267, 0.025307337938577795, 0.08421338765953819, 0.07428219944633549, 0.058344697543446754, 0.04626905778749604, 0.04218140005229138, 0.07595639494382025, 0.0798393541047624, 0.08569919919110225, 0.0875813647270252, 0.088805782962017, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5976694064375795]\n",
      "[Training]| Epochs   3 | Batch     2 /   321 | ms/batch 23062.90 | Loss 00.88 |\n",
      "[Training]| Epochs   3 | Batch     4 /   321 | ms/batch 10281.45 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch     6 /   321 | ms/batch 13850.27 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch     8 /   321 | ms/batch 19470.17 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch    10 /   321 | ms/batch 17517.42 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    12 /   321 | ms/batch 18478.43 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    14 /   321 | ms/batch 11500.00 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    16 /   321 | ms/batch 12036.25 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    18 /   321 | ms/batch 13007.44 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    20 /   321 | ms/batch 9892.96 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    22 /   321 | ms/batch 13479.31 | Loss 00.62 |\n",
      "[Training]| Epochs   3 | Batch    24 /   321 | ms/batch 10601.81 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    26 /   321 | ms/batch 9423.29 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    28 /   321 | ms/batch 13698.62 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    30 /   321 | ms/batch 12233.80 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    32 /   321 | ms/batch 9852.18 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    34 /   321 | ms/batch 10290.64 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    36 /   321 | ms/batch 14102.75 | Loss 00.60 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   3 | Batch    38 /   321 | ms/batch 11378.48 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    40 /   321 | ms/batch 12178.48 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    42 /   321 | ms/batch 10569.45 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    44 /   321 | ms/batch 9265.00 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    46 /   321 | ms/batch 9462.80 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    48 /   321 | ms/batch 12355.59 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    50 /   321 | ms/batch 10381.41 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    52 /   321 | ms/batch 8656.08 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    54 /   321 | ms/batch 10921.09 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    56 /   321 | ms/batch 9460.72 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    58 /   321 | ms/batch 11438.06 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    60 /   321 | ms/batch 9494.80 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    62 /   321 | ms/batch 10059.24 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch    64 /   321 | ms/batch 12735.72 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    66 /   321 | ms/batch 9305.75 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch    68 /   321 | ms/batch 8530.97 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    70 /   321 | ms/batch 5945.73 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    72 /   321 | ms/batch 5577.97 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch    74 /   321 | ms/batch 8274.44 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    76 /   321 | ms/batch 11570.44 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    78 /   321 | ms/batch 11668.01 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    80 /   321 | ms/batch 12391.71 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch    82 /   321 | ms/batch 13441.45 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch    84 /   321 | ms/batch 7779.26 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch    86 /   321 | ms/batch 12891.37 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    88 /   321 | ms/batch 11980.25 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    90 /   321 | ms/batch 13554.85 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    92 /   321 | ms/batch 13556.78 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    94 /   321 | ms/batch 9929.91 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch    96 /   321 | ms/batch 10044.35 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch    98 /   321 | ms/batch 15653.02 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   100 /   321 | ms/batch 23771.01 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   102 /   321 | ms/batch 11467.86 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   104 /   321 | ms/batch 13508.94 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   106 /   321 | ms/batch 10283.20 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   108 /   321 | ms/batch 10455.09 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   110 /   321 | ms/batch 14064.40 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   112 /   321 | ms/batch 10018.92 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   114 /   321 | ms/batch 8883.61 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   116 /   321 | ms/batch 8595.22 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   118 /   321 | ms/batch 12437.38 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   120 /   321 | ms/batch 9804.93 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   122 /   321 | ms/batch 10947.82 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   124 /   321 | ms/batch 14205.37 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   126 /   321 | ms/batch 8911.08 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   128 /   321 | ms/batch 10067.70 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   130 /   321 | ms/batch 11065.61 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   132 /   321 | ms/batch 5894.82 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   134 /   321 | ms/batch 11631.80 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   136 /   321 | ms/batch 9852.66 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   138 /   321 | ms/batch 6739.54 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   140 /   321 | ms/batch 9422.80 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   142 /   321 | ms/batch 10465.08 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   144 /   321 | ms/batch 13789.61 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   146 /   321 | ms/batch 10836.39 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   148 /   321 | ms/batch 12860.35 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   150 /   321 | ms/batch 9288.61 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   152 /   321 | ms/batch 12325.12 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   154 /   321 | ms/batch 12530.67 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   156 /   321 | ms/batch 10641.06 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   158 /   321 | ms/batch 9936.91 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   160 /   321 | ms/batch 10502.68 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   162 /   321 | ms/batch 9985.16 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   164 /   321 | ms/batch 6839.53 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   166 /   321 | ms/batch 8016.68 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   168 /   321 | ms/batch 17558.72 | Loss 00.62 |\n",
      "[Training]| Epochs   3 | Batch   170 /   321 | ms/batch 12001.97 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   172 /   321 | ms/batch 10115.02 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   174 /   321 | ms/batch 10897.95 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   176 /   321 | ms/batch 12610.23 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   178 /   321 | ms/batch 14616.33 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   180 /   321 | ms/batch 16750.32 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   182 /   321 | ms/batch 14011.24 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   184 /   321 | ms/batch 11805.58 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   186 /   321 | ms/batch 17583.06 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   188 /   321 | ms/batch 15417.12 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   190 /   321 | ms/batch 16323.47 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   192 /   321 | ms/batch 15192.21 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   194 /   321 | ms/batch 18666.48 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   196 /   321 | ms/batch 18263.39 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   198 /   321 | ms/batch 14366.01 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   200 /   321 | ms/batch 5109.85 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   202 /   321 | ms/batch 6946.83 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   204 /   321 | ms/batch 8865.77 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   206 /   321 | ms/batch 10588.61 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   208 /   321 | ms/batch 10983.46 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   210 /   321 | ms/batch 9337.34 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   212 /   321 | ms/batch 11139.11 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   214 /   321 | ms/batch 13512.46 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   216 /   321 | ms/batch 9194.89 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   218 /   321 | ms/batch 14318.65 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   220 /   321 | ms/batch 12140.36 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   222 /   321 | ms/batch 9446.15 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   224 /   321 | ms/batch 8440.43 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   226 /   321 | ms/batch 7927.36 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   228 /   321 | ms/batch 8669.58 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   230 /   321 | ms/batch 11702.80 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   232 /   321 | ms/batch 10014.35 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   234 /   321 | ms/batch 9978.14 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   236 /   321 | ms/batch 10570.37 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   238 /   321 | ms/batch 12757.31 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   240 /   321 | ms/batch 7291.25 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   242 /   321 | ms/batch 6576.93 | Loss 00.53 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   3 | Batch   244 /   321 | ms/batch 13204.33 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   246 /   321 | ms/batch 10704.33 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   248 /   321 | ms/batch 11945.84 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   250 /   321 | ms/batch 9983.34 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   252 /   321 | ms/batch 9451.25 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   254 /   321 | ms/batch 9456.76 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   256 /   321 | ms/batch 10803.00 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   258 /   321 | ms/batch 5668.83 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   260 /   321 | ms/batch 10365.98 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   262 /   321 | ms/batch 11984.57 | Loss 00.61 |\n",
      "[Training]| Epochs   3 | Batch   264 /   321 | ms/batch 9054.63 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   266 /   321 | ms/batch 12615.69 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   268 /   321 | ms/batch 10515.42 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   270 /   321 | ms/batch 18346.91 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   272 /   321 | ms/batch 17948.48 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   274 /   321 | ms/batch 14425.35 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   276 /   321 | ms/batch 12304.26 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   278 /   321 | ms/batch 19963.30 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   280 /   321 | ms/batch 12793.51 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   282 /   321 | ms/batch 14726.69 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   284 /   321 | ms/batch 14075.63 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   286 /   321 | ms/batch 14706.81 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   288 /   321 | ms/batch 13835.85 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   290 /   321 | ms/batch 13849.29 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   292 /   321 | ms/batch 6866.89 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   294 /   321 | ms/batch 9213.26 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   296 /   321 | ms/batch 9310.18 | Loss 00.57 |\n",
      "[Training]| Epochs   3 | Batch   298 /   321 | ms/batch 12103.97 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   300 /   321 | ms/batch 10916.48 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   302 /   321 | ms/batch 10185.76 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   304 /   321 | ms/batch 9332.62 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   306 /   321 | ms/batch 12789.31 | Loss 00.60 |\n",
      "[Training]| Epochs   3 | Batch   308 /   321 | ms/batch 11091.54 | Loss 00.58 |\n",
      "[Training]| Epochs   3 | Batch   310 /   321 | ms/batch 8984.05 | Loss 00.56 |\n",
      "[Training]| Epochs   3 | Batch   312 /   321 | ms/batch 10178.10 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   314 /   321 | ms/batch 13434.36 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   316 /   321 | ms/batch 10469.78 | Loss 00.59 |\n",
      "[Training]| Epochs   3 | Batch   318 /   321 | ms/batch 7530.91 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs   3 | Batch   320 /   321 | ms/batch 9203.14 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   3 | Elapsed 3784.99 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 1.70\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 1.37\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 1.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 1.08\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 1.27\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 1.38\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 2.02\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 1.54\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 1.63\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 1.57\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 1.81\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 1.26\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 1.77\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 2.24\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 1.93\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 1.41\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 2.07\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 1.89\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.96\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 1.33\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 1.37\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 1.65\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 1.52\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 1.34\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 1.98\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 1.78\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 1.61\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 1.99\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.97\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 1.87\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 1.22\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 1.11\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 1.57\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 1.35\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 1.26\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 1.45\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 1.27\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 1.13\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 1.15\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 1.43\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 1.67\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 2.06\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 1.76\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 1.52\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 2.19\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 1.50\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 2.03\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 1.79\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 1.07\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 1.41\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 1.68\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 1.07\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 1.00\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 1.14\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 1.14\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 1.12\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.92\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 1.38\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.96\n",
      "result= [4, 0.09126351312650709, 0.11373296010707935, 0.17540187345104857, 0.20810252878690083, 0.23092275546661875, 0.12236925632063524, 0.07634469892304109, 0.04124415004429925, 0.029875388855313476, 0.025882872583711882, 0.0846380767465474, 0.07499370738480672, 0.05886292550495056, 0.047684946751464466, 0.04314735989410805, 0.08383921594144841, 0.08793650062962356, 0.09431263243573386, 0.09640603866934448, 0.09764767122599757, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5881757066573625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   4 | Batch     2 /   321 | ms/batch 23991.56 | Loss 00.86 |\n",
      "[Training]| Epochs   4 | Batch     4 /   321 | ms/batch 14007.16 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch     6 /   321 | ms/batch 21047.82 | Loss 00.61 |\n",
      "[Training]| Epochs   4 | Batch     8 /   321 | ms/batch 16566.86 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch    10 /   321 | ms/batch 12443.56 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    12 /   321 | ms/batch 14291.80 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    14 /   321 | ms/batch 6283.33 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    16 /   321 | ms/batch 9326.89 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    18 /   321 | ms/batch 8626.94 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    20 /   321 | ms/batch 12789.95 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    22 /   321 | ms/batch 15196.19 | Loss 00.61 |\n",
      "[Training]| Epochs   4 | Batch    24 /   321 | ms/batch 11022.74 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    26 /   321 | ms/batch 10732.48 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    28 /   321 | ms/batch 11518.69 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    30 /   321 | ms/batch 10576.64 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    32 /   321 | ms/batch 12551.21 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    34 /   321 | ms/batch 8082.89 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    36 /   321 | ms/batch 10424.58 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch    38 /   321 | ms/batch 8543.14 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    40 /   321 | ms/batch 11075.03 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    42 /   321 | ms/batch 11949.64 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    44 /   321 | ms/batch 12377.53 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    46 /   321 | ms/batch 11597.92 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    48 /   321 | ms/batch 10575.78 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    50 /   321 | ms/batch 10741.52 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    52 /   321 | ms/batch 9256.27 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    54 /   321 | ms/batch 12317.23 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    56 /   321 | ms/batch 9047.69 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    58 /   321 | ms/batch 12295.49 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    60 /   321 | ms/batch 7643.32 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    62 /   321 | ms/batch 6836.07 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    64 /   321 | ms/batch 9771.54 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    66 /   321 | ms/batch 7736.11 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch    68 /   321 | ms/batch 12689.56 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    70 /   321 | ms/batch 9199.08 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    72 /   321 | ms/batch 11113.53 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch    74 /   321 | ms/batch 8708.01 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    76 /   321 | ms/batch 11597.52 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    78 /   321 | ms/batch 12024.58 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    80 /   321 | ms/batch 13272.02 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    82 /   321 | ms/batch 18373.01 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    84 /   321 | ms/batch 10185.95 | Loss 00.55 |\n",
      "[Training]| Epochs   4 | Batch    86 /   321 | ms/batch 14799.89 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch    88 /   321 | ms/batch 16082.99 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    90 /   321 | ms/batch 12117.27 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    92 /   321 | ms/batch 18505.61 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch    94 /   321 | ms/batch 13186.62 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch    96 /   321 | ms/batch 12956.44 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch    98 /   321 | ms/batch 16148.70 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   100 /   321 | ms/batch 14326.20 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   102 /   321 | ms/batch 9823.78 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   104 /   321 | ms/batch 11316.87 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   106 /   321 | ms/batch 6000.53 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   108 /   321 | ms/batch 7884.22 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   110 /   321 | ms/batch 14322.80 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   112 /   321 | ms/batch 8733.48 | Loss 00.55 |\n",
      "[Training]| Epochs   4 | Batch   114 /   321 | ms/batch 12200.38 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   116 /   321 | ms/batch 13954.44 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   118 /   321 | ms/batch 11239.85 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   120 /   321 | ms/batch 8813.67 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   122 /   321 | ms/batch 10301.35 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   124 /   321 | ms/batch 12673.91 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   126 /   321 | ms/batch 9575.27 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   128 /   321 | ms/batch 10942.57 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   130 /   321 | ms/batch 10260.45 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   132 /   321 | ms/batch 9256.08 | Loss 00.54 |\n",
      "[Training]| Epochs   4 | Batch   134 /   321 | ms/batch 11541.36 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   136 /   321 | ms/batch 11798.98 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   138 /   321 | ms/batch 10069.63 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   140 /   321 | ms/batch 9364.95 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   142 /   321 | ms/batch 11520.11 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   144 /   321 | ms/batch 11815.18 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   146 /   321 | ms/batch 11192.05 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   148 /   321 | ms/batch 14949.26 | Loss 00.61 |\n",
      "[Training]| Epochs   4 | Batch   150 /   321 | ms/batch 8307.78 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   152 /   321 | ms/batch 9499.34 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   154 /   321 | ms/batch 6088.67 | Loss 00.55 |\n",
      "[Training]| Epochs   4 | Batch   156 /   321 | ms/batch 11934.63 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   158 /   321 | ms/batch 11573.25 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   160 /   321 | ms/batch 10187.67 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   162 /   321 | ms/batch 9772.10 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   164 /   321 | ms/batch 7488.95 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   166 /   321 | ms/batch 10073.68 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   168 /   321 | ms/batch 14512.54 | Loss 00.61 |\n",
      "[Training]| Epochs   4 | Batch   170 /   321 | ms/batch 16699.61 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   172 /   321 | ms/batch 15391.28 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   174 /   321 | ms/batch 9954.70 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   176 /   321 | ms/batch 13179.67 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   178 /   321 | ms/batch 21216.89 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   180 /   321 | ms/batch 17341.71 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   182 /   321 | ms/batch 14787.53 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   184 /   321 | ms/batch 18115.52 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   186 /   321 | ms/batch 16733.38 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   188 /   321 | ms/batch 10212.01 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   190 /   321 | ms/batch 11551.96 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   192 /   321 | ms/batch 10517.35 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   194 /   321 | ms/batch 9947.97 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   196 /   321 | ms/batch 16207.40 | Loss 00.61 |\n",
      "[Training]| Epochs   4 | Batch   198 /   321 | ms/batch 10501.65 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   200 /   321 | ms/batch 10117.47 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   202 /   321 | ms/batch 10245.31 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   204 /   321 | ms/batch 10239.91 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   206 /   321 | ms/batch 10933.81 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   4 | Batch   208 /   321 | ms/batch 9927.46 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   210 /   321 | ms/batch 8225.00 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   212 /   321 | ms/batch 8864.33 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   214 /   321 | ms/batch 9084.65 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   216 /   321 | ms/batch 9729.47 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   218 /   321 | ms/batch 7958.49 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   220 /   321 | ms/batch 11364.61 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   222 /   321 | ms/batch 13306.83 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   224 /   321 | ms/batch 9453.15 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   226 /   321 | ms/batch 12299.82 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   228 /   321 | ms/batch 9749.26 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   230 /   321 | ms/batch 10190.90 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   232 /   321 | ms/batch 11004.39 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   234 /   321 | ms/batch 11639.89 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   236 /   321 | ms/batch 12211.35 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   238 /   321 | ms/batch 13513.25 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   240 /   321 | ms/batch 6662.96 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   242 /   321 | ms/batch 5500.09 | Loss 00.53 |\n",
      "[Training]| Epochs   4 | Batch   244 /   321 | ms/batch 8603.44 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   246 /   321 | ms/batch 4817.11 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   248 /   321 | ms/batch 5056.01 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   250 /   321 | ms/batch 4854.91 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   252 /   321 | ms/batch 5340.16 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   254 /   321 | ms/batch 4561.64 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   256 /   321 | ms/batch 5016.33 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   258 /   321 | ms/batch 3022.21 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   260 /   321 | ms/batch 5344.23 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   262 /   321 | ms/batch 6115.68 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   264 /   321 | ms/batch 4086.92 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   266 /   321 | ms/batch 4371.98 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   268 /   321 | ms/batch 4665.46 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   270 /   321 | ms/batch 4776.66 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   272 /   321 | ms/batch 5115.84 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   274 /   321 | ms/batch 5241.43 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   276 /   321 | ms/batch 4468.37 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   278 /   321 | ms/batch 5610.45 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   280 /   321 | ms/batch 5118.27 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   282 /   321 | ms/batch 6190.61 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   284 /   321 | ms/batch 5382.81 | Loss 00.55 |\n",
      "[Training]| Epochs   4 | Batch   286 /   321 | ms/batch 6642.40 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   288 /   321 | ms/batch 6171.86 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   290 /   321 | ms/batch 4837.28 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   292 /   321 | ms/batch 4604.42 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   294 /   321 | ms/batch 3665.79 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   296 /   321 | ms/batch 4517.21 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   298 /   321 | ms/batch 5136.53 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   300 /   321 | ms/batch 5405.23 | Loss 00.58 |\n",
      "[Training]| Epochs   4 | Batch   302 /   321 | ms/batch 3588.03 | Loss 00.56 |\n",
      "[Training]| Epochs   4 | Batch   304 /   321 | ms/batch 6201.06 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   306 /   321 | ms/batch 6742.93 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   308 /   321 | ms/batch 3260.61 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   310 /   321 | ms/batch 5088.68 | Loss 00.57 |\n",
      "[Training]| Epochs   4 | Batch   312 /   321 | ms/batch 6764.62 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   314 /   321 | ms/batch 6502.36 | Loss 00.60 |\n",
      "[Training]| Epochs   4 | Batch   316 /   321 | ms/batch 6289.82 | Loss 00.59 |\n",
      "[Training]| Epochs   4 | Batch   318 /   321 | ms/batch 4157.59 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs   4 | Batch   320 /   321 | ms/batch 5267.74 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   4 | Elapsed 1688.65 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.85\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.90\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.88\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 1.07\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.88\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.29\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.82\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.61\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.59\n",
      "result= [5, 0.0922882960164166, 0.11785875589514931, 0.17461326878607877, 0.2077556797423032, 0.22807798437652232, 0.11909559789525283, 0.08082644194248405, 0.04090640215974066, 0.029672752017632927, 0.025283358567223732, 0.0841483683299944, 0.07899941900049627, 0.058420717948503266, 0.04736567770050358, 0.04219810682075245, 0.08371996794067271, 0.08876606449884067, 0.09446862129137408, 0.09658904730295871, 0.09767815231259212, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5863154339201656]\n",
      "[Training]| Epochs   5 | Batch     2 /   321 | ms/batch 6051.43 | Loss 00.86 |\n",
      "[Training]| Epochs   5 | Batch     4 /   321 | ms/batch 4323.44 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch     6 /   321 | ms/batch 7403.36 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch     8 /   321 | ms/batch 5694.65 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch    10 /   321 | ms/batch 5480.57 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    12 /   321 | ms/batch 5210.87 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    14 /   321 | ms/batch 3859.29 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    16 /   321 | ms/batch 3889.58 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    18 /   321 | ms/batch 4176.27 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    20 /   321 | ms/batch 4574.81 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    22 /   321 | ms/batch 5836.74 | Loss 00.61 |\n",
      "[Training]| Epochs   5 | Batch    24 /   321 | ms/batch 4434.39 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    26 /   321 | ms/batch 4463.80 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    28 /   321 | ms/batch 6397.40 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    30 /   321 | ms/batch 6349.91 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    32 /   321 | ms/batch 5707.18 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    34 /   321 | ms/batch 5609.67 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    36 /   321 | ms/batch 5588.51 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    38 /   321 | ms/batch 5689.46 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    40 /   321 | ms/batch 4541.72 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    42 /   321 | ms/batch 5998.41 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    44 /   321 | ms/batch 4475.94 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    46 /   321 | ms/batch 4777.09 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    48 /   321 | ms/batch 4409.56 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    50 /   321 | ms/batch 5405.71 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    52 /   321 | ms/batch 5419.52 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    54 /   321 | ms/batch 5442.89 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    56 /   321 | ms/batch 5085.83 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    58 /   321 | ms/batch 4253.32 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    60 /   321 | ms/batch 3959.17 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    62 /   321 | ms/batch 4139.43 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    64 /   321 | ms/batch 5130.98 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    66 /   321 | ms/batch 3514.08 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    68 /   321 | ms/batch 5393.28 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    70 /   321 | ms/batch 5348.03 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    72 /   321 | ms/batch 3193.99 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    74 /   321 | ms/batch 4238.53 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    76 /   321 | ms/batch 4523.39 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    78 /   321 | ms/batch 6099.10 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    80 /   321 | ms/batch 6063.47 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch    82 /   321 | ms/batch 5301.94 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    84 /   321 | ms/batch 4684.55 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    86 /   321 | ms/batch 5344.78 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    88 /   321 | ms/batch 6180.23 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    90 /   321 | ms/batch 3763.89 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    92 /   321 | ms/batch 4773.25 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch    94 /   321 | ms/batch 4435.53 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch    96 /   321 | ms/batch 4524.71 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch    98 /   321 | ms/batch 6628.09 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   100 /   321 | ms/batch 5020.37 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   102 /   321 | ms/batch 3632.70 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   104 /   321 | ms/batch 5252.11 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   106 /   321 | ms/batch 5047.87 | Loss 00.55 |\n",
      "[Training]| Epochs   5 | Batch   108 /   321 | ms/batch 5051.04 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   110 /   321 | ms/batch 4526.93 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   112 /   321 | ms/batch 2842.42 | Loss 00.54 |\n",
      "[Training]| Epochs   5 | Batch   114 /   321 | ms/batch 6030.49 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   116 /   321 | ms/batch 5515.66 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   118 /   321 | ms/batch 5132.79 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   120 /   321 | ms/batch 4989.46 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   122 /   321 | ms/batch 4489.06 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   124 /   321 | ms/batch 4598.35 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   126 /   321 | ms/batch 6400.93 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   128 /   321 | ms/batch 5514.65 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   130 /   321 | ms/batch 7080.01 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   132 /   321 | ms/batch 3201.20 | Loss 00.55 |\n",
      "[Training]| Epochs   5 | Batch   134 /   321 | ms/batch 5870.63 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   136 /   321 | ms/batch 4454.82 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   138 /   321 | ms/batch 3694.15 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   140 /   321 | ms/batch 4424.11 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   142 /   321 | ms/batch 5306.15 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   144 /   321 | ms/batch 5110.43 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   146 /   321 | ms/batch 4596.09 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   148 /   321 | ms/batch 5655.70 | Loss 00.61 |\n",
      "[Training]| Epochs   5 | Batch   150 /   321 | ms/batch 3925.33 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   152 /   321 | ms/batch 6075.22 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   154 /   321 | ms/batch 5346.65 | Loss 00.55 |\n",
      "[Training]| Epochs   5 | Batch   156 /   321 | ms/batch 6990.27 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   158 /   321 | ms/batch 7760.95 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   160 /   321 | ms/batch 4061.24 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   162 /   321 | ms/batch 4591.56 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   164 /   321 | ms/batch 3854.03 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   166 /   321 | ms/batch 3992.95 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   168 /   321 | ms/batch 6917.90 | Loss 00.61 |\n",
      "[Training]| Epochs   5 | Batch   170 /   321 | ms/batch 5206.28 | Loss 00.60 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   5 | Batch   172 /   321 | ms/batch 5608.78 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   174 /   321 | ms/batch 4431.36 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   176 /   321 | ms/batch 5344.65 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   178 /   321 | ms/batch 5750.13 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   180 /   321 | ms/batch 4065.62 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   182 /   321 | ms/batch 4938.77 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   184 /   321 | ms/batch 5211.04 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   186 /   321 | ms/batch 5416.60 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   188 /   321 | ms/batch 3635.23 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   190 /   321 | ms/batch 5279.49 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   192 /   321 | ms/batch 6297.20 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   194 /   321 | ms/batch 5873.56 | Loss 00.61 |\n",
      "[Training]| Epochs   5 | Batch   196 /   321 | ms/batch 6468.70 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   198 /   321 | ms/batch 5876.52 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   200 /   321 | ms/batch 4016.08 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   202 /   321 | ms/batch 3504.50 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   204 /   321 | ms/batch 5398.32 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   206 /   321 | ms/batch 5604.20 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   208 /   321 | ms/batch 3622.46 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   210 /   321 | ms/batch 4426.42 | Loss 00.55 |\n",
      "[Training]| Epochs   5 | Batch   212 /   321 | ms/batch 5202.66 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   214 /   321 | ms/batch 6297.49 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   216 /   321 | ms/batch 4313.82 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   218 /   321 | ms/batch 4667.50 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   220 /   321 | ms/batch 4846.06 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   222 /   321 | ms/batch 5287.28 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   224 /   321 | ms/batch 5200.94 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   226 /   321 | ms/batch 6057.31 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   228 /   321 | ms/batch 5383.75 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   230 /   321 | ms/batch 4769.09 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   232 /   321 | ms/batch 4101.28 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   234 /   321 | ms/batch 4415.97 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   236 /   321 | ms/batch 6049.59 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   238 /   321 | ms/batch 4044.87 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   240 /   321 | ms/batch 4478.41 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   242 /   321 | ms/batch 3247.35 | Loss 00.53 |\n",
      "[Training]| Epochs   5 | Batch   244 /   321 | ms/batch 5709.42 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   246 /   321 | ms/batch 4437.24 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   248 /   321 | ms/batch 6865.27 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   250 /   321 | ms/batch 4992.83 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   252 /   321 | ms/batch 5368.78 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   254 /   321 | ms/batch 5398.22 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   256 /   321 | ms/batch 6373.21 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   258 /   321 | ms/batch 3685.59 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   260 /   321 | ms/batch 4332.38 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   262 /   321 | ms/batch 5415.71 | Loss 00.61 |\n",
      "[Training]| Epochs   5 | Batch   264 /   321 | ms/batch 4135.73 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   266 /   321 | ms/batch 3991.75 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   268 /   321 | ms/batch 4748.15 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   270 /   321 | ms/batch 4644.71 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   272 /   321 | ms/batch 5290.60 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   274 /   321 | ms/batch 6023.83 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   276 /   321 | ms/batch 6123.42 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   278 /   321 | ms/batch 7372.72 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   280 /   321 | ms/batch 5932.36 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   282 /   321 | ms/batch 5043.89 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   284 /   321 | ms/batch 3932.06 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   286 /   321 | ms/batch 5274.98 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   288 /   321 | ms/batch 5773.18 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   290 /   321 | ms/batch 6074.52 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   292 /   321 | ms/batch 3643.55 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   294 /   321 | ms/batch 4654.72 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   296 /   321 | ms/batch 4057.01 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   298 /   321 | ms/batch 5875.89 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   300 /   321 | ms/batch 5234.22 | Loss 00.58 |\n",
      "[Training]| Epochs   5 | Batch   302 /   321 | ms/batch 4224.65 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   304 /   321 | ms/batch 6617.17 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   306 /   321 | ms/batch 6614.79 | Loss 00.60 |\n",
      "[Training]| Epochs   5 | Batch   308 /   321 | ms/batch 4884.35 | Loss 00.57 |\n",
      "[Training]| Epochs   5 | Batch   310 /   321 | ms/batch 3431.62 | Loss 00.56 |\n",
      "[Training]| Epochs   5 | Batch   312 /   321 | ms/batch 6342.54 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   314 /   321 | ms/batch 4990.28 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   316 /   321 | ms/batch 6072.98 | Loss 00.59 |\n",
      "[Training]| Epochs   5 | Batch   318 /   321 | ms/batch 5421.41 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs   5 | Batch   320 /   321 | ms/batch 5703.63 | Loss 00.61 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   5 | Elapsed 1659.80 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.74\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 1.05\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.92\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.82\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.28\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 1.16\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.92\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.27\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.75\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 1.08\n",
      "result= [6, 0.09264878044812269, 0.11891176695017172, 0.17285680733708228, 0.20671763014997807, 0.231721397869678, 0.1197191864275319, 0.08078748029558712, 0.040529677761966885, 0.029571429138897178, 0.025732984416482974, 0.08457185621848816, 0.07918288720741183, 0.057907562374789924, 0.047209064010893656, 0.0429378786035004, 0.08371838082981152, 0.08885954616414216, 0.09435479505320057, 0.09654376663057829, 0.09786812362510824, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5844390215697112]\n",
      "[Training]| Epochs   6 | Batch     2 /   321 | ms/batch 7242.44 | Loss 00.85 |\n",
      "[Training]| Epochs   6 | Batch     4 /   321 | ms/batch 4370.92 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch     6 /   321 | ms/batch 5713.68 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch     8 /   321 | ms/batch 5121.49 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch    10 /   321 | ms/batch 5161.35 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    12 /   321 | ms/batch 5121.27 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    14 /   321 | ms/batch 3734.61 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    16 /   321 | ms/batch 3768.46 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch    18 /   321 | ms/batch 4147.00 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    20 /   321 | ms/batch 4807.51 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    22 /   321 | ms/batch 8467.28 | Loss 00.61 |\n",
      "[Training]| Epochs   6 | Batch    24 /   321 | ms/batch 5997.02 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    26 /   321 | ms/batch 5656.59 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    28 /   321 | ms/batch 5092.07 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    30 /   321 | ms/batch 4400.73 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    32 /   321 | ms/batch 4955.83 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    34 /   321 | ms/batch 3391.01 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch    36 /   321 | ms/batch 5355.04 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    38 /   321 | ms/batch 5239.79 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    40 /   321 | ms/batch 5035.05 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    42 /   321 | ms/batch 6292.79 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    44 /   321 | ms/batch 4904.26 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    46 /   321 | ms/batch 4346.82 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    48 /   321 | ms/batch 5048.28 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    50 /   321 | ms/batch 3934.58 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    52 /   321 | ms/batch 4004.07 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    54 /   321 | ms/batch 4712.01 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch    56 /   321 | ms/batch 5342.13 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    58 /   321 | ms/batch 4662.80 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    60 /   321 | ms/batch 4143.34 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    62 /   321 | ms/batch 5534.51 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch    64 /   321 | ms/batch 4485.00 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    66 /   321 | ms/batch 3537.79 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    68 /   321 | ms/batch 3545.94 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    70 /   321 | ms/batch 5614.06 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    72 /   321 | ms/batch 4660.47 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    74 /   321 | ms/batch 3594.18 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    76 /   321 | ms/batch 6150.83 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    78 /   321 | ms/batch 4638.20 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    80 /   321 | ms/batch 4884.12 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    82 /   321 | ms/batch 4300.24 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    84 /   321 | ms/batch 3845.48 | Loss 00.55 |\n",
      "[Training]| Epochs   6 | Batch    86 /   321 | ms/batch 5260.19 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch    88 /   321 | ms/batch 7137.45 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    90 /   321 | ms/batch 4581.42 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch    92 /   321 | ms/batch 4589.43 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch    94 /   321 | ms/batch 4578.29 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch    96 /   321 | ms/batch 5123.88 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch    98 /   321 | ms/batch 5691.20 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   100 /   321 | ms/batch 5847.04 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   102 /   321 | ms/batch 5040.18 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   104 /   321 | ms/batch 6440.52 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   106 /   321 | ms/batch 4204.07 | Loss 00.55 |\n",
      "[Training]| Epochs   6 | Batch   108 /   321 | ms/batch 4403.49 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   110 /   321 | ms/batch 4771.22 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   112 /   321 | ms/batch 4424.55 | Loss 00.55 |\n",
      "[Training]| Epochs   6 | Batch   114 /   321 | ms/batch 5789.79 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   116 /   321 | ms/batch 4275.60 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   118 /   321 | ms/batch 5531.81 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   120 /   321 | ms/batch 4963.53 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   122 /   321 | ms/batch 5173.18 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   124 /   321 | ms/batch 4198.29 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   126 /   321 | ms/batch 5778.21 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   128 /   321 | ms/batch 5023.80 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   130 /   321 | ms/batch 4839.74 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   132 /   321 | ms/batch 3128.34 | Loss 00.54 |\n",
      "[Training]| Epochs   6 | Batch   134 /   321 | ms/batch 5548.79 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   6 | Batch   136 /   321 | ms/batch 4784.19 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   138 /   321 | ms/batch 4111.84 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   140 /   321 | ms/batch 4290.20 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   142 /   321 | ms/batch 5496.00 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   144 /   321 | ms/batch 5228.96 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   146 /   321 | ms/batch 5658.94 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   148 /   321 | ms/batch 8406.29 | Loss 00.61 |\n",
      "[Training]| Epochs   6 | Batch   150 /   321 | ms/batch 5523.46 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   152 /   321 | ms/batch 6326.36 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   154 /   321 | ms/batch 3590.03 | Loss 00.54 |\n",
      "[Training]| Epochs   6 | Batch   156 /   321 | ms/batch 5117.86 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   158 /   321 | ms/batch 5486.18 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   160 /   321 | ms/batch 5100.94 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   162 /   321 | ms/batch 5534.71 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   164 /   321 | ms/batch 3615.38 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   166 /   321 | ms/batch 6303.19 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   168 /   321 | ms/batch 5870.66 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   170 /   321 | ms/batch 6921.86 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   172 /   321 | ms/batch 4977.39 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   174 /   321 | ms/batch 4728.34 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   176 /   321 | ms/batch 6495.33 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   178 /   321 | ms/batch 5837.89 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   180 /   321 | ms/batch 4593.37 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   182 /   321 | ms/batch 5273.88 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   184 /   321 | ms/batch 4630.62 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   186 /   321 | ms/batch 8205.35 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   188 /   321 | ms/batch 3898.46 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   190 /   321 | ms/batch 5632.81 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   192 /   321 | ms/batch 5679.81 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   194 /   321 | ms/batch 6562.41 | Loss 00.61 |\n",
      "[Training]| Epochs   6 | Batch   196 /   321 | ms/batch 5601.50 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   198 /   321 | ms/batch 4367.19 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   200 /   321 | ms/batch 5132.84 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   202 /   321 | ms/batch 5184.50 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   204 /   321 | ms/batch 4518.59 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   206 /   321 | ms/batch 4405.62 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   208 /   321 | ms/batch 3945.96 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   210 /   321 | ms/batch 3794.01 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   212 /   321 | ms/batch 5245.04 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   214 /   321 | ms/batch 4701.45 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   216 /   321 | ms/batch 5103.03 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   218 /   321 | ms/batch 6697.22 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   220 /   321 | ms/batch 4676.77 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   222 /   321 | ms/batch 5093.04 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   224 /   321 | ms/batch 4947.31 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   226 /   321 | ms/batch 6128.25 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   228 /   321 | ms/batch 4699.82 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   230 /   321 | ms/batch 5461.16 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   232 /   321 | ms/batch 4343.54 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   234 /   321 | ms/batch 6335.88 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   236 /   321 | ms/batch 6548.31 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   238 /   321 | ms/batch 4413.60 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   240 /   321 | ms/batch 4585.83 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   242 /   321 | ms/batch 2572.27 | Loss 00.52 |\n",
      "[Training]| Epochs   6 | Batch   244 /   321 | ms/batch 6867.96 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   246 /   321 | ms/batch 5152.49 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   248 /   321 | ms/batch 4518.55 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   250 /   321 | ms/batch 4818.74 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   252 /   321 | ms/batch 4704.42 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   254 /   321 | ms/batch 4315.79 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   256 /   321 | ms/batch 5101.82 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   258 /   321 | ms/batch 3920.82 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   260 /   321 | ms/batch 4141.53 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   262 /   321 | ms/batch 5484.06 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   264 /   321 | ms/batch 4052.86 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   266 /   321 | ms/batch 5479.22 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   268 /   321 | ms/batch 6702.06 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   270 /   321 | ms/batch 6675.45 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   272 /   321 | ms/batch 7251.74 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   274 /   321 | ms/batch 5619.74 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   276 /   321 | ms/batch 4386.56 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   278 /   321 | ms/batch 5714.04 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   280 /   321 | ms/batch 5021.61 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   282 /   321 | ms/batch 4698.60 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   284 /   321 | ms/batch 3747.50 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   286 /   321 | ms/batch 6478.03 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   288 /   321 | ms/batch 4816.53 | Loss 00.60 |\n",
      "[Training]| Epochs   6 | Batch   290 /   321 | ms/batch 4729.34 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   292 /   321 | ms/batch 4807.46 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   294 /   321 | ms/batch 5200.72 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   296 /   321 | ms/batch 5258.87 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   298 /   321 | ms/batch 5441.17 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   300 /   321 | ms/batch 5488.26 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   302 /   321 | ms/batch 4136.18 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   304 /   321 | ms/batch 5063.95 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   306 /   321 | ms/batch 7749.02 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   308 /   321 | ms/batch 4390.48 | Loss 00.57 |\n",
      "[Training]| Epochs   6 | Batch   310 /   321 | ms/batch 4902.70 | Loss 00.56 |\n",
      "[Training]| Epochs   6 | Batch   312 /   321 | ms/batch 4972.92 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   314 /   321 | ms/batch 5158.59 | Loss 00.59 |\n",
      "[Training]| Epochs   6 | Batch   316 /   321 | ms/batch 4900.58 | Loss 00.58 |\n",
      "[Training]| Epochs   6 | Batch   318 /   321 | ms/batch 4852.90 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs   6 | Batch   320 /   321 | ms/batch 5905.39 | Loss 00.61 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   6 | Elapsed 1740.67 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 1.22\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 1.04\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.94\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.27\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.76\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.56\n",
      "result= [7, 0.09246407536352787, 0.11805731044183554, 0.17448477622409025, 0.20910363855161243, 0.2304867085124586, 0.12003097474714415, 0.07930652145659282, 0.04088042475521239, 0.029859805980512896, 0.02557710809625877, 0.08458937468792625, 0.078098763921534, 0.05835285617890808, 0.04762479465451913, 0.04267605003303415, 0.08417590937649051, 0.08916907858637772, 0.09492785359924442, 0.09712247263849949, 0.09827353412485794, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847565561165045]\n",
      "[Training]| Epochs   7 | Batch     2 /   321 | ms/batch 6674.40 | Loss 00.85 |\n",
      "[Training]| Epochs   7 | Batch     4 /   321 | ms/batch 4521.90 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch     6 /   321 | ms/batch 5618.48 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch     8 /   321 | ms/batch 5010.79 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch    10 /   321 | ms/batch 4957.01 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    12 /   321 | ms/batch 5226.25 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    14 /   321 | ms/batch 3990.90 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    16 /   321 | ms/batch 5414.20 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    18 /   321 | ms/batch 6106.76 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    20 /   321 | ms/batch 6076.18 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    22 /   321 | ms/batch 7986.68 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch    24 /   321 | ms/batch 4258.44 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    26 /   321 | ms/batch 4889.69 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    28 /   321 | ms/batch 4346.60 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    30 /   321 | ms/batch 5671.38 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    32 /   321 | ms/batch 5245.57 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    34 /   321 | ms/batch 3522.50 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    36 /   321 | ms/batch 6171.69 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    38 /   321 | ms/batch 6100.95 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    40 /   321 | ms/batch 5461.80 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    42 /   321 | ms/batch 4944.29 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    44 /   321 | ms/batch 4303.71 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    46 /   321 | ms/batch 4523.96 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    48 /   321 | ms/batch 4491.46 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    50 /   321 | ms/batch 5381.60 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    52 /   321 | ms/batch 4782.97 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    54 /   321 | ms/batch 5319.98 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    56 /   321 | ms/batch 5015.42 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    58 /   321 | ms/batch 4253.43 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    60 /   321 | ms/batch 2988.45 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    62 /   321 | ms/batch 5320.43 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    64 /   321 | ms/batch 3557.88 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    66 /   321 | ms/batch 3626.54 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    68 /   321 | ms/batch 4383.93 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    70 /   321 | ms/batch 3631.09 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    72 /   321 | ms/batch 3740.76 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    74 /   321 | ms/batch 4739.19 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    76 /   321 | ms/batch 4277.37 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch    78 /   321 | ms/batch 5493.80 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    80 /   321 | ms/batch 4927.40 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    82 /   321 | ms/batch 6920.78 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    84 /   321 | ms/batch 3981.11 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch    86 /   321 | ms/batch 7492.11 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch    88 /   321 | ms/batch 5708.11 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    90 /   321 | ms/batch 5339.05 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    92 /   321 | ms/batch 6177.96 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch    94 /   321 | ms/batch 5993.16 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch    96 /   321 | ms/batch 3367.27 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch    98 /   321 | ms/batch 6492.03 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   7 | Batch   100 /   321 | ms/batch 5704.44 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   102 /   321 | ms/batch 3548.92 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   104 /   321 | ms/batch 7017.85 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   106 /   321 | ms/batch 4187.01 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   108 /   321 | ms/batch 4121.84 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   110 /   321 | ms/batch 5972.25 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   112 /   321 | ms/batch 4765.83 | Loss 00.54 |\n",
      "[Training]| Epochs   7 | Batch   114 /   321 | ms/batch 4432.99 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   116 /   321 | ms/batch 6616.24 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   118 /   321 | ms/batch 6232.33 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   120 /   321 | ms/batch 6189.22 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   122 /   321 | ms/batch 4693.48 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   124 /   321 | ms/batch 4782.20 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   126 /   321 | ms/batch 4340.05 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   128 /   321 | ms/batch 4894.37 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   130 /   321 | ms/batch 4871.18 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   132 /   321 | ms/batch 3276.34 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   134 /   321 | ms/batch 5275.45 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   136 /   321 | ms/batch 4501.30 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   138 /   321 | ms/batch 3639.78 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   140 /   321 | ms/batch 5040.14 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   142 /   321 | ms/batch 8012.38 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   144 /   321 | ms/batch 6717.33 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   146 /   321 | ms/batch 6011.91 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   148 /   321 | ms/batch 6710.65 | Loss 00.61 |\n",
      "[Training]| Epochs   7 | Batch   150 /   321 | ms/batch 3972.03 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   152 /   321 | ms/batch 4995.31 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   154 /   321 | ms/batch 3671.85 | Loss 00.54 |\n",
      "[Training]| Epochs   7 | Batch   156 /   321 | ms/batch 5273.52 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   158 /   321 | ms/batch 7616.05 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   160 /   321 | ms/batch 5407.32 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   162 /   321 | ms/batch 6202.37 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   164 /   321 | ms/batch 3786.55 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   166 /   321 | ms/batch 4628.21 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   168 /   321 | ms/batch 6884.01 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   170 /   321 | ms/batch 4670.75 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   172 /   321 | ms/batch 6072.47 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   174 /   321 | ms/batch 4266.11 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   176 /   321 | ms/batch 5624.60 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   178 /   321 | ms/batch 6351.48 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   180 /   321 | ms/batch 5549.96 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   182 /   321 | ms/batch 6119.12 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   184 /   321 | ms/batch 4600.37 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   186 /   321 | ms/batch 5555.51 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   188 /   321 | ms/batch 4708.60 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   190 /   321 | ms/batch 5274.35 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   192 /   321 | ms/batch 7125.88 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   194 /   321 | ms/batch 6953.42 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   196 /   321 | ms/batch 7024.61 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   198 /   321 | ms/batch 5371.62 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   200 /   321 | ms/batch 3944.52 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   202 /   321 | ms/batch 5556.09 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   204 /   321 | ms/batch 5413.92 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   206 /   321 | ms/batch 3755.60 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   208 /   321 | ms/batch 5179.02 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   210 /   321 | ms/batch 3791.38 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   212 /   321 | ms/batch 5702.87 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   214 /   321 | ms/batch 6121.25 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   216 /   321 | ms/batch 3375.65 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   218 /   321 | ms/batch 5368.46 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   220 /   321 | ms/batch 4592.99 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   222 /   321 | ms/batch 4146.56 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   224 /   321 | ms/batch 5266.07 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   226 /   321 | ms/batch 4808.77 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   228 /   321 | ms/batch 4763.49 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   230 /   321 | ms/batch 7068.31 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   232 /   321 | ms/batch 4734.83 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   234 /   321 | ms/batch 4763.76 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   236 /   321 | ms/batch 4759.78 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   238 /   321 | ms/batch 3740.50 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   240 /   321 | ms/batch 3550.61 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   242 /   321 | ms/batch 4379.43 | Loss 00.52 |\n",
      "[Training]| Epochs   7 | Batch   244 /   321 | ms/batch 5848.71 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   246 /   321 | ms/batch 4512.58 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   248 /   321 | ms/batch 4965.46 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   250 /   321 | ms/batch 4732.99 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   252 /   321 | ms/batch 4669.86 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   254 /   321 | ms/batch 4254.36 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   256 /   321 | ms/batch 4971.37 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   258 /   321 | ms/batch 4065.37 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   260 /   321 | ms/batch 4483.46 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   262 /   321 | ms/batch 8036.22 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   264 /   321 | ms/batch 5949.57 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   266 /   321 | ms/batch 5809.37 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   268 /   321 | ms/batch 6467.25 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   270 /   321 | ms/batch 4622.90 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   272 /   321 | ms/batch 6245.45 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   274 /   321 | ms/batch 5102.48 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   276 /   321 | ms/batch 4298.10 | Loss 00.56 |\n",
      "[Training]| Epochs   7 | Batch   278 /   321 | ms/batch 5715.19 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   280 /   321 | ms/batch 4277.43 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   282 /   321 | ms/batch 4457.52 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   284 /   321 | ms/batch 5408.20 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   286 /   321 | ms/batch 6063.18 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   288 /   321 | ms/batch 6181.32 | Loss 00.60 |\n",
      "[Training]| Epochs   7 | Batch   290 /   321 | ms/batch 4988.14 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   292 /   321 | ms/batch 5224.69 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   294 /   321 | ms/batch 4480.45 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   296 /   321 | ms/batch 4103.18 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   298 /   321 | ms/batch 5935.48 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   300 /   321 | ms/batch 5165.63 | Loss 00.58 |\n",
      "[Training]| Epochs   7 | Batch   302 /   321 | ms/batch 4308.32 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   304 /   321 | ms/batch 7236.41 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   306 /   321 | ms/batch 6108.95 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   7 | Batch   308 /   321 | ms/batch 5243.90 | Loss 00.57 |\n",
      "[Training]| Epochs   7 | Batch   310 /   321 | ms/batch 4317.76 | Loss 00.55 |\n",
      "[Training]| Epochs   7 | Batch   312 /   321 | ms/batch 7664.75 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   314 /   321 | ms/batch 5103.32 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   316 /   321 | ms/batch 4598.59 | Loss 00.59 |\n",
      "[Training]| Epochs   7 | Batch   318 /   321 | ms/batch 5473.36 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs   7 | Batch   320 /   321 | ms/batch 5909.11 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   7 | Elapsed 1790.37 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.83\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.86\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.88\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.83\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 1.00\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.84\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 1.09\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.78\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.79\n",
      "result= [8, 0.0927313717658503, 0.11924560499302782, 0.17414939208413507, 0.2082148943668404, 0.22734415912107853, 0.1197191745344773, 0.08031979186964147, 0.040776497297517415, 0.029992303528640758, 0.02518742918875286, 0.08450961986372273, 0.07891192774427003, 0.058263129028415454, 0.04784561597269583, 0.04203071802366737, 0.08343653099296364, 0.08859036506653785, 0.09423380003518346, 0.09645106146962833, 0.09743296597040435, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5839098809677877]\n",
      "[Training]| Epochs   8 | Batch     2 /   321 | ms/batch 6736.71 | Loss 00.85 |\n",
      "[Training]| Epochs   8 | Batch     4 /   321 | ms/batch 4392.76 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch     6 /   321 | ms/batch 5679.29 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch     8 /   321 | ms/batch 5344.08 | Loss 00.61 |\n",
      "[Training]| Epochs   8 | Batch    10 /   321 | ms/batch 5982.63 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    12 /   321 | ms/batch 7524.14 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    14 /   321 | ms/batch 5313.87 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    16 /   321 | ms/batch 5395.67 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    18 /   321 | ms/batch 5951.87 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    20 /   321 | ms/batch 4139.08 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    22 /   321 | ms/batch 6588.57 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch    24 /   321 | ms/batch 4125.88 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    26 /   321 | ms/batch 5441.15 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    28 /   321 | ms/batch 5594.76 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    30 /   321 | ms/batch 4420.73 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    32 /   321 | ms/batch 4502.78 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    34 /   321 | ms/batch 3853.20 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    36 /   321 | ms/batch 6282.11 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    38 /   321 | ms/batch 5516.23 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    40 /   321 | ms/batch 5464.43 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    42 /   321 | ms/batch 6565.26 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch    44 /   321 | ms/batch 4625.74 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    46 /   321 | ms/batch 4641.72 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    48 /   321 | ms/batch 6351.82 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    50 /   321 | ms/batch 3548.71 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    52 /   321 | ms/batch 5013.69 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    54 /   321 | ms/batch 5664.50 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    56 /   321 | ms/batch 5272.15 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    58 /   321 | ms/batch 5665.24 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    60 /   321 | ms/batch 3733.83 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    62 /   321 | ms/batch 4187.49 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   8 | Batch    64 /   321 | ms/batch 4605.97 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    66 /   321 | ms/batch 5763.23 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    68 /   321 | ms/batch 4677.82 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    70 /   321 | ms/batch 5207.72 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    72 /   321 | ms/batch 3099.22 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    74 /   321 | ms/batch 3947.13 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    76 /   321 | ms/batch 4045.11 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    78 /   321 | ms/batch 5904.27 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    80 /   321 | ms/batch 3963.59 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch    82 /   321 | ms/batch 5361.64 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    84 /   321 | ms/batch 4631.94 | Loss 00.55 |\n",
      "[Training]| Epochs   8 | Batch    86 /   321 | ms/batch 4927.42 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    88 /   321 | ms/batch 7348.24 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    90 /   321 | ms/batch 4528.07 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    92 /   321 | ms/batch 6946.68 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch    94 /   321 | ms/batch 5063.45 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch    96 /   321 | ms/batch 5170.18 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch    98 /   321 | ms/batch 6577.28 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   100 /   321 | ms/batch 5900.79 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   102 /   321 | ms/batch 5198.44 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   104 /   321 | ms/batch 5777.00 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   106 /   321 | ms/batch 4676.97 | Loss 00.54 |\n",
      "[Training]| Epochs   8 | Batch   108 /   321 | ms/batch 4492.29 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   110 /   321 | ms/batch 5320.86 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   112 /   321 | ms/batch 3796.71 | Loss 00.55 |\n",
      "[Training]| Epochs   8 | Batch   114 /   321 | ms/batch 3819.35 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   116 /   321 | ms/batch 5231.71 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   118 /   321 | ms/batch 5068.48 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   120 /   321 | ms/batch 4627.78 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   122 /   321 | ms/batch 4646.59 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   124 /   321 | ms/batch 4889.16 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   126 /   321 | ms/batch 4768.87 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   128 /   321 | ms/batch 5214.70 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   130 /   321 | ms/batch 5110.84 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   132 /   321 | ms/batch 3184.79 | Loss 00.54 |\n",
      "[Training]| Epochs   8 | Batch   134 /   321 | ms/batch 5197.53 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   136 /   321 | ms/batch 5381.80 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   138 /   321 | ms/batch 5463.44 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   140 /   321 | ms/batch 5943.37 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   142 /   321 | ms/batch 7518.84 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   144 /   321 | ms/batch 5717.50 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   146 /   321 | ms/batch 4657.81 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   148 /   321 | ms/batch 6100.27 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   150 /   321 | ms/batch 3792.16 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   152 /   321 | ms/batch 5284.77 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   154 /   321 | ms/batch 3601.78 | Loss 00.54 |\n",
      "[Training]| Epochs   8 | Batch   156 /   321 | ms/batch 6076.95 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   158 /   321 | ms/batch 5458.36 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   160 /   321 | ms/batch 3829.94 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   162 /   321 | ms/batch 5175.08 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   164 /   321 | ms/batch 3697.11 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   166 /   321 | ms/batch 6213.25 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   168 /   321 | ms/batch 7853.98 | Loss 00.61 |\n",
      "[Training]| Epochs   8 | Batch   170 /   321 | ms/batch 4942.40 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   172 /   321 | ms/batch 4070.24 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   174 /   321 | ms/batch 5847.48 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   176 /   321 | ms/batch 6075.81 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   178 /   321 | ms/batch 7821.74 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   180 /   321 | ms/batch 4211.41 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   182 /   321 | ms/batch 6245.67 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   184 /   321 | ms/batch 5396.29 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   186 /   321 | ms/batch 5441.57 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   188 /   321 | ms/batch 5057.32 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   190 /   321 | ms/batch 4385.69 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   192 /   321 | ms/batch 6582.13 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   194 /   321 | ms/batch 6390.74 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   196 /   321 | ms/batch 7309.46 | Loss 00.61 |\n",
      "[Training]| Epochs   8 | Batch   198 /   321 | ms/batch 4028.79 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   200 /   321 | ms/batch 3742.12 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   202 /   321 | ms/batch 3976.17 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   204 /   321 | ms/batch 4635.93 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   206 /   321 | ms/batch 3997.99 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   208 /   321 | ms/batch 4814.23 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   210 /   321 | ms/batch 5228.97 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   212 /   321 | ms/batch 4504.92 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   214 /   321 | ms/batch 4299.79 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   216 /   321 | ms/batch 3955.63 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   218 /   321 | ms/batch 6352.37 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   220 /   321 | ms/batch 6085.92 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   222 /   321 | ms/batch 5568.85 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   224 /   321 | ms/batch 5765.12 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   226 /   321 | ms/batch 5980.86 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   228 /   321 | ms/batch 5142.38 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   230 /   321 | ms/batch 5732.87 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   232 /   321 | ms/batch 5172.01 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   234 /   321 | ms/batch 4444.87 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   236 /   321 | ms/batch 5864.18 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   238 /   321 | ms/batch 5698.37 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   240 /   321 | ms/batch 3360.06 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   242 /   321 | ms/batch 3243.65 | Loss 00.52 |\n",
      "[Training]| Epochs   8 | Batch   244 /   321 | ms/batch 5413.96 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   246 /   321 | ms/batch 4417.65 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   248 /   321 | ms/batch 4905.73 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   250 /   321 | ms/batch 4636.75 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   252 /   321 | ms/batch 4580.11 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   254 /   321 | ms/batch 4299.02 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   256 /   321 | ms/batch 4982.65 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   258 /   321 | ms/batch 4363.31 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   260 /   321 | ms/batch 6025.88 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   262 /   321 | ms/batch 7673.63 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   264 /   321 | ms/batch 5699.82 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   266 /   321 | ms/batch 5633.98 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   268 /   321 | ms/batch 4404.48 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   270 /   321 | ms/batch 5690.94 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   8 | Batch   272 /   321 | ms/batch 4555.38 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   274 /   321 | ms/batch 5106.93 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   276 /   321 | ms/batch 5724.82 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   278 /   321 | ms/batch 6193.26 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   280 /   321 | ms/batch 6169.11 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   282 /   321 | ms/batch 5233.93 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   284 /   321 | ms/batch 4864.10 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   286 /   321 | ms/batch 5176.32 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   288 /   321 | ms/batch 5541.89 | Loss 00.60 |\n",
      "[Training]| Epochs   8 | Batch   290 /   321 | ms/batch 6555.00 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   292 /   321 | ms/batch 3817.24 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   294 /   321 | ms/batch 4138.76 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   296 /   321 | ms/batch 4560.62 | Loss 00.57 |\n",
      "[Training]| Epochs   8 | Batch   298 /   321 | ms/batch 6036.41 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   300 /   321 | ms/batch 4490.08 | Loss 00.58 |\n",
      "[Training]| Epochs   8 | Batch   302 /   321 | ms/batch 4479.86 | Loss 00.55 |\n",
      "[Training]| Epochs   8 | Batch   304 /   321 | ms/batch 7112.15 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   306 /   321 | ms/batch 6100.44 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   308 /   321 | ms/batch 5645.83 | Loss 00.56 |\n",
      "[Training]| Epochs   8 | Batch   310 /   321 | ms/batch 3916.41 | Loss 00.55 |\n",
      "[Training]| Epochs   8 | Batch   312 /   321 | ms/batch 5410.12 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   314 /   321 | ms/batch 7555.66 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   316 /   321 | ms/batch 5881.17 | Loss 00.59 |\n",
      "[Training]| Epochs   8 | Batch   318 /   321 | ms/batch 3724.73 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs   8 | Batch   320 /   321 | ms/batch 5289.42 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   8 | Elapsed 1686.95 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.84\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.23\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.28\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.74\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.74\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.95\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.83\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.59\n",
      "result= [9, 0.09252607980372844, 0.11945429242224036, 0.1754261471755042, 0.2094418532385644, 0.2316192841028108, 0.11948532140171352, 0.08008600414867803, 0.04068556300198205, 0.029727311405648688, 0.025505167008932923, 0.08432996338080914, 0.07876990683266454, 0.058240787925333694, 0.0474515574478304, 0.04256766862638835, 0.08297945824629589, 0.08816727580080158, 0.0938114694391166, 0.09597415802020373, 0.09714821333483745, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5840746098094516]\n",
      "[Training]| Epochs   9 | Batch     2 /   321 | ms/batch 6399.90 | Loss 00.85 |\n",
      "[Training]| Epochs   9 | Batch     4 /   321 | ms/batch 4056.52 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch     6 /   321 | ms/batch 5830.93 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch     8 /   321 | ms/batch 5719.96 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch    10 /   321 | ms/batch 7104.23 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    12 /   321 | ms/batch 7294.43 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    14 /   321 | ms/batch 5159.70 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    16 /   321 | ms/batch 5344.83 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    18 /   321 | ms/batch 4669.50 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    20 /   321 | ms/batch 4736.33 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    22 /   321 | ms/batch 6224.02 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch    24 /   321 | ms/batch 5643.78 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    26 /   321 | ms/batch 4236.77 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   9 | Batch    28 /   321 | ms/batch 4977.22 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    30 /   321 | ms/batch 4935.07 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    32 /   321 | ms/batch 4546.80 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    34 /   321 | ms/batch 4631.26 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    36 /   321 | ms/batch 6518.98 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    38 /   321 | ms/batch 4246.32 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    40 /   321 | ms/batch 6245.61 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    42 /   321 | ms/batch 5540.54 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch    44 /   321 | ms/batch 5539.32 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    46 /   321 | ms/batch 5506.25 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    48 /   321 | ms/batch 5333.28 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    50 /   321 | ms/batch 4156.13 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    52 /   321 | ms/batch 4284.90 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    54 /   321 | ms/batch 4264.86 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    56 /   321 | ms/batch 4338.76 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    58 /   321 | ms/batch 4926.60 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    60 /   321 | ms/batch 5314.03 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    62 /   321 | ms/batch 4458.95 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    64 /   321 | ms/batch 4148.32 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    66 /   321 | ms/batch 3807.41 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    68 /   321 | ms/batch 4592.86 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    70 /   321 | ms/batch 4617.03 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    72 /   321 | ms/batch 4529.22 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    74 /   321 | ms/batch 4047.32 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    76 /   321 | ms/batch 5744.08 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch    78 /   321 | ms/batch 4936.90 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    80 /   321 | ms/batch 4453.59 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    82 /   321 | ms/batch 5368.77 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    84 /   321 | ms/batch 5307.47 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch    86 /   321 | ms/batch 6215.83 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch    88 /   321 | ms/batch 5485.78 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    90 /   321 | ms/batch 5744.76 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    92 /   321 | ms/batch 5491.16 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    94 /   321 | ms/batch 4257.64 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch    96 /   321 | ms/batch 4001.60 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch    98 /   321 | ms/batch 5300.23 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   100 /   321 | ms/batch 6931.05 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   102 /   321 | ms/batch 4108.97 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   104 /   321 | ms/batch 5886.95 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   106 /   321 | ms/batch 5236.27 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch   108 /   321 | ms/batch 4347.72 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   110 /   321 | ms/batch 4828.55 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   112 /   321 | ms/batch 4713.54 | Loss 00.54 |\n",
      "[Training]| Epochs   9 | Batch   114 /   321 | ms/batch 5200.26 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   116 /   321 | ms/batch 7083.77 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   118 /   321 | ms/batch 5290.80 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   120 /   321 | ms/batch 4501.36 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   122 /   321 | ms/batch 4798.23 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   124 /   321 | ms/batch 4991.05 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   126 /   321 | ms/batch 4506.73 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   128 /   321 | ms/batch 4634.52 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   130 /   321 | ms/batch 5125.80 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   132 /   321 | ms/batch 3028.29 | Loss 00.54 |\n",
      "[Training]| Epochs   9 | Batch   134 /   321 | ms/batch 4997.60 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   136 /   321 | ms/batch 6350.33 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   138 /   321 | ms/batch 5490.55 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   140 /   321 | ms/batch 5988.88 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   142 /   321 | ms/batch 7519.70 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   144 /   321 | ms/batch 4923.34 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   146 /   321 | ms/batch 4890.30 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   148 /   321 | ms/batch 6744.87 | Loss 00.61 |\n",
      "[Training]| Epochs   9 | Batch   150 /   321 | ms/batch 5810.34 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   152 /   321 | ms/batch 4268.31 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   154 /   321 | ms/batch 3460.26 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch   156 /   321 | ms/batch 5026.42 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   158 /   321 | ms/batch 7740.93 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   160 /   321 | ms/batch 4255.70 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   162 /   321 | ms/batch 4074.66 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   164 /   321 | ms/batch 4820.72 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   166 /   321 | ms/batch 4605.31 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   168 /   321 | ms/batch 6635.61 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   170 /   321 | ms/batch 6448.45 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   172 /   321 | ms/batch 6087.30 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   174 /   321 | ms/batch 5118.92 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   176 /   321 | ms/batch 4615.43 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   178 /   321 | ms/batch 5031.97 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   180 /   321 | ms/batch 6978.59 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   182 /   321 | ms/batch 5602.99 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   184 /   321 | ms/batch 3912.86 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   186 /   321 | ms/batch 6128.47 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   188 /   321 | ms/batch 5636.49 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   190 /   321 | ms/batch 5078.21 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   192 /   321 | ms/batch 5729.40 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   194 /   321 | ms/batch 5457.91 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   196 /   321 | ms/batch 6134.19 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   198 /   321 | ms/batch 5053.31 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   200 /   321 | ms/batch 4271.60 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   202 /   321 | ms/batch 5356.36 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   204 /   321 | ms/batch 5525.08 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   206 /   321 | ms/batch 4899.86 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   208 /   321 | ms/batch 3516.96 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   210 /   321 | ms/batch 4057.01 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch   212 /   321 | ms/batch 3319.36 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   214 /   321 | ms/batch 5745.37 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   216 /   321 | ms/batch 4351.26 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   218 /   321 | ms/batch 5340.68 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   220 /   321 | ms/batch 4172.46 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   222 /   321 | ms/batch 5666.76 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   224 /   321 | ms/batch 5986.18 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   226 /   321 | ms/batch 5091.05 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   228 /   321 | ms/batch 4954.19 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   230 /   321 | ms/batch 7411.93 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   232 /   321 | ms/batch 5139.40 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   234 /   321 | ms/batch 4802.36 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs   9 | Batch   236 /   321 | ms/batch 6546.58 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   238 /   321 | ms/batch 5312.58 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   240 /   321 | ms/batch 3724.85 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   242 /   321 | ms/batch 3054.37 | Loss 00.52 |\n",
      "[Training]| Epochs   9 | Batch   244 /   321 | ms/batch 5009.77 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   246 /   321 | ms/batch 4576.19 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   248 /   321 | ms/batch 5157.84 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   250 /   321 | ms/batch 4753.62 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   252 /   321 | ms/batch 4682.04 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   254 /   321 | ms/batch 4503.02 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   256 /   321 | ms/batch 5546.35 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   258 /   321 | ms/batch 5481.16 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   260 /   321 | ms/batch 5819.38 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   262 /   321 | ms/batch 7659.64 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   264 /   321 | ms/batch 5462.12 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   266 /   321 | ms/batch 4092.26 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   268 /   321 | ms/batch 5449.49 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   270 /   321 | ms/batch 5618.90 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   272 /   321 | ms/batch 5959.28 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   274 /   321 | ms/batch 5157.65 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   276 /   321 | ms/batch 5020.17 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   278 /   321 | ms/batch 6027.56 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   280 /   321 | ms/batch 6159.69 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   282 /   321 | ms/batch 4943.73 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   284 /   321 | ms/batch 3829.35 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   286 /   321 | ms/batch 6649.99 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   288 /   321 | ms/batch 6408.34 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   290 /   321 | ms/batch 6405.25 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   292 /   321 | ms/batch 4076.02 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   294 /   321 | ms/batch 6121.35 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   296 /   321 | ms/batch 3960.82 | Loss 00.57 |\n",
      "[Training]| Epochs   9 | Batch   298 /   321 | ms/batch 4375.61 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   300 /   321 | ms/batch 4074.37 | Loss 00.58 |\n",
      "[Training]| Epochs   9 | Batch   302 /   321 | ms/batch 4983.71 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch   304 /   321 | ms/batch 4499.10 | Loss 00.60 |\n",
      "[Training]| Epochs   9 | Batch   306 /   321 | ms/batch 5787.09 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   308 /   321 | ms/batch 5441.51 | Loss 00.56 |\n",
      "[Training]| Epochs   9 | Batch   310 /   321 | ms/batch 4341.76 | Loss 00.55 |\n",
      "[Training]| Epochs   9 | Batch   312 /   321 | ms/batch 4504.91 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   314 /   321 | ms/batch 7310.78 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   316 /   321 | ms/batch 5994.39 | Loss 00.59 |\n",
      "[Training]| Epochs   9 | Batch   318 /   321 | ms/batch 4252.84 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs   9 | Batch   320 /   321 | ms/batch 4088.69 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs   9 | Elapsed 1770.23 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.97\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.28\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.93\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.94\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.97\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.82\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.83\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.88\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.34\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.44\n",
      "result= [10, 0.09248864046782139, 0.11977300249963464, 0.17415200855614893, 0.20870277125298373, 0.2289491744126924, 0.11932943913496201, 0.08004701276914458, 0.040672590652668004, 0.029719509561825615, 0.025157452744612542, 0.084248906267126, 0.07885934260331924, 0.058137413494678, 0.047419006157367376, 0.042017947856281664, 0.08409541435844597, 0.08931947664255371, 0.0949073260984313, 0.0971011191046494, 0.09817013293301464, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5836855886894979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  10 | Batch     2 /   321 | ms/batch 6526.75 | Loss 00.85 |\n",
      "[Training]| Epochs  10 | Batch     4 /   321 | ms/batch 4586.41 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch     6 /   321 | ms/batch 7240.88 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch     8 /   321 | ms/batch 7405.42 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch    10 /   321 | ms/batch 6987.99 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    12 /   321 | ms/batch 7276.10 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    14 /   321 | ms/batch 3528.86 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    16 /   321 | ms/batch 4212.27 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    18 /   321 | ms/batch 4432.76 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    20 /   321 | ms/batch 4188.68 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    22 /   321 | ms/batch 5797.75 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch    24 /   321 | ms/batch 3828.03 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    26 /   321 | ms/batch 5582.55 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    28 /   321 | ms/batch 5509.73 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    30 /   321 | ms/batch 4939.00 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    32 /   321 | ms/batch 4531.77 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    34 /   321 | ms/batch 3255.01 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    36 /   321 | ms/batch 6442.37 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    38 /   321 | ms/batch 6260.50 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    40 /   321 | ms/batch 6616.56 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    42 /   321 | ms/batch 4799.54 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch    44 /   321 | ms/batch 4610.68 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    46 /   321 | ms/batch 5498.53 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    48 /   321 | ms/batch 4964.19 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    50 /   321 | ms/batch 3558.84 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    52 /   321 | ms/batch 4209.08 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    54 /   321 | ms/batch 5003.40 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    56 /   321 | ms/batch 5421.86 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    58 /   321 | ms/batch 5868.07 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    60 /   321 | ms/batch 3809.27 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    62 /   321 | ms/batch 5889.27 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    64 /   321 | ms/batch 4555.96 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    66 /   321 | ms/batch 4866.92 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    68 /   321 | ms/batch 5290.37 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    70 /   321 | ms/batch 3485.02 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    72 /   321 | ms/batch 3432.48 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    74 /   321 | ms/batch 4311.22 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    76 /   321 | ms/batch 4132.87 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    78 /   321 | ms/batch 5440.55 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    80 /   321 | ms/batch 4753.31 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch    82 /   321 | ms/batch 4337.09 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    84 /   321 | ms/batch 5232.46 | Loss 00.55 |\n",
      "[Training]| Epochs  10 | Batch    86 /   321 | ms/batch 5978.59 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch    88 /   321 | ms/batch 6944.39 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    90 /   321 | ms/batch 5023.01 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    92 /   321 | ms/batch 5089.02 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch    94 /   321 | ms/batch 3894.97 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch    96 /   321 | ms/batch 5066.16 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch    98 /   321 | ms/batch 5905.71 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   100 /   321 | ms/batch 6624.24 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   102 /   321 | ms/batch 5251.21 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   104 /   321 | ms/batch 7211.29 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   106 /   321 | ms/batch 3345.66 | Loss 00.55 |\n",
      "[Training]| Epochs  10 | Batch   108 /   321 | ms/batch 4108.44 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   110 /   321 | ms/batch 5782.03 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   112 /   321 | ms/batch 3771.57 | Loss 00.55 |\n",
      "[Training]| Epochs  10 | Batch   114 /   321 | ms/batch 4541.73 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   116 /   321 | ms/batch 4674.71 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   118 /   321 | ms/batch 5003.37 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   120 /   321 | ms/batch 4635.86 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   122 /   321 | ms/batch 4940.83 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   124 /   321 | ms/batch 4855.83 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   126 /   321 | ms/batch 4474.46 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   128 /   321 | ms/batch 4894.60 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   130 /   321 | ms/batch 4957.04 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   132 /   321 | ms/batch 4487.80 | Loss 00.54 |\n",
      "[Training]| Epochs  10 | Batch   134 /   321 | ms/batch 7098.16 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   136 /   321 | ms/batch 5948.95 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   138 /   321 | ms/batch 5150.30 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   140 /   321 | ms/batch 4710.12 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   142 /   321 | ms/batch 5713.18 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   144 /   321 | ms/batch 5183.37 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   146 /   321 | ms/batch 5648.24 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   148 /   321 | ms/batch 6832.38 | Loss 00.61 |\n",
      "[Training]| Epochs  10 | Batch   150 /   321 | ms/batch 5060.60 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   152 /   321 | ms/batch 5928.85 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   154 /   321 | ms/batch 4584.68 | Loss 00.54 |\n",
      "[Training]| Epochs  10 | Batch   156 /   321 | ms/batch 6694.36 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   158 /   321 | ms/batch 4792.45 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   160 /   321 | ms/batch 4393.26 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   162 /   321 | ms/batch 4364.19 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   164 /   321 | ms/batch 3243.28 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   166 /   321 | ms/batch 5672.33 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   168 /   321 | ms/batch 7041.49 | Loss 00.61 |\n",
      "[Training]| Epochs  10 | Batch   170 /   321 | ms/batch 5223.25 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   172 /   321 | ms/batch 6559.89 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   174 /   321 | ms/batch 4240.33 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   176 /   321 | ms/batch 6223.12 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   178 /   321 | ms/batch 7021.56 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   180 /   321 | ms/batch 5566.53 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   182 /   321 | ms/batch 5290.95 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   184 /   321 | ms/batch 4943.69 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   186 /   321 | ms/batch 7346.03 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   188 /   321 | ms/batch 4471.03 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   190 /   321 | ms/batch 5434.78 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   192 /   321 | ms/batch 4524.76 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   194 /   321 | ms/batch 6495.56 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   196 /   321 | ms/batch 5644.03 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   198 /   321 | ms/batch 5298.77 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   200 /   321 | ms/batch 4184.41 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   202 /   321 | ms/batch 4510.61 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   204 /   321 | ms/batch 5891.43 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   206 /   321 | ms/batch 4160.52 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   208 /   321 | ms/batch 4019.10 | Loss 00.55 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  10 | Batch   210 /   321 | ms/batch 3805.01 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   212 /   321 | ms/batch 5743.72 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   214 /   321 | ms/batch 5600.01 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   216 /   321 | ms/batch 5530.13 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   218 /   321 | ms/batch 5154.68 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   220 /   321 | ms/batch 5026.50 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   222 /   321 | ms/batch 3738.69 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   224 /   321 | ms/batch 5881.93 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   226 /   321 | ms/batch 5473.25 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   228 /   321 | ms/batch 4997.28 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   230 /   321 | ms/batch 4936.61 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   232 /   321 | ms/batch 4140.98 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   234 /   321 | ms/batch 4897.94 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   236 /   321 | ms/batch 5119.36 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   238 /   321 | ms/batch 4338.47 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   240 /   321 | ms/batch 3570.26 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   242 /   321 | ms/batch 3266.22 | Loss 00.52 |\n",
      "[Training]| Epochs  10 | Batch   244 /   321 | ms/batch 5063.25 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   246 /   321 | ms/batch 4519.28 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   248 /   321 | ms/batch 4797.42 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   250 /   321 | ms/batch 4936.84 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   252 /   321 | ms/batch 4694.69 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   254 /   321 | ms/batch 5949.64 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   256 /   321 | ms/batch 7004.44 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   258 /   321 | ms/batch 5177.38 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   260 /   321 | ms/batch 5973.07 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   262 /   321 | ms/batch 6203.32 | Loss 00.60 |\n",
      "[Training]| Epochs  10 | Batch   264 /   321 | ms/batch 4143.03 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   266 /   321 | ms/batch 4682.04 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   268 /   321 | ms/batch 5242.71 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   270 /   321 | ms/batch 5410.98 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   272 /   321 | ms/batch 5597.68 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   274 /   321 | ms/batch 5739.22 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   276 /   321 | ms/batch 4393.69 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   278 /   321 | ms/batch 6044.75 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   280 /   321 | ms/batch 4864.01 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   282 /   321 | ms/batch 4956.72 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   284 /   321 | ms/batch 4331.32 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   286 /   321 | ms/batch 5240.42 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   288 /   321 | ms/batch 5766.66 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   290 /   321 | ms/batch 5560.20 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   292 /   321 | ms/batch 4388.36 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   294 /   321 | ms/batch 4868.48 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   296 /   321 | ms/batch 4066.44 | Loss 00.57 |\n",
      "[Training]| Epochs  10 | Batch   298 /   321 | ms/batch 5160.56 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   300 /   321 | ms/batch 4568.24 | Loss 00.58 |\n",
      "[Training]| Epochs  10 | Batch   302 /   321 | ms/batch 4295.16 | Loss 00.55 |\n",
      "[Training]| Epochs  10 | Batch   304 /   321 | ms/batch 5731.19 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   306 /   321 | ms/batch 6265.45 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   308 /   321 | ms/batch 4474.57 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   310 /   321 | ms/batch 4294.51 | Loss 00.56 |\n",
      "[Training]| Epochs  10 | Batch   312 /   321 | ms/batch 6046.62 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   314 /   321 | ms/batch 5891.71 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   316 /   321 | ms/batch 5806.73 | Loss 00.59 |\n",
      "[Training]| Epochs  10 | Batch   318 /   321 | ms/batch 5005.81 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs  10 | Batch   320 /   321 | ms/batch 5013.14 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  10 | Elapsed 1463.90 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.49\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.48\n",
      "result= [11, 0.09188311153896507, 0.11911549497561258, 0.17391537244860678, 0.2098452656508793, 0.23189061225064547, 0.11909559789525283, 0.07782563992245348, 0.04078949640620433, 0.029774083816159636, 0.02570301391886996, 0.08394026366045766, 0.07732593539350217, 0.05825973356132478, 0.04750270947570087, 0.04287029037416121, 0.08318136721582313, 0.08825625178156622, 0.09400762739901257, 0.09623783926384426, 0.09744159890402657, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5832190366438877]\n",
      "[Training]| Epochs  11 | Batch     2 /   321 | ms/batch 6882.85 | Loss 00.85 |\n",
      "[Training]| Epochs  11 | Batch     4 /   321 | ms/batch 4961.85 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch     6 /   321 | ms/batch 6189.78 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch     8 /   321 | ms/batch 5812.35 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch    10 /   321 | ms/batch 5595.98 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    12 /   321 | ms/batch 5702.55 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    14 /   321 | ms/batch 4218.03 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    16 /   321 | ms/batch 4246.27 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    18 /   321 | ms/batch 4443.36 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    20 /   321 | ms/batch 4742.65 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    22 /   321 | ms/batch 6077.62 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch    24 /   321 | ms/batch 4916.75 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    26 /   321 | ms/batch 4831.77 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    28 /   321 | ms/batch 4552.26 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    30 /   321 | ms/batch 4482.72 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    32 /   321 | ms/batch 4004.95 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    34 /   321 | ms/batch 4376.37 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    36 /   321 | ms/batch 5618.19 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    38 /   321 | ms/batch 5497.14 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    40 /   321 | ms/batch 5203.27 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    42 /   321 | ms/batch 5309.53 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    44 /   321 | ms/batch 5033.85 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    46 /   321 | ms/batch 4373.88 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    48 /   321 | ms/batch 5326.66 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    50 /   321 | ms/batch 4504.35 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    52 /   321 | ms/batch 4812.99 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    54 /   321 | ms/batch 4498.13 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    56 /   321 | ms/batch 5007.94 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    58 /   321 | ms/batch 4547.17 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    60 /   321 | ms/batch 4188.69 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    62 /   321 | ms/batch 4281.91 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    64 /   321 | ms/batch 4306.60 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    66 /   321 | ms/batch 4338.65 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    68 /   321 | ms/batch 4612.57 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    70 /   321 | ms/batch 4637.56 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    72 /   321 | ms/batch 3591.83 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    74 /   321 | ms/batch 4350.24 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    76 /   321 | ms/batch 4682.94 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    78 /   321 | ms/batch 5251.23 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    80 /   321 | ms/batch 4988.58 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    82 /   321 | ms/batch 5421.80 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    84 /   321 | ms/batch 3893.53 | Loss 00.55 |\n",
      "[Training]| Epochs  11 | Batch    86 /   321 | ms/batch 5856.37 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch    88 /   321 | ms/batch 6001.68 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch    90 /   321 | ms/batch 4523.42 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    92 /   321 | ms/batch 5371.19 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch    94 /   321 | ms/batch 4755.37 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch    96 /   321 | ms/batch 4382.29 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch    98 /   321 | ms/batch 5326.07 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   100 /   321 | ms/batch 5504.82 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   102 /   321 | ms/batch 4373.05 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   104 /   321 | ms/batch 5457.73 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   106 /   321 | ms/batch 3707.21 | Loss 00.54 |\n",
      "[Training]| Epochs  11 | Batch   108 /   321 | ms/batch 3997.50 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   110 /   321 | ms/batch 5426.00 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   112 /   321 | ms/batch 3837.51 | Loss 00.54 |\n",
      "[Training]| Epochs  11 | Batch   114 /   321 | ms/batch 4750.31 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   116 /   321 | ms/batch 5163.32 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   118 /   321 | ms/batch 5036.13 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   120 /   321 | ms/batch 4928.22 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   122 /   321 | ms/batch 5073.99 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   124 /   321 | ms/batch 5159.33 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   126 /   321 | ms/batch 4818.93 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   128 /   321 | ms/batch 4839.76 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   130 /   321 | ms/batch 5467.51 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   132 /   321 | ms/batch 3465.20 | Loss 00.54 |\n",
      "[Training]| Epochs  11 | Batch   134 /   321 | ms/batch 5348.84 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   136 /   321 | ms/batch 4823.80 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   138 /   321 | ms/batch 4083.80 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   140 /   321 | ms/batch 4486.84 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   142 /   321 | ms/batch 6100.49 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   144 /   321 | ms/batch 5352.48 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   146 /   321 | ms/batch 4929.84 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   148 /   321 | ms/batch 6287.83 | Loss 00.61 |\n",
      "[Training]| Epochs  11 | Batch   150 /   321 | ms/batch 4301.18 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   152 /   321 | ms/batch 4877.19 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   154 /   321 | ms/batch 4106.42 | Loss 00.54 |\n",
      "[Training]| Epochs  11 | Batch   156 /   321 | ms/batch 5737.04 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   158 /   321 | ms/batch 5756.84 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   160 /   321 | ms/batch 4399.72 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   162 /   321 | ms/batch 4644.18 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   164 /   321 | ms/batch 3876.01 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   166 /   321 | ms/batch 4855.98 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   168 /   321 | ms/batch 6920.61 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   170 /   321 | ms/batch 5296.29 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   172 /   321 | ms/batch 5057.76 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  11 | Batch   174 /   321 | ms/batch 4729.22 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   176 /   321 | ms/batch 5229.39 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   178 /   321 | ms/batch 6044.81 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   180 /   321 | ms/batch 4933.31 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   182 /   321 | ms/batch 5147.40 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   184 /   321 | ms/batch 4563.20 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   186 /   321 | ms/batch 5713.07 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   188 /   321 | ms/batch 4189.40 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   190 /   321 | ms/batch 4963.94 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   192 /   321 | ms/batch 5693.59 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   194 /   321 | ms/batch 6180.69 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   196 /   321 | ms/batch 6086.93 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   198 /   321 | ms/batch 4858.74 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   200 /   321 | ms/batch 4323.07 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   202 /   321 | ms/batch 4700.87 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   204 /   321 | ms/batch 4421.60 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   206 /   321 | ms/batch 4655.67 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   208 /   321 | ms/batch 4217.71 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   210 /   321 | ms/batch 4079.41 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   212 /   321 | ms/batch 4550.25 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   214 /   321 | ms/batch 5049.90 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   216 /   321 | ms/batch 4548.71 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   218 /   321 | ms/batch 5442.42 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   220 /   321 | ms/batch 4983.99 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   222 /   321 | ms/batch 4395.97 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   224 /   321 | ms/batch 5274.40 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   226 /   321 | ms/batch 4815.73 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   228 /   321 | ms/batch 4867.91 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   230 /   321 | ms/batch 5548.03 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   232 /   321 | ms/batch 4623.46 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   234 /   321 | ms/batch 5521.14 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   236 /   321 | ms/batch 5285.31 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   238 /   321 | ms/batch 4547.16 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   240 /   321 | ms/batch 3860.89 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   242 /   321 | ms/batch 3377.28 | Loss 00.52 |\n",
      "[Training]| Epochs  11 | Batch   244 /   321 | ms/batch 5547.85 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   246 /   321 | ms/batch 4836.56 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   248 /   321 | ms/batch 5383.69 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   250 /   321 | ms/batch 5297.27 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   252 /   321 | ms/batch 4958.05 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   254 /   321 | ms/batch 4553.82 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   256 /   321 | ms/batch 5396.43 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   258 /   321 | ms/batch 4031.41 | Loss 00.55 |\n",
      "[Training]| Epochs  11 | Batch   260 /   321 | ms/batch 4259.11 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   262 /   321 | ms/batch 6408.64 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   264 /   321 | ms/batch 4334.76 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   266 /   321 | ms/batch 4621.11 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   268 /   321 | ms/batch 4913.79 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   270 /   321 | ms/batch 5037.08 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   272 /   321 | ms/batch 5426.37 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   274 /   321 | ms/batch 5270.37 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   276 /   321 | ms/batch 4146.17 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   278 /   321 | ms/batch 5597.89 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   280 /   321 | ms/batch 4572.81 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   282 /   321 | ms/batch 5097.87 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   284 /   321 | ms/batch 4194.46 | Loss 00.55 |\n",
      "[Training]| Epochs  11 | Batch   286 /   321 | ms/batch 5138.71 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   288 /   321 | ms/batch 5630.12 | Loss 00.60 |\n",
      "[Training]| Epochs  11 | Batch   290 /   321 | ms/batch 5248.04 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   292 /   321 | ms/batch 4168.33 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   294 /   321 | ms/batch 4877.13 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   296 /   321 | ms/batch 3989.58 | Loss 00.57 |\n",
      "[Training]| Epochs  11 | Batch   298 /   321 | ms/batch 4977.96 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   300 /   321 | ms/batch 4224.75 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   302 /   321 | ms/batch 4123.31 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   304 /   321 | ms/batch 5654.60 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   306 /   321 | ms/batch 5974.85 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   308 /   321 | ms/batch 4318.04 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   310 /   321 | ms/batch 4220.19 | Loss 00.56 |\n",
      "[Training]| Epochs  11 | Batch   312 /   321 | ms/batch 5919.51 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   314 /   321 | ms/batch 5675.77 | Loss 00.59 |\n",
      "[Training]| Epochs  11 | Batch   316 /   321 | ms/batch 5663.53 | Loss 00.58 |\n",
      "[Training]| Epochs  11 | Batch   318 /   321 | ms/batch 4592.34 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  11 | Batch   320 /   321 | ms/batch 4992.13 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  11 | Elapsed 1411.33 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.54\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.44\n",
      "result= [12, 0.0925924727810795, 0.11907458286775989, 0.17417228621425615, 0.2085161930122881, 0.22967705314083203, 0.1193294510280166, 0.08121615950241742, 0.04062060611097495, 0.029532464518736604, 0.025223402705679437, 0.08430590967786378, 0.07942990595162705, 0.058090091030391415, 0.047133162536383236, 0.042123237068729294, 0.08324187307742686, 0.08852065425432945, 0.0940307470672602, 0.09618311680509292, 0.0973202600955037, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847596857282851]\n",
      "[Training]| Epochs  12 | Batch     2 /   321 | ms/batch 6627.94 | Loss 00.85 |\n",
      "[Training]| Epochs  12 | Batch     4 /   321 | ms/batch 4650.82 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch     6 /   321 | ms/batch 6214.48 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch     8 /   321 | ms/batch 5708.61 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch    10 /   321 | ms/batch 5340.56 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    12 /   321 | ms/batch 5606.70 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    14 /   321 | ms/batch 3832.93 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    16 /   321 | ms/batch 4198.81 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    18 /   321 | ms/batch 4441.83 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    20 /   321 | ms/batch 5079.40 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    22 /   321 | ms/batch 6303.21 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch    24 /   321 | ms/batch 4800.49 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    26 /   321 | ms/batch 4725.28 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    28 /   321 | ms/batch 4991.46 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    30 /   321 | ms/batch 4531.17 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    32 /   321 | ms/batch 4306.59 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    34 /   321 | ms/batch 4332.33 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    36 /   321 | ms/batch 5660.28 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    38 /   321 | ms/batch 5512.14 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    40 /   321 | ms/batch 5205.27 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    42 /   321 | ms/batch 4930.77 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    44 /   321 | ms/batch 5008.94 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    46 /   321 | ms/batch 4440.75 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    48 /   321 | ms/batch 5417.74 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    50 /   321 | ms/batch 4280.92 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    52 /   321 | ms/batch 5131.00 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    54 /   321 | ms/batch 4537.32 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    56 /   321 | ms/batch 4875.33 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    58 /   321 | ms/batch 4661.30 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    60 /   321 | ms/batch 4123.24 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    62 /   321 | ms/batch 4311.12 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    64 /   321 | ms/batch 4284.43 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    66 /   321 | ms/batch 4058.88 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    68 /   321 | ms/batch 4312.66 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    70 /   321 | ms/batch 4548.36 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    72 /   321 | ms/batch 3520.65 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    74 /   321 | ms/batch 4374.33 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    76 /   321 | ms/batch 4582.73 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    78 /   321 | ms/batch 5497.68 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    80 /   321 | ms/batch 5024.73 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    82 /   321 | ms/batch 5535.68 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    84 /   321 | ms/batch 4111.46 | Loss 00.55 |\n",
      "[Training]| Epochs  12 | Batch    86 /   321 | ms/batch 5768.91 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch    88 /   321 | ms/batch 5974.86 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    90 /   321 | ms/batch 4400.38 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch    92 /   321 | ms/batch 5232.10 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch    94 /   321 | ms/batch 4818.43 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch    96 /   321 | ms/batch 4267.31 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch    98 /   321 | ms/batch 5502.23 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   100 /   321 | ms/batch 5820.23 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   102 /   321 | ms/batch 4260.88 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   104 /   321 | ms/batch 5404.64 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   106 /   321 | ms/batch 3570.20 | Loss 00.54 |\n",
      "[Training]| Epochs  12 | Batch   108 /   321 | ms/batch 4088.21 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   110 /   321 | ms/batch 5181.99 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   112 /   321 | ms/batch 3762.03 | Loss 00.54 |\n",
      "[Training]| Epochs  12 | Batch   114 /   321 | ms/batch 4390.46 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   116 /   321 | ms/batch 5366.19 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   118 /   321 | ms/batch 4915.67 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   120 /   321 | ms/batch 4707.75 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   122 /   321 | ms/batch 5210.19 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   124 /   321 | ms/batch 4994.39 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   126 /   321 | ms/batch 4558.65 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   128 /   321 | ms/batch 5319.97 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   130 /   321 | ms/batch 5470.08 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   132 /   321 | ms/batch 3435.49 | Loss 00.54 |\n",
      "[Training]| Epochs  12 | Batch   134 /   321 | ms/batch 5382.73 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   136 /   321 | ms/batch 4864.34 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  12 | Batch   138 /   321 | ms/batch 3960.81 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   140 /   321 | ms/batch 4455.31 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   142 /   321 | ms/batch 5808.01 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   144 /   321 | ms/batch 5663.04 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   146 /   321 | ms/batch 4675.38 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   148 /   321 | ms/batch 6274.42 | Loss 00.61 |\n",
      "[Training]| Epochs  12 | Batch   150 /   321 | ms/batch 4292.79 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   152 /   321 | ms/batch 4849.64 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   154 /   321 | ms/batch 3829.13 | Loss 00.55 |\n",
      "[Training]| Epochs  12 | Batch   156 /   321 | ms/batch 5690.84 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   158 /   321 | ms/batch 5796.10 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   160 /   321 | ms/batch 4290.29 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   162 /   321 | ms/batch 4755.27 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   164 /   321 | ms/batch 4021.30 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   166 /   321 | ms/batch 4811.33 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   168 /   321 | ms/batch 6619.88 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   170 /   321 | ms/batch 5504.67 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   172 /   321 | ms/batch 4888.33 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   174 /   321 | ms/batch 4456.28 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   176 /   321 | ms/batch 4979.93 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   178 /   321 | ms/batch 6067.35 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   180 /   321 | ms/batch 5014.73 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   182 /   321 | ms/batch 5234.31 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   184 /   321 | ms/batch 4754.30 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   186 /   321 | ms/batch 5980.36 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   188 /   321 | ms/batch 4618.69 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   190 /   321 | ms/batch 4724.68 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   192 /   321 | ms/batch 5438.71 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   194 /   321 | ms/batch 6268.08 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   196 /   321 | ms/batch 5923.18 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   198 /   321 | ms/batch 4955.60 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   200 /   321 | ms/batch 4320.03 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   202 /   321 | ms/batch 4532.61 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   204 /   321 | ms/batch 4396.44 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   206 /   321 | ms/batch 4617.93 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   208 /   321 | ms/batch 4094.63 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   210 /   321 | ms/batch 4041.77 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   212 /   321 | ms/batch 4492.01 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   214 /   321 | ms/batch 5114.88 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   216 /   321 | ms/batch 4524.72 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   218 /   321 | ms/batch 5195.12 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   220 /   321 | ms/batch 4978.93 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   222 /   321 | ms/batch 4369.22 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   224 /   321 | ms/batch 5125.35 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   226 /   321 | ms/batch 4898.85 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   228 /   321 | ms/batch 4962.03 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   230 /   321 | ms/batch 5706.92 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   232 /   321 | ms/batch 4768.09 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   234 /   321 | ms/batch 5233.72 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   236 /   321 | ms/batch 5272.93 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   238 /   321 | ms/batch 4674.45 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   240 /   321 | ms/batch 3744.93 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   242 /   321 | ms/batch 3441.20 | Loss 00.52 |\n",
      "[Training]| Epochs  12 | Batch   244 /   321 | ms/batch 5373.32 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   246 /   321 | ms/batch 4849.67 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   248 /   321 | ms/batch 5479.67 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   250 /   321 | ms/batch 5368.81 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   252 /   321 | ms/batch 4967.95 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   254 /   321 | ms/batch 4612.42 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   256 /   321 | ms/batch 5486.66 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   258 /   321 | ms/batch 3894.37 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   260 /   321 | ms/batch 4438.89 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   262 /   321 | ms/batch 6048.98 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   264 /   321 | ms/batch 4389.79 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   266 /   321 | ms/batch 4493.74 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   268 /   321 | ms/batch 4924.54 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   270 /   321 | ms/batch 5111.81 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   272 /   321 | ms/batch 5591.08 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   274 /   321 | ms/batch 5642.28 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   276 /   321 | ms/batch 4581.47 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   278 /   321 | ms/batch 5688.11 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   280 /   321 | ms/batch 4855.01 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   282 /   321 | ms/batch 4912.71 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   284 /   321 | ms/batch 4211.70 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   286 /   321 | ms/batch 4944.42 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   288 /   321 | ms/batch 5784.84 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   290 /   321 | ms/batch 5154.00 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   292 /   321 | ms/batch 4191.31 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   294 /   321 | ms/batch 4984.73 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   296 /   321 | ms/batch 4256.57 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   298 /   321 | ms/batch 4873.21 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   300 /   321 | ms/batch 4376.68 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   302 /   321 | ms/batch 3857.61 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   304 /   321 | ms/batch 5726.98 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   306 /   321 | ms/batch 6066.37 | Loss 00.59 |\n",
      "[Training]| Epochs  12 | Batch   308 /   321 | ms/batch 4487.26 | Loss 00.57 |\n",
      "[Training]| Epochs  12 | Batch   310 /   321 | ms/batch 4268.04 | Loss 00.56 |\n",
      "[Training]| Epochs  12 | Batch   312 /   321 | ms/batch 5791.44 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   314 /   321 | ms/batch 5492.12 | Loss 00.60 |\n",
      "[Training]| Epochs  12 | Batch   316 /   321 | ms/batch 5885.79 | Loss 00.58 |\n",
      "[Training]| Epochs  12 | Batch   318 /   321 | ms/batch 4961.90 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  12 | Batch   320 /   321 | ms/batch 5947.68 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  12 | Elapsed 1406.63 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.50\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.52\n",
      "result= [13, 0.09242028513645996, 0.1205203263720589, 0.17429536743639784, 0.20947313197218434, 0.22821361277127583, 0.11917353308210128, 0.08004701276914458, 0.040724548434988184, 0.029883184752609242, 0.025163446844135146, 0.0841177020886868, 0.0790623570454836, 0.058202486342967534, 0.04769968521938785, 0.042009780301029385, 0.08343366660694544, 0.08875023678964293, 0.09428383905291035, 0.09653758744031209, 0.09752210529211877, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5835785218227056]\n",
      "[Training]| Epochs  13 | Batch     2 /   321 | ms/batch 6579.15 | Loss 00.85 |\n",
      "[Training]| Epochs  13 | Batch     4 /   321 | ms/batch 4689.56 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch     6 /   321 | ms/batch 6050.24 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch     8 /   321 | ms/batch 5830.70 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch    10 /   321 | ms/batch 5456.89 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    12 /   321 | ms/batch 5407.69 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    14 /   321 | ms/batch 4013.15 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    16 /   321 | ms/batch 4120.13 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    18 /   321 | ms/batch 4316.50 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    20 /   321 | ms/batch 4909.84 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    22 /   321 | ms/batch 6480.23 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch    24 /   321 | ms/batch 4704.66 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    26 /   321 | ms/batch 4743.68 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    28 /   321 | ms/batch 4986.41 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    30 /   321 | ms/batch 4555.99 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    32 /   321 | ms/batch 4357.73 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    34 /   321 | ms/batch 4176.19 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    36 /   321 | ms/batch 5490.65 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    38 /   321 | ms/batch 5431.65 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    40 /   321 | ms/batch 5182.88 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    42 /   321 | ms/batch 5152.59 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    44 /   321 | ms/batch 5304.60 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    46 /   321 | ms/batch 4143.48 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    48 /   321 | ms/batch 5236.04 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    50 /   321 | ms/batch 4464.44 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    52 /   321 | ms/batch 5141.28 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    54 /   321 | ms/batch 4539.97 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    56 /   321 | ms/batch 4977.21 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    58 /   321 | ms/batch 4590.97 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    60 /   321 | ms/batch 3912.64 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    62 /   321 | ms/batch 4464.63 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    64 /   321 | ms/batch 4511.77 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    66 /   321 | ms/batch 4278.98 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    68 /   321 | ms/batch 4748.28 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    70 /   321 | ms/batch 4745.34 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    72 /   321 | ms/batch 3638.93 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch    74 /   321 | ms/batch 4226.37 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    76 /   321 | ms/batch 4454.95 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    78 /   321 | ms/batch 5536.40 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    80 /   321 | ms/batch 4831.01 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    82 /   321 | ms/batch 5242.04 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    84 /   321 | ms/batch 3873.21 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch    86 /   321 | ms/batch 5853.03 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    88 /   321 | ms/batch 5769.29 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    90 /   321 | ms/batch 4536.82 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch    92 /   321 | ms/batch 5322.69 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch    94 /   321 | ms/batch 5035.40 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch    96 /   321 | ms/batch 4339.15 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch    98 /   321 | ms/batch 5466.65 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   100 /   321 | ms/batch 5459.54 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  13 | Batch   102 /   321 | ms/batch 4495.15 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   104 /   321 | ms/batch 5721.65 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   106 /   321 | ms/batch 3960.40 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch   108 /   321 | ms/batch 4240.23 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   110 /   321 | ms/batch 5439.75 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   112 /   321 | ms/batch 3685.70 | Loss 00.54 |\n",
      "[Training]| Epochs  13 | Batch   114 /   321 | ms/batch 4804.79 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   116 /   321 | ms/batch 5309.09 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   118 /   321 | ms/batch 5353.36 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   120 /   321 | ms/batch 5015.93 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   122 /   321 | ms/batch 5059.22 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   124 /   321 | ms/batch 5297.46 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   126 /   321 | ms/batch 4870.48 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   128 /   321 | ms/batch 5137.43 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   130 /   321 | ms/batch 5549.39 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   132 /   321 | ms/batch 3209.71 | Loss 00.54 |\n",
      "[Training]| Epochs  13 | Batch   134 /   321 | ms/batch 5533.77 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   136 /   321 | ms/batch 4487.12 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   138 /   321 | ms/batch 3987.51 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   140 /   321 | ms/batch 4660.57 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   142 /   321 | ms/batch 5960.97 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   144 /   321 | ms/batch 5396.42 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   146 /   321 | ms/batch 4833.87 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   148 /   321 | ms/batch 6147.30 | Loss 00.61 |\n",
      "[Training]| Epochs  13 | Batch   150 /   321 | ms/batch 4409.08 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   152 /   321 | ms/batch 4815.25 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   154 /   321 | ms/batch 4043.54 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch   156 /   321 | ms/batch 5452.45 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   158 /   321 | ms/batch 5905.17 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   160 /   321 | ms/batch 4455.07 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   162 /   321 | ms/batch 4693.28 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   164 /   321 | ms/batch 3752.73 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   166 /   321 | ms/batch 4882.46 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   168 /   321 | ms/batch 6808.81 | Loss 00.61 |\n",
      "[Training]| Epochs  13 | Batch   170 /   321 | ms/batch 5478.04 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   172 /   321 | ms/batch 5079.44 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   174 /   321 | ms/batch 4629.32 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   176 /   321 | ms/batch 5171.52 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   178 /   321 | ms/batch 6168.42 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   180 /   321 | ms/batch 5074.42 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   182 /   321 | ms/batch 5309.94 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   184 /   321 | ms/batch 4913.09 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   186 /   321 | ms/batch 5939.70 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   188 /   321 | ms/batch 4283.23 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   190 /   321 | ms/batch 4825.17 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   192 /   321 | ms/batch 5638.41 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   194 /   321 | ms/batch 6231.81 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   196 /   321 | ms/batch 6128.46 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   198 /   321 | ms/batch 4936.70 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   200 /   321 | ms/batch 4141.61 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   202 /   321 | ms/batch 4417.67 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   204 /   321 | ms/batch 4412.30 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   206 /   321 | ms/batch 4542.77 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   208 /   321 | ms/batch 4155.96 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   210 /   321 | ms/batch 3935.94 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   212 /   321 | ms/batch 4536.71 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   214 /   321 | ms/batch 5043.61 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   216 /   321 | ms/batch 4609.41 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   218 /   321 | ms/batch 5300.51 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   220 /   321 | ms/batch 5148.86 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   222 /   321 | ms/batch 4461.87 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   224 /   321 | ms/batch 5491.18 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   226 /   321 | ms/batch 4973.08 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   228 /   321 | ms/batch 4831.26 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   230 /   321 | ms/batch 5686.40 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   232 /   321 | ms/batch 4637.37 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   234 /   321 | ms/batch 5341.78 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   236 /   321 | ms/batch 5505.99 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   238 /   321 | ms/batch 4598.61 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   240 /   321 | ms/batch 3770.23 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   242 /   321 | ms/batch 3496.70 | Loss 00.52 |\n",
      "[Training]| Epochs  13 | Batch   244 /   321 | ms/batch 5608.14 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   246 /   321 | ms/batch 5229.20 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   248 /   321 | ms/batch 5449.39 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   250 /   321 | ms/batch 5156.96 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   252 /   321 | ms/batch 4801.90 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   254 /   321 | ms/batch 4527.35 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   256 /   321 | ms/batch 5173.46 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   258 /   321 | ms/batch 3906.86 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   260 /   321 | ms/batch 4491.00 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   262 /   321 | ms/batch 5927.37 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   264 /   321 | ms/batch 4221.70 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   266 /   321 | ms/batch 4598.19 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   268 /   321 | ms/batch 4755.04 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   270 /   321 | ms/batch 5024.96 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   272 /   321 | ms/batch 5614.08 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   274 /   321 | ms/batch 5747.94 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   276 /   321 | ms/batch 4370.04 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   278 /   321 | ms/batch 5790.26 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   280 /   321 | ms/batch 4782.86 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   282 /   321 | ms/batch 5001.20 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   284 /   321 | ms/batch 4320.61 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch   286 /   321 | ms/batch 5031.33 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   288 /   321 | ms/batch 5601.57 | Loss 00.60 |\n",
      "[Training]| Epochs  13 | Batch   290 /   321 | ms/batch 5108.89 | Loss 00.57 |\n",
      "[Training]| Epochs  13 | Batch   292 /   321 | ms/batch 3262.41 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   294 /   321 | ms/batch 5329.19 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   296 /   321 | ms/batch 3055.72 | Loss 00.56 |\n",
      "[Training]| Epochs  13 | Batch   298 /   321 | ms/batch 5177.30 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   300 /   321 | ms/batch 4303.90 | Loss 00.58 |\n",
      "[Training]| Epochs  13 | Batch   302 /   321 | ms/batch 4206.69 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch   304 /   321 | ms/batch 5782.39 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   306 /   321 | ms/batch 6215.30 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   308 /   321 | ms/batch 4230.67 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  13 | Batch   310 /   321 | ms/batch 4134.16 | Loss 00.55 |\n",
      "[Training]| Epochs  13 | Batch   312 /   321 | ms/batch 5775.52 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   314 /   321 | ms/batch 5822.34 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   316 /   321 | ms/batch 5729.51 | Loss 00.59 |\n",
      "[Training]| Epochs  13 | Batch   318 /   321 | ms/batch 4642.94 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs  13 | Batch   320 /   321 | ms/batch 4983.83 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  13 | Elapsed 1429.93 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 1.03\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.53\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.47\n",
      "result= [14, 0.09263806480592057, 0.11956216242753799, 0.17425983098922812, 0.20857896455451091, 0.22838147134401793, 0.11956326848161657, 0.07985218074855076, 0.04073753565062049, 0.029680536021874086, 0.025307334965314145, 0.08437646522432775, 0.07868734524757344, 0.05823193949270509, 0.047375034561216695, 0.042224857273830255, 0.0833994791801621, 0.08854988297020737, 0.09411658956803562, 0.09627870656186983, 0.09735480225964399, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5834291776021322]\n",
      "[Training]| Epochs  14 | Batch     2 /   321 | ms/batch 6800.76 | Loss 00.85 |\n",
      "[Training]| Epochs  14 | Batch     4 /   321 | ms/batch 4943.91 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch     6 /   321 | ms/batch 6204.69 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch     8 /   321 | ms/batch 5643.72 | Loss 00.61 |\n",
      "[Training]| Epochs  14 | Batch    10 /   321 | ms/batch 5400.67 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    12 /   321 | ms/batch 5495.61 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    14 /   321 | ms/batch 4045.04 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    16 /   321 | ms/batch 4240.01 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch    18 /   321 | ms/batch 4579.57 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    20 /   321 | ms/batch 4897.60 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    22 /   321 | ms/batch 6453.05 | Loss 00.61 |\n",
      "[Training]| Epochs  14 | Batch    24 /   321 | ms/batch 4854.40 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    26 /   321 | ms/batch 4921.55 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    28 /   321 | ms/batch 5052.95 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    30 /   321 | ms/batch 4626.83 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    32 /   321 | ms/batch 4680.11 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    34 /   321 | ms/batch 4102.69 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    36 /   321 | ms/batch 5704.64 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    38 /   321 | ms/batch 5571.65 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    40 /   321 | ms/batch 5446.99 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    42 /   321 | ms/batch 5045.24 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    44 /   321 | ms/batch 5045.97 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    46 /   321 | ms/batch 4490.83 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    48 /   321 | ms/batch 5223.91 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    50 /   321 | ms/batch 4270.54 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    52 /   321 | ms/batch 4930.07 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    54 /   321 | ms/batch 4497.41 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    56 /   321 | ms/batch 5051.76 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    58 /   321 | ms/batch 4765.41 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    60 /   321 | ms/batch 4302.74 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    62 /   321 | ms/batch 4063.31 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    64 /   321 | ms/batch 4437.12 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  14 | Batch    66 /   321 | ms/batch 4475.34 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    68 /   321 | ms/batch 4641.80 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    70 /   321 | ms/batch 4482.75 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    72 /   321 | ms/batch 3604.75 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch    74 /   321 | ms/batch 4336.74 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    76 /   321 | ms/batch 4831.99 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch    78 /   321 | ms/batch 5571.98 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    80 /   321 | ms/batch 4906.31 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    82 /   321 | ms/batch 5341.60 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    84 /   321 | ms/batch 4036.34 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch    86 /   321 | ms/batch 5721.57 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    88 /   321 | ms/batch 5684.99 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    90 /   321 | ms/batch 4466.70 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    92 /   321 | ms/batch 5255.33 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch    94 /   321 | ms/batch 4712.61 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch    96 /   321 | ms/batch 4395.40 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch    98 /   321 | ms/batch 5278.63 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   100 /   321 | ms/batch 5547.49 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   102 /   321 | ms/batch 4372.03 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   104 /   321 | ms/batch 5574.55 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   106 /   321 | ms/batch 3815.33 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch   108 /   321 | ms/batch 4069.80 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   110 /   321 | ms/batch 5563.41 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   112 /   321 | ms/batch 3675.01 | Loss 00.54 |\n",
      "[Training]| Epochs  14 | Batch   114 /   321 | ms/batch 4833.28 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   116 /   321 | ms/batch 5263.68 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   118 /   321 | ms/batch 5195.83 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   120 /   321 | ms/batch 4928.09 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   122 /   321 | ms/batch 4956.10 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   124 /   321 | ms/batch 5200.24 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   126 /   321 | ms/batch 4827.77 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   128 /   321 | ms/batch 5347.27 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   130 /   321 | ms/batch 5555.66 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   132 /   321 | ms/batch 3304.53 | Loss 00.53 |\n",
      "[Training]| Epochs  14 | Batch   134 /   321 | ms/batch 5466.64 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   136 /   321 | ms/batch 4952.19 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   138 /   321 | ms/batch 4011.03 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   140 /   321 | ms/batch 4444.27 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   142 /   321 | ms/batch 6085.08 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   144 /   321 | ms/batch 5299.50 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   146 /   321 | ms/batch 4629.39 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   148 /   321 | ms/batch 6248.84 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   150 /   321 | ms/batch 4446.89 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   152 /   321 | ms/batch 4687.83 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   154 /   321 | ms/batch 3865.74 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch   156 /   321 | ms/batch 5417.17 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   158 /   321 | ms/batch 5818.00 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   160 /   321 | ms/batch 4647.64 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   162 /   321 | ms/batch 4788.56 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   164 /   321 | ms/batch 3908.11 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   166 /   321 | ms/batch 4685.70 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   168 /   321 | ms/batch 6730.56 | Loss 00.61 |\n",
      "[Training]| Epochs  14 | Batch   170 /   321 | ms/batch 5384.12 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   172 /   321 | ms/batch 5071.45 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   174 /   321 | ms/batch 4600.51 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   176 /   321 | ms/batch 5152.84 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   178 /   321 | ms/batch 6069.53 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   180 /   321 | ms/batch 5217.38 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   182 /   321 | ms/batch 5221.64 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   184 /   321 | ms/batch 4772.66 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   186 /   321 | ms/batch 6041.03 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   188 /   321 | ms/batch 4518.30 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   190 /   321 | ms/batch 4988.14 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   192 /   321 | ms/batch 5543.98 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   194 /   321 | ms/batch 6341.32 | Loss 00.61 |\n",
      "[Training]| Epochs  14 | Batch   196 /   321 | ms/batch 6055.67 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   198 /   321 | ms/batch 4951.59 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   200 /   321 | ms/batch 4009.41 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   202 /   321 | ms/batch 4706.35 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   204 /   321 | ms/batch 4372.54 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   206 /   321 | ms/batch 4812.47 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   208 /   321 | ms/batch 4238.46 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   210 /   321 | ms/batch 4052.79 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   212 /   321 | ms/batch 4402.64 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   214 /   321 | ms/batch 5037.09 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   216 /   321 | ms/batch 4592.11 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   218 /   321 | ms/batch 5363.04 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   220 /   321 | ms/batch 5116.72 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   222 /   321 | ms/batch 4544.70 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   224 /   321 | ms/batch 5367.29 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   226 /   321 | ms/batch 5033.58 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   228 /   321 | ms/batch 4925.31 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   230 /   321 | ms/batch 5627.78 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   232 /   321 | ms/batch 4916.29 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   234 /   321 | ms/batch 5485.34 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   236 /   321 | ms/batch 5484.25 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   238 /   321 | ms/batch 4745.33 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   240 /   321 | ms/batch 3827.76 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   242 /   321 | ms/batch 3532.91 | Loss 00.52 |\n",
      "[Training]| Epochs  14 | Batch   244 /   321 | ms/batch 5754.32 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   246 /   321 | ms/batch 4891.98 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   248 /   321 | ms/batch 5409.17 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   250 /   321 | ms/batch 5111.93 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   252 /   321 | ms/batch 4886.27 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   254 /   321 | ms/batch 4529.73 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   256 /   321 | ms/batch 5533.17 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   258 /   321 | ms/batch 4162.80 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   260 /   321 | ms/batch 4438.97 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   262 /   321 | ms/batch 6012.58 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   264 /   321 | ms/batch 4241.32 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   266 /   321 | ms/batch 4794.70 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   268 /   321 | ms/batch 4966.78 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   270 /   321 | ms/batch 5217.07 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   272 /   321 | ms/batch 5532.29 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  14 | Batch   274 /   321 | ms/batch 5530.08 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   276 /   321 | ms/batch 4448.14 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   278 /   321 | ms/batch 5884.50 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   280 /   321 | ms/batch 4780.19 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   282 /   321 | ms/batch 4832.24 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   284 /   321 | ms/batch 4201.20 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   286 /   321 | ms/batch 5102.47 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   288 /   321 | ms/batch 5725.36 | Loss 00.60 |\n",
      "[Training]| Epochs  14 | Batch   290 /   321 | ms/batch 5376.22 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   292 /   321 | ms/batch 4204.35 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   294 /   321 | ms/batch 4869.32 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   296 /   321 | ms/batch 4160.27 | Loss 00.57 |\n",
      "[Training]| Epochs  14 | Batch   298 /   321 | ms/batch 4868.61 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   300 /   321 | ms/batch 4293.70 | Loss 00.58 |\n",
      "[Training]| Epochs  14 | Batch   302 /   321 | ms/batch 4017.87 | Loss 00.55 |\n",
      "[Training]| Epochs  14 | Batch   304 /   321 | ms/batch 5608.92 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   306 /   321 | ms/batch 6074.93 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   308 /   321 | ms/batch 4408.34 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   310 /   321 | ms/batch 4183.69 | Loss 00.56 |\n",
      "[Training]| Epochs  14 | Batch   312 /   321 | ms/batch 5771.71 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   314 /   321 | ms/batch 5583.29 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   316 /   321 | ms/batch 5829.30 | Loss 00.59 |\n",
      "[Training]| Epochs  14 | Batch   318 /   321 | ms/batch 4749.83 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  14 | Batch   320 /   321 | ms/batch 4966.93 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  14 | Elapsed 1425.21 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.79\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.44\n",
      "result= [15, 0.09263741663444441, 0.11904018815383258, 0.17580059999984776, 0.20894807739733534, 0.2280202317033442, 0.1193294510280166, 0.07918964246242936, 0.04112724429076292, 0.029805255512288093, 0.02515146161835359, 0.08430467874671181, 0.07814175731394316, 0.05875034585002801, 0.04753566810328423, 0.04199511021817, 0.083439992976666, 0.08851516230354145, 0.09429514106066893, 0.09639076855242694, 0.09740240528794124, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847434563401305]\n",
      "[Training]| Epochs  15 | Batch     2 /   321 | ms/batch 6565.93 | Loss 00.86 |\n",
      "[Training]| Epochs  15 | Batch     4 /   321 | ms/batch 4693.17 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch     6 /   321 | ms/batch 6151.02 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch     8 /   321 | ms/batch 5723.22 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch    10 /   321 | ms/batch 5593.35 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch    12 /   321 | ms/batch 5533.68 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    14 /   321 | ms/batch 4098.79 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    16 /   321 | ms/batch 4333.79 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    18 /   321 | ms/batch 4485.19 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    20 /   321 | ms/batch 4745.49 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    22 /   321 | ms/batch 6395.69 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch    24 /   321 | ms/batch 4887.21 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    26 /   321 | ms/batch 4801.64 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    28 /   321 | ms/batch 4811.37 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  15 | Batch    30 /   321 | ms/batch 4563.80 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    32 /   321 | ms/batch 4175.37 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    34 /   321 | ms/batch 4204.95 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    36 /   321 | ms/batch 5620.69 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch    38 /   321 | ms/batch 5312.40 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    40 /   321 | ms/batch 5151.75 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    42 /   321 | ms/batch 5059.11 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch    44 /   321 | ms/batch 5088.09 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    46 /   321 | ms/batch 4120.37 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    48 /   321 | ms/batch 5134.41 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    50 /   321 | ms/batch 4292.35 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    52 /   321 | ms/batch 5067.57 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    54 /   321 | ms/batch 4598.53 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    56 /   321 | ms/batch 4931.09 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    58 /   321 | ms/batch 4685.71 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    60 /   321 | ms/batch 3928.03 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    62 /   321 | ms/batch 4239.32 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    64 /   321 | ms/batch 4401.42 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    66 /   321 | ms/batch 4444.85 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    68 /   321 | ms/batch 4434.90 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    70 /   321 | ms/batch 4568.91 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    72 /   321 | ms/batch 3596.90 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    74 /   321 | ms/batch 4243.21 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    76 /   321 | ms/batch 4797.67 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    78 /   321 | ms/batch 5324.76 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    80 /   321 | ms/batch 5069.01 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    82 /   321 | ms/batch 5381.03 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch    84 /   321 | ms/batch 3854.93 | Loss 00.55 |\n",
      "[Training]| Epochs  15 | Batch    86 /   321 | ms/batch 5967.42 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch    88 /   321 | ms/batch 5862.47 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    90 /   321 | ms/batch 4477.36 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    92 /   321 | ms/batch 5429.06 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch    94 /   321 | ms/batch 4680.77 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch    96 /   321 | ms/batch 4256.00 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch    98 /   321 | ms/batch 5423.04 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   100 /   321 | ms/batch 5510.01 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   102 /   321 | ms/batch 4511.40 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   104 /   321 | ms/batch 5345.12 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   106 /   321 | ms/batch 3778.13 | Loss 00.55 |\n",
      "[Training]| Epochs  15 | Batch   108 /   321 | ms/batch 4218.34 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   110 /   321 | ms/batch 5398.60 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   112 /   321 | ms/batch 3839.55 | Loss 00.54 |\n",
      "[Training]| Epochs  15 | Batch   114 /   321 | ms/batch 4836.42 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   116 /   321 | ms/batch 5078.00 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   118 /   321 | ms/batch 5281.59 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   120 /   321 | ms/batch 4960.16 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   122 /   321 | ms/batch 5091.30 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   124 /   321 | ms/batch 5407.47 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   126 /   321 | ms/batch 4885.00 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   128 /   321 | ms/batch 5284.22 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   130 /   321 | ms/batch 5220.03 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   132 /   321 | ms/batch 3380.81 | Loss 00.53 |\n",
      "[Training]| Epochs  15 | Batch   134 /   321 | ms/batch 5641.37 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   136 /   321 | ms/batch 4658.16 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   138 /   321 | ms/batch 4146.54 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   140 /   321 | ms/batch 4694.51 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   142 /   321 | ms/batch 5902.45 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   144 /   321 | ms/batch 5296.40 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   146 /   321 | ms/batch 4836.04 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   148 /   321 | ms/batch 6231.42 | Loss 00.61 |\n",
      "[Training]| Epochs  15 | Batch   150 /   321 | ms/batch 4079.47 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   152 /   321 | ms/batch 4658.77 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   154 /   321 | ms/batch 4041.73 | Loss 00.55 |\n",
      "[Training]| Epochs  15 | Batch   156 /   321 | ms/batch 5757.60 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   158 /   321 | ms/batch 5840.01 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   160 /   321 | ms/batch 4448.11 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   162 /   321 | ms/batch 4958.41 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   164 /   321 | ms/batch 3957.71 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   166 /   321 | ms/batch 4943.33 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   168 /   321 | ms/batch 6884.67 | Loss 00.61 |\n",
      "[Training]| Epochs  15 | Batch   170 /   321 | ms/batch 5535.86 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   172 /   321 | ms/batch 5178.38 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   174 /   321 | ms/batch 4682.10 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   176 /   321 | ms/batch 5075.04 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   178 /   321 | ms/batch 6064.35 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   180 /   321 | ms/batch 5170.31 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   182 /   321 | ms/batch 5301.39 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   184 /   321 | ms/batch 4873.45 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   186 /   321 | ms/batch 5910.59 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   188 /   321 | ms/batch 4369.71 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   190 /   321 | ms/batch 4854.40 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   192 /   321 | ms/batch 5591.85 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   194 /   321 | ms/batch 6326.70 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   196 /   321 | ms/batch 6123.04 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   198 /   321 | ms/batch 4911.16 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   200 /   321 | ms/batch 4012.48 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   202 /   321 | ms/batch 4522.44 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   204 /   321 | ms/batch 4546.30 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   206 /   321 | ms/batch 4548.35 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   208 /   321 | ms/batch 4177.22 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   210 /   321 | ms/batch 4026.10 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   212 /   321 | ms/batch 4543.07 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   214 /   321 | ms/batch 5154.24 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   216 /   321 | ms/batch 4377.89 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   218 /   321 | ms/batch 5301.69 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   220 /   321 | ms/batch 5157.01 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   222 /   321 | ms/batch 4421.17 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   224 /   321 | ms/batch 5318.78 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   226 /   321 | ms/batch 5077.97 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   228 /   321 | ms/batch 4929.92 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   230 /   321 | ms/batch 5727.91 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   232 /   321 | ms/batch 4913.57 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   234 /   321 | ms/batch 5315.61 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   236 /   321 | ms/batch 5310.07 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  15 | Batch   238 /   321 | ms/batch 4552.35 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   240 /   321 | ms/batch 3639.13 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   242 /   321 | ms/batch 3553.42 | Loss 00.52 |\n",
      "[Training]| Epochs  15 | Batch   244 /   321 | ms/batch 5608.22 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   246 /   321 | ms/batch 4842.00 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   248 /   321 | ms/batch 5399.98 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   250 /   321 | ms/batch 5149.48 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   252 /   321 | ms/batch 4933.73 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   254 /   321 | ms/batch 4696.12 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   256 /   321 | ms/batch 5613.68 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   258 /   321 | ms/batch 4187.57 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   260 /   321 | ms/batch 4510.77 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   262 /   321 | ms/batch 5979.18 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   264 /   321 | ms/batch 4439.73 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   266 /   321 | ms/batch 4685.58 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   268 /   321 | ms/batch 5227.91 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   270 /   321 | ms/batch 4985.85 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   272 /   321 | ms/batch 5708.81 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   274 /   321 | ms/batch 5378.94 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   276 /   321 | ms/batch 4518.51 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   278 /   321 | ms/batch 5771.30 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   280 /   321 | ms/batch 4868.98 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   282 /   321 | ms/batch 4853.70 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   284 /   321 | ms/batch 4240.48 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   286 /   321 | ms/batch 4922.55 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   288 /   321 | ms/batch 5589.76 | Loss 00.60 |\n",
      "[Training]| Epochs  15 | Batch   290 /   321 | ms/batch 5162.55 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   292 /   321 | ms/batch 4102.44 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   294 /   321 | ms/batch 4856.87 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   296 /   321 | ms/batch 3907.80 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   298 /   321 | ms/batch 5130.40 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   300 /   321 | ms/batch 4345.00 | Loss 00.58 |\n",
      "[Training]| Epochs  15 | Batch   302 /   321 | ms/batch 4068.56 | Loss 00.55 |\n",
      "[Training]| Epochs  15 | Batch   304 /   321 | ms/batch 5783.19 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   306 /   321 | ms/batch 6114.60 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   308 /   321 | ms/batch 4338.28 | Loss 00.57 |\n",
      "[Training]| Epochs  15 | Batch   310 /   321 | ms/batch 4177.71 | Loss 00.56 |\n",
      "[Training]| Epochs  15 | Batch   312 /   321 | ms/batch 5944.30 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   314 /   321 | ms/batch 5734.06 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   316 /   321 | ms/batch 5830.02 | Loss 00.59 |\n",
      "[Training]| Epochs  15 | Batch   318 /   321 | ms/batch 4820.87 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  15 | Batch   320 /   321 | ms/batch 4932.05 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  15 | Elapsed 1425.91 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.32\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.55\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.52\n",
      "result= [16, 0.09379489628114648, 0.11933554027197608, 0.17507416033131576, 0.20880559860312745, 0.22880705241012275, 0.12431789763904788, 0.08004702466219919, 0.04065959451724474, 0.0299143564487377, 0.025271364421651222, 0.08681771686461784, 0.07873061812676588, 0.05817751687481732, 0.04772008180804115, 0.04219207704206602, 0.08375014811289654, 0.08842106025291437, 0.09407827735149729, 0.09626580728726085, 0.09731137583000311, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5852971334516266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  16 | Batch     2 /   321 | ms/batch 6796.49 | Loss 00.85 |\n",
      "[Training]| Epochs  16 | Batch     4 /   321 | ms/batch 4762.46 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch     6 /   321 | ms/batch 6159.16 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch     8 /   321 | ms/batch 5612.87 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch    10 /   321 | ms/batch 5533.46 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    12 /   321 | ms/batch 5767.46 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    14 /   321 | ms/batch 4151.05 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    16 /   321 | ms/batch 4244.95 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    18 /   321 | ms/batch 4315.78 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    20 /   321 | ms/batch 4817.16 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    22 /   321 | ms/batch 6310.01 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch    24 /   321 | ms/batch 4800.70 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    26 /   321 | ms/batch 4705.58 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    28 /   321 | ms/batch 5060.97 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    30 /   321 | ms/batch 4565.10 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    32 /   321 | ms/batch 4532.40 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    34 /   321 | ms/batch 4135.59 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    36 /   321 | ms/batch 5561.91 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch    38 /   321 | ms/batch 5521.86 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    40 /   321 | ms/batch 5346.98 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    42 /   321 | ms/batch 5160.36 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    44 /   321 | ms/batch 5222.02 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    46 /   321 | ms/batch 4304.68 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    48 /   321 | ms/batch 5182.99 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    50 /   321 | ms/batch 4834.89 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    52 /   321 | ms/batch 4951.36 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    54 /   321 | ms/batch 4505.90 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    56 /   321 | ms/batch 4728.89 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    58 /   321 | ms/batch 4875.72 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    60 /   321 | ms/batch 4208.06 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    62 /   321 | ms/batch 4396.99 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    64 /   321 | ms/batch 4271.43 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    66 /   321 | ms/batch 4360.46 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    68 /   321 | ms/batch 4644.56 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    70 /   321 | ms/batch 4560.51 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    72 /   321 | ms/batch 3956.06 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    74 /   321 | ms/batch 4325.82 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    76 /   321 | ms/batch 4589.57 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    78 /   321 | ms/batch 5795.60 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    80 /   321 | ms/batch 4931.65 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    82 /   321 | ms/batch 5405.57 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    84 /   321 | ms/batch 4043.14 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch    86 /   321 | ms/batch 6021.19 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    88 /   321 | ms/batch 5744.60 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    90 /   321 | ms/batch 4614.05 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch    92 /   321 | ms/batch 5276.05 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch    94 /   321 | ms/batch 4748.44 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch    96 /   321 | ms/batch 4308.41 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch    98 /   321 | ms/batch 5588.49 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   100 /   321 | ms/batch 5579.18 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   102 /   321 | ms/batch 4260.20 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   104 /   321 | ms/batch 5306.70 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   106 /   321 | ms/batch 3722.69 | Loss 00.54 |\n",
      "[Training]| Epochs  16 | Batch   108 /   321 | ms/batch 4174.72 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   110 /   321 | ms/batch 5505.87 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   112 /   321 | ms/batch 3785.16 | Loss 00.54 |\n",
      "[Training]| Epochs  16 | Batch   114 /   321 | ms/batch 4768.33 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   116 /   321 | ms/batch 5391.59 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   118 /   321 | ms/batch 5126.13 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   120 /   321 | ms/batch 5087.01 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   122 /   321 | ms/batch 5234.53 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   124 /   321 | ms/batch 5162.96 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   126 /   321 | ms/batch 4935.13 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   128 /   321 | ms/batch 5125.39 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   130 /   321 | ms/batch 5419.92 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   132 /   321 | ms/batch 3228.74 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch   134 /   321 | ms/batch 5530.04 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   136 /   321 | ms/batch 4633.80 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   138 /   321 | ms/batch 4157.79 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   140 /   321 | ms/batch 4567.49 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   142 /   321 | ms/batch 6082.49 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   144 /   321 | ms/batch 5439.92 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   146 /   321 | ms/batch 4911.29 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   148 /   321 | ms/batch 6322.96 | Loss 00.61 |\n",
      "[Training]| Epochs  16 | Batch   150 /   321 | ms/batch 4298.34 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   152 /   321 | ms/batch 4837.98 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   154 /   321 | ms/batch 4045.17 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch   156 /   321 | ms/batch 5635.81 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   158 /   321 | ms/batch 6006.31 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   160 /   321 | ms/batch 4345.95 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   162 /   321 | ms/batch 4824.14 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   164 /   321 | ms/batch 3810.78 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   166 /   321 | ms/batch 5010.00 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   168 /   321 | ms/batch 6596.63 | Loss 00.61 |\n",
      "[Training]| Epochs  16 | Batch   170 /   321 | ms/batch 5356.93 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   172 /   321 | ms/batch 5096.36 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   174 /   321 | ms/batch 4621.26 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   176 /   321 | ms/batch 5087.91 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   178 /   321 | ms/batch 5928.14 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   180 /   321 | ms/batch 5097.85 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   182 /   321 | ms/batch 5293.52 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   184 /   321 | ms/batch 4810.17 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   186 /   321 | ms/batch 5881.66 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   188 /   321 | ms/batch 4415.05 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   190 /   321 | ms/batch 4844.96 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   192 /   321 | ms/batch 5811.42 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   194 /   321 | ms/batch 6273.33 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   196 /   321 | ms/batch 6277.33 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   198 /   321 | ms/batch 5026.92 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   200 /   321 | ms/batch 4236.01 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   202 /   321 | ms/batch 4675.44 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   204 /   321 | ms/batch 4309.40 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   206 /   321 | ms/batch 4755.03 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   208 /   321 | ms/batch 4387.82 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  16 | Batch   210 /   321 | ms/batch 4016.18 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   212 /   321 | ms/batch 4276.18 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   214 /   321 | ms/batch 4971.09 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   216 /   321 | ms/batch 4558.45 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   218 /   321 | ms/batch 5391.92 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   220 /   321 | ms/batch 5114.50 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   222 /   321 | ms/batch 4371.98 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   224 /   321 | ms/batch 5313.79 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   226 /   321 | ms/batch 4970.21 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   228 /   321 | ms/batch 4941.39 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   230 /   321 | ms/batch 5642.50 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   232 /   321 | ms/batch 4676.77 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   234 /   321 | ms/batch 5123.11 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   236 /   321 | ms/batch 5340.12 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   238 /   321 | ms/batch 4461.88 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   240 /   321 | ms/batch 3701.29 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   242 /   321 | ms/batch 3458.17 | Loss 00.52 |\n",
      "[Training]| Epochs  16 | Batch   244 /   321 | ms/batch 5558.26 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   246 /   321 | ms/batch 5223.15 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   248 /   321 | ms/batch 5227.78 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   250 /   321 | ms/batch 5263.58 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   252 /   321 | ms/batch 5026.44 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   254 /   321 | ms/batch 4689.11 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   256 /   321 | ms/batch 5625.17 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   258 /   321 | ms/batch 4027.91 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   260 /   321 | ms/batch 4470.13 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   262 /   321 | ms/batch 6142.83 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   264 /   321 | ms/batch 4568.40 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   266 /   321 | ms/batch 4625.68 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   268 /   321 | ms/batch 5019.92 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   270 /   321 | ms/batch 5335.80 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   272 /   321 | ms/batch 5517.70 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   274 /   321 | ms/batch 5541.98 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   276 /   321 | ms/batch 4399.99 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   278 /   321 | ms/batch 5870.58 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   280 /   321 | ms/batch 4500.54 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   282 /   321 | ms/batch 4731.56 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   284 /   321 | ms/batch 4112.57 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch   286 /   321 | ms/batch 5094.84 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   288 /   321 | ms/batch 5691.58 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   290 /   321 | ms/batch 5359.84 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   292 /   321 | ms/batch 4001.64 | Loss 00.57 |\n",
      "[Training]| Epochs  16 | Batch   294 /   321 | ms/batch 4809.50 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   296 /   321 | ms/batch 4118.10 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   298 /   321 | ms/batch 4952.29 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   300 /   321 | ms/batch 4230.26 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   302 /   321 | ms/batch 4052.93 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch   304 /   321 | ms/batch 5609.32 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   306 /   321 | ms/batch 6012.94 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   308 /   321 | ms/batch 4188.03 | Loss 00.56 |\n",
      "[Training]| Epochs  16 | Batch   310 /   321 | ms/batch 4154.48 | Loss 00.55 |\n",
      "[Training]| Epochs  16 | Batch   312 /   321 | ms/batch 5886.91 | Loss 00.59 |\n",
      "[Training]| Epochs  16 | Batch   314 /   321 | ms/batch 5755.86 | Loss 00.60 |\n",
      "[Training]| Epochs  16 | Batch   316 /   321 | ms/batch 5846.20 | Loss 00.58 |\n",
      "[Training]| Epochs  16 | Batch   318 /   321 | ms/batch 4661.33 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  16 | Batch   320 /   321 | ms/batch 5938.91 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  16 | Elapsed 1423.14 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.94\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.58\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.50\n",
      "result= [17, 0.0929484200124525, 0.11879578588163119, 0.1752135231452163, 0.20857401704379383, 0.23028626297008964, 0.11979712161438036, 0.08004701276914458, 0.04091939532190027, 0.03014037503177824, 0.0255471346253821, 0.08463817783751157, 0.07868075649532041, 0.058450914414153836, 0.04803188499397165, 0.04262643220920816, 0.08371262786153556, 0.08875779497574374, 0.09448629269933158, 0.09668345713809376, 0.09779554670760067, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847414062346941]\n",
      "[Training]| Epochs  17 | Batch     2 /   321 | ms/batch 6613.96 | Loss 00.86 |\n",
      "[Training]| Epochs  17 | Batch     4 /   321 | ms/batch 4932.55 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch     6 /   321 | ms/batch 5947.78 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch     8 /   321 | ms/batch 5705.30 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch    10 /   321 | ms/batch 5340.17 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    12 /   321 | ms/batch 5651.71 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    14 /   321 | ms/batch 4234.81 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    16 /   321 | ms/batch 4466.34 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    18 /   321 | ms/batch 4388.90 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    20 /   321 | ms/batch 4724.16 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    22 /   321 | ms/batch 6327.83 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch    24 /   321 | ms/batch 4797.48 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    26 /   321 | ms/batch 4798.29 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    28 /   321 | ms/batch 4958.93 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    30 /   321 | ms/batch 4586.02 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    32 /   321 | ms/batch 4574.55 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    34 /   321 | ms/batch 4213.44 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    36 /   321 | ms/batch 5728.22 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    38 /   321 | ms/batch 5633.75 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    40 /   321 | ms/batch 5413.62 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    42 /   321 | ms/batch 5004.80 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    44 /   321 | ms/batch 4849.59 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    46 /   321 | ms/batch 4255.91 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    48 /   321 | ms/batch 5267.26 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    50 /   321 | ms/batch 4522.51 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    52 /   321 | ms/batch 5004.91 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    54 /   321 | ms/batch 4482.25 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    56 /   321 | ms/batch 4958.64 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    58 /   321 | ms/batch 4998.01 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    60 /   321 | ms/batch 4142.79 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    62 /   321 | ms/batch 4294.37 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    64 /   321 | ms/batch 4613.46 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    66 /   321 | ms/batch 4521.18 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    68 /   321 | ms/batch 4815.28 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    70 /   321 | ms/batch 4698.34 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    72 /   321 | ms/batch 3807.42 | Loss 00.55 |\n",
      "[Training]| Epochs  17 | Batch    74 /   321 | ms/batch 4283.03 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    76 /   321 | ms/batch 4782.71 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    78 /   321 | ms/batch 5574.00 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    80 /   321 | ms/batch 4936.93 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    82 /   321 | ms/batch 5531.77 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    84 /   321 | ms/batch 3769.80 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    86 /   321 | ms/batch 5907.05 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch    88 /   321 | ms/batch 5945.28 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    90 /   321 | ms/batch 4371.84 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    92 /   321 | ms/batch 5288.73 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch    94 /   321 | ms/batch 4684.76 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch    96 /   321 | ms/batch 4374.94 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch    98 /   321 | ms/batch 5391.95 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   100 /   321 | ms/batch 5519.49 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   102 /   321 | ms/batch 4313.22 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   104 /   321 | ms/batch 5634.73 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   106 /   321 | ms/batch 3895.00 | Loss 00.54 |\n",
      "[Training]| Epochs  17 | Batch   108 /   321 | ms/batch 4222.12 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   110 /   321 | ms/batch 5489.27 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   112 /   321 | ms/batch 3990.21 | Loss 00.55 |\n",
      "[Training]| Epochs  17 | Batch   114 /   321 | ms/batch 4689.22 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   116 /   321 | ms/batch 5256.87 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   118 /   321 | ms/batch 5201.80 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   120 /   321 | ms/batch 5053.30 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   122 /   321 | ms/batch 5236.32 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   124 /   321 | ms/batch 5251.27 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   126 /   321 | ms/batch 5218.29 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   128 /   321 | ms/batch 5335.14 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   130 /   321 | ms/batch 5449.34 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   132 /   321 | ms/batch 3311.12 | Loss 00.54 |\n",
      "[Training]| Epochs  17 | Batch   134 /   321 | ms/batch 5469.22 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   136 /   321 | ms/batch 4674.68 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   138 /   321 | ms/batch 3791.97 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   140 /   321 | ms/batch 4451.26 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   142 /   321 | ms/batch 5969.23 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   144 /   321 | ms/batch 5276.07 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   146 /   321 | ms/batch 4843.06 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   148 /   321 | ms/batch 5927.61 | Loss 00.61 |\n",
      "[Training]| Epochs  17 | Batch   150 /   321 | ms/batch 4067.53 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   152 /   321 | ms/batch 5001.53 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   154 /   321 | ms/batch 4170.44 | Loss 00.54 |\n",
      "[Training]| Epochs  17 | Batch   156 /   321 | ms/batch 5831.17 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   158 /   321 | ms/batch 6263.17 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   160 /   321 | ms/batch 4340.63 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   162 /   321 | ms/batch 4830.44 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   164 /   321 | ms/batch 3991.62 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   166 /   321 | ms/batch 4736.55 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   168 /   321 | ms/batch 6615.64 | Loss 00.61 |\n",
      "[Training]| Epochs  17 | Batch   170 /   321 | ms/batch 5349.98 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   172 /   321 | ms/batch 4919.93 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  17 | Batch   174 /   321 | ms/batch 4629.18 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   176 /   321 | ms/batch 5175.00 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   178 /   321 | ms/batch 5928.09 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   180 /   321 | ms/batch 5203.74 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   182 /   321 | ms/batch 5438.11 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   184 /   321 | ms/batch 4767.06 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   186 /   321 | ms/batch 5841.07 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   188 /   321 | ms/batch 4369.93 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   190 /   321 | ms/batch 4819.06 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   192 /   321 | ms/batch 5665.07 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   194 /   321 | ms/batch 6347.71 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   196 /   321 | ms/batch 6122.92 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   198 /   321 | ms/batch 5054.86 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   200 /   321 | ms/batch 4145.74 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   202 /   321 | ms/batch 4586.09 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   204 /   321 | ms/batch 4461.99 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   206 /   321 | ms/batch 4744.46 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   208 /   321 | ms/batch 4338.75 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   210 /   321 | ms/batch 4131.60 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   212 /   321 | ms/batch 4453.56 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   214 /   321 | ms/batch 5052.90 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   216 /   321 | ms/batch 4694.74 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   218 /   321 | ms/batch 5611.53 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   220 /   321 | ms/batch 5102.28 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   222 /   321 | ms/batch 4306.84 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   224 /   321 | ms/batch 5327.04 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   226 /   321 | ms/batch 4921.77 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   228 /   321 | ms/batch 4887.58 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   230 /   321 | ms/batch 5735.73 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   232 /   321 | ms/batch 4704.69 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   234 /   321 | ms/batch 5169.14 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   236 /   321 | ms/batch 5565.79 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   238 /   321 | ms/batch 4560.44 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   240 /   321 | ms/batch 3846.52 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   242 /   321 | ms/batch 3380.69 | Loss 00.51 |\n",
      "[Training]| Epochs  17 | Batch   244 /   321 | ms/batch 5445.11 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   246 /   321 | ms/batch 4714.47 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   248 /   321 | ms/batch 5238.82 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   250 /   321 | ms/batch 4899.65 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   252 /   321 | ms/batch 5099.33 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   254 /   321 | ms/batch 4584.30 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   256 /   321 | ms/batch 5277.42 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   258 /   321 | ms/batch 4079.83 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   260 /   321 | ms/batch 4574.22 | Loss 00.56 |\n",
      "[Training]| Epochs  17 | Batch   262 /   321 | ms/batch 6061.84 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   264 /   321 | ms/batch 4451.22 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   266 /   321 | ms/batch 4539.90 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   268 /   321 | ms/batch 4839.09 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   270 /   321 | ms/batch 5173.19 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   272 /   321 | ms/batch 5483.97 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   274 /   321 | ms/batch 5644.41 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   276 /   321 | ms/batch 4531.68 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   278 /   321 | ms/batch 5801.85 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   280 /   321 | ms/batch 4901.21 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   282 /   321 | ms/batch 4817.41 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   284 /   321 | ms/batch 4101.18 | Loss 00.55 |\n",
      "[Training]| Epochs  17 | Batch   286 /   321 | ms/batch 5181.79 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   288 /   321 | ms/batch 5938.54 | Loss 00.60 |\n",
      "[Training]| Epochs  17 | Batch   290 /   321 | ms/batch 5277.79 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   292 /   321 | ms/batch 4171.70 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   294 /   321 | ms/batch 4929.53 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   296 /   321 | ms/batch 4100.67 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   298 /   321 | ms/batch 5114.00 | Loss 00.58 |\n",
      "[Training]| Epochs  17 | Batch   300 /   321 | ms/batch 4515.64 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   302 /   321 | ms/batch 4015.64 | Loss 00.54 |\n",
      "[Training]| Epochs  17 | Batch   304 /   321 | ms/batch 5554.98 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   306 /   321 | ms/batch 6038.37 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   308 /   321 | ms/batch 4382.90 | Loss 00.57 |\n",
      "[Training]| Epochs  17 | Batch   310 /   321 | ms/batch 4343.15 | Loss 00.55 |\n",
      "[Training]| Epochs  17 | Batch   312 /   321 | ms/batch 5639.79 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   314 /   321 | ms/batch 5848.16 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   316 /   321 | ms/batch 5662.57 | Loss 00.59 |\n",
      "[Training]| Epochs  17 | Batch   318 /   321 | ms/batch 4704.53 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs  17 | Batch   320 /   321 | ms/batch 4751.82 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  17 | Elapsed 1438.80 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.37\n",
      "result= [18, 0.09265166451386521, 0.11896055226017513, 0.17478518289044231, 0.20849057537266172, 0.22804527847634937, 0.11995300388113186, 0.07985217480202346, 0.04107527461538813, 0.0297117166377935, 0.025109491028640758, 0.08463205886091558, 0.07847177173974267, 0.05869441281420499, 0.047415566091321915, 0.04192668055521726, 0.08339937785400814, 0.08846067914932339, 0.09417716017633891, 0.0962761838841292, 0.09731312503490891, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5842190605622751]\n",
      "[Training]| Epochs  18 | Batch     2 /   321 | ms/batch 6403.54 | Loss 00.85 |\n",
      "[Training]| Epochs  18 | Batch     4 /   321 | ms/batch 4960.12 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch     6 /   321 | ms/batch 6206.52 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch     8 /   321 | ms/batch 5982.30 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch    10 /   321 | ms/batch 5402.87 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    12 /   321 | ms/batch 5718.61 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    14 /   321 | ms/batch 4123.09 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    16 /   321 | ms/batch 4187.82 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    18 /   321 | ms/batch 4239.85 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    20 /   321 | ms/batch 4776.81 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    22 /   321 | ms/batch 6227.86 | Loss 00.61 |\n",
      "[Training]| Epochs  18 | Batch    24 /   321 | ms/batch 4716.14 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    26 /   321 | ms/batch 4745.18 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    28 /   321 | ms/batch 5125.93 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    30 /   321 | ms/batch 4795.40 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    32 /   321 | ms/batch 4774.33 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    34 /   321 | ms/batch 4214.49 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    36 /   321 | ms/batch 5536.48 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch    38 /   321 | ms/batch 5557.36 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    40 /   321 | ms/batch 5293.99 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch    42 /   321 | ms/batch 5084.33 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch    44 /   321 | ms/batch 5024.51 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    46 /   321 | ms/batch 4134.20 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    48 /   321 | ms/batch 5277.20 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    50 /   321 | ms/batch 4380.84 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    52 /   321 | ms/batch 5077.67 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    54 /   321 | ms/batch 4557.99 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    56 /   321 | ms/batch 4813.96 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    58 /   321 | ms/batch 4681.07 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    60 /   321 | ms/batch 3926.73 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    62 /   321 | ms/batch 4039.58 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    64 /   321 | ms/batch 4312.13 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    66 /   321 | ms/batch 4400.40 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    68 /   321 | ms/batch 4571.25 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch    70 /   321 | ms/batch 4453.37 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    72 /   321 | ms/batch 3884.87 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    74 /   321 | ms/batch 4349.17 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    76 /   321 | ms/batch 4698.18 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    78 /   321 | ms/batch 5579.76 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch    80 /   321 | ms/batch 5061.78 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch    82 /   321 | ms/batch 5121.32 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    84 /   321 | ms/batch 3804.96 | Loss 00.55 |\n",
      "[Training]| Epochs  18 | Batch    86 /   321 | ms/batch 6007.22 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch    88 /   321 | ms/batch 5850.17 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch    90 /   321 | ms/batch 4422.74 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    92 /   321 | ms/batch 5295.27 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    94 /   321 | ms/batch 4774.05 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch    96 /   321 | ms/batch 4285.65 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch    98 /   321 | ms/batch 5444.18 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   100 /   321 | ms/batch 5410.47 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   102 /   321 | ms/batch 4337.19 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   104 /   321 | ms/batch 5399.61 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   106 /   321 | ms/batch 3892.05 | Loss 00.54 |\n",
      "[Training]| Epochs  18 | Batch   108 /   321 | ms/batch 4517.72 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   110 /   321 | ms/batch 5741.43 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   112 /   321 | ms/batch 4018.29 | Loss 00.55 |\n",
      "[Training]| Epochs  18 | Batch   114 /   321 | ms/batch 4657.59 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   116 /   321 | ms/batch 5020.59 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   118 /   321 | ms/batch 5006.93 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   120 /   321 | ms/batch 4953.83 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   122 /   321 | ms/batch 5168.99 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   124 /   321 | ms/batch 5135.59 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   126 /   321 | ms/batch 4796.15 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   128 /   321 | ms/batch 5258.63 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   130 /   321 | ms/batch 5430.65 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   132 /   321 | ms/batch 3167.57 | Loss 00.54 |\n",
      "[Training]| Epochs  18 | Batch   134 /   321 | ms/batch 5479.01 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   136 /   321 | ms/batch 4619.95 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  18 | Batch   138 /   321 | ms/batch 4091.69 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   140 /   321 | ms/batch 4544.47 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   142 /   321 | ms/batch 6147.74 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   144 /   321 | ms/batch 5375.01 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   146 /   321 | ms/batch 4909.28 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   148 /   321 | ms/batch 6349.29 | Loss 00.61 |\n",
      "[Training]| Epochs  18 | Batch   150 /   321 | ms/batch 4299.76 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   152 /   321 | ms/batch 4935.69 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   154 /   321 | ms/batch 4274.41 | Loss 00.54 |\n",
      "[Training]| Epochs  18 | Batch   156 /   321 | ms/batch 5618.79 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   158 /   321 | ms/batch 5941.59 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   160 /   321 | ms/batch 4499.70 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   162 /   321 | ms/batch 4354.79 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   164 /   321 | ms/batch 3687.10 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   166 /   321 | ms/batch 4446.03 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   168 /   321 | ms/batch 6255.06 | Loss 00.61 |\n",
      "[Training]| Epochs  18 | Batch   170 /   321 | ms/batch 4553.88 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   172 /   321 | ms/batch 4986.85 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   174 /   321 | ms/batch 4606.60 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   176 /   321 | ms/batch 5057.62 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   178 /   321 | ms/batch 6200.34 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   180 /   321 | ms/batch 5274.82 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   182 /   321 | ms/batch 5430.37 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   184 /   321 | ms/batch 4751.66 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   186 /   321 | ms/batch 6074.66 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   188 /   321 | ms/batch 4303.70 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   190 /   321 | ms/batch 4786.40 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   192 /   321 | ms/batch 5659.14 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   194 /   321 | ms/batch 6102.78 | Loss 00.61 |\n",
      "[Training]| Epochs  18 | Batch   196 /   321 | ms/batch 6099.72 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   198 /   321 | ms/batch 5008.72 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   200 /   321 | ms/batch 4103.07 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   202 /   321 | ms/batch 4449.08 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   204 /   321 | ms/batch 4471.05 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   206 /   321 | ms/batch 4623.37 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   208 /   321 | ms/batch 4395.28 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   210 /   321 | ms/batch 3869.47 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   212 /   321 | ms/batch 4556.93 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   214 /   321 | ms/batch 5109.60 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   216 /   321 | ms/batch 4424.19 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   218 /   321 | ms/batch 5515.47 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   220 /   321 | ms/batch 5125.59 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   222 /   321 | ms/batch 4398.80 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   224 /   321 | ms/batch 5266.81 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   226 /   321 | ms/batch 5076.13 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   228 /   321 | ms/batch 4841.51 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   230 /   321 | ms/batch 5597.51 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   232 /   321 | ms/batch 4934.22 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   234 /   321 | ms/batch 5432.55 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   236 /   321 | ms/batch 5339.06 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   238 /   321 | ms/batch 4443.74 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   240 /   321 | ms/batch 3918.47 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   242 /   321 | ms/batch 3351.64 | Loss 00.52 |\n",
      "[Training]| Epochs  18 | Batch   244 /   321 | ms/batch 5413.53 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   246 /   321 | ms/batch 4944.68 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   248 /   321 | ms/batch 5559.68 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   250 /   321 | ms/batch 5227.32 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   252 /   321 | ms/batch 4886.27 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   254 /   321 | ms/batch 4401.25 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   256 /   321 | ms/batch 5451.16 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   258 /   321 | ms/batch 4184.03 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   260 /   321 | ms/batch 4505.87 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   262 /   321 | ms/batch 6351.02 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   264 /   321 | ms/batch 4305.33 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   266 /   321 | ms/batch 4608.41 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   268 /   321 | ms/batch 4909.76 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   270 /   321 | ms/batch 5384.70 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   272 /   321 | ms/batch 5678.96 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   274 /   321 | ms/batch 5521.29 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   276 /   321 | ms/batch 4505.52 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   278 /   321 | ms/batch 5575.63 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   280 /   321 | ms/batch 4616.98 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   282 /   321 | ms/batch 4666.83 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   284 /   321 | ms/batch 4227.81 | Loss 00.55 |\n",
      "[Training]| Epochs  18 | Batch   286 /   321 | ms/batch 5250.73 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   288 /   321 | ms/batch 5584.70 | Loss 00.60 |\n",
      "[Training]| Epochs  18 | Batch   290 /   321 | ms/batch 5491.52 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   292 /   321 | ms/batch 4196.90 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   294 /   321 | ms/batch 4776.20 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   296 /   321 | ms/batch 4220.51 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   298 /   321 | ms/batch 4915.72 | Loss 00.58 |\n",
      "[Training]| Epochs  18 | Batch   300 /   321 | ms/batch 4325.88 | Loss 00.57 |\n",
      "[Training]| Epochs  18 | Batch   302 /   321 | ms/batch 3920.09 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   304 /   321 | ms/batch 5453.36 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   306 /   321 | ms/batch 6256.47 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   308 /   321 | ms/batch 4492.14 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   310 /   321 | ms/batch 4090.63 | Loss 00.56 |\n",
      "[Training]| Epochs  18 | Batch   312 /   321 | ms/batch 5762.05 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   314 /   321 | ms/batch 5928.57 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   316 /   321 | ms/batch 5852.52 | Loss 00.59 |\n",
      "[Training]| Epochs  18 | Batch   318 /   321 | ms/batch 4790.96 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  18 | Batch   320 /   321 | ms/batch 5083.32 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  18 | Elapsed 1428.61 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.77\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.84\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.56\n",
      "result= [19, 0.09292415223452419, 0.1194640447270192, 0.1753394468074094, 0.20811810868843775, 0.23045012547648333, 0.1197191864275319, 0.08082644788901135, 0.04086743456631643, 0.029789672637487517, 0.02555313467143201, 0.0846015710154271, 0.07934101726148432, 0.058464395191552394, 0.047493159352850374, 0.042617973274117975, 0.082907363127798, 0.08809963605182997, 0.0937209345045734, 0.09583741645651724, 0.09701644757088196, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5843417637142134]\n",
      "[Training]| Epochs  19 | Batch     2 /   321 | ms/batch 6710.19 | Loss 00.85 |\n",
      "[Training]| Epochs  19 | Batch     4 /   321 | ms/batch 4835.45 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch     6 /   321 | ms/batch 6208.42 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch     8 /   321 | ms/batch 5601.45 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch    10 /   321 | ms/batch 5463.06 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    12 /   321 | ms/batch 5613.44 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    14 /   321 | ms/batch 4189.00 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    16 /   321 | ms/batch 4287.08 | Loss 00.55 |\n",
      "[Training]| Epochs  19 | Batch    18 /   321 | ms/batch 4367.76 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    20 /   321 | ms/batch 4712.01 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    22 /   321 | ms/batch 6351.36 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch    24 /   321 | ms/batch 4566.70 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    26 /   321 | ms/batch 4762.22 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    28 /   321 | ms/batch 5083.81 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    30 /   321 | ms/batch 4818.46 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    32 /   321 | ms/batch 4471.33 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    34 /   321 | ms/batch 4127.86 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    36 /   321 | ms/batch 5562.81 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    38 /   321 | ms/batch 5557.39 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    40 /   321 | ms/batch 5252.60 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    42 /   321 | ms/batch 5036.04 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    44 /   321 | ms/batch 5017.75 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    46 /   321 | ms/batch 4382.51 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    48 /   321 | ms/batch 5321.02 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    50 /   321 | ms/batch 4673.06 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    52 /   321 | ms/batch 5125.92 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    54 /   321 | ms/batch 4671.50 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    56 /   321 | ms/batch 4900.05 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    58 /   321 | ms/batch 4789.03 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    60 /   321 | ms/batch 4154.49 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    62 /   321 | ms/batch 4427.75 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    64 /   321 | ms/batch 4204.63 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    66 /   321 | ms/batch 4279.20 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    68 /   321 | ms/batch 4560.76 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    70 /   321 | ms/batch 4653.08 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    72 /   321 | ms/batch 4002.28 | Loss 00.55 |\n",
      "[Training]| Epochs  19 | Batch    74 /   321 | ms/batch 4260.62 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    76 /   321 | ms/batch 4889.79 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch    78 /   321 | ms/batch 5671.18 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    80 /   321 | ms/batch 4887.38 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch    82 /   321 | ms/batch 5420.30 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    84 /   321 | ms/batch 3948.02 | Loss 00.55 |\n",
      "[Training]| Epochs  19 | Batch    86 /   321 | ms/batch 6005.32 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    88 /   321 | ms/batch 5748.30 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    90 /   321 | ms/batch 4383.64 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    92 /   321 | ms/batch 5408.44 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch    94 /   321 | ms/batch 4662.23 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch    96 /   321 | ms/batch 4393.05 | Loss 00.55 |\n",
      "[Training]| Epochs  19 | Batch    98 /   321 | ms/batch 5502.48 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   100 /   321 | ms/batch 5529.11 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  19 | Batch   102 /   321 | ms/batch 4524.75 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   104 /   321 | ms/batch 5423.27 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   106 /   321 | ms/batch 3820.28 | Loss 00.54 |\n",
      "[Training]| Epochs  19 | Batch   108 /   321 | ms/batch 4072.24 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   110 /   321 | ms/batch 5398.35 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   112 /   321 | ms/batch 3726.86 | Loss 00.54 |\n",
      "[Training]| Epochs  19 | Batch   114 /   321 | ms/batch 4724.21 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   116 /   321 | ms/batch 5265.36 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   118 /   321 | ms/batch 5307.20 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   120 /   321 | ms/batch 4895.16 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   122 /   321 | ms/batch 5043.25 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   124 /   321 | ms/batch 5077.12 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   126 /   321 | ms/batch 4635.76 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   128 /   321 | ms/batch 5329.18 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   130 /   321 | ms/batch 5331.51 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   132 /   321 | ms/batch 3176.03 | Loss 00.53 |\n",
      "[Training]| Epochs  19 | Batch   134 /   321 | ms/batch 5597.99 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   136 /   321 | ms/batch 4856.37 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   138 /   321 | ms/batch 4118.54 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   140 /   321 | ms/batch 4545.89 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   142 /   321 | ms/batch 6239.18 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   144 /   321 | ms/batch 5242.91 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   146 /   321 | ms/batch 5134.14 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   148 /   321 | ms/batch 6181.08 | Loss 00.61 |\n",
      "[Training]| Epochs  19 | Batch   150 /   321 | ms/batch 4482.57 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   152 /   321 | ms/batch 5015.44 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   154 /   321 | ms/batch 3864.86 | Loss 00.54 |\n",
      "[Training]| Epochs  19 | Batch   156 /   321 | ms/batch 5588.48 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   158 /   321 | ms/batch 5883.62 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   160 /   321 | ms/batch 4429.61 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   162 /   321 | ms/batch 4790.19 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   164 /   321 | ms/batch 3857.37 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   166 /   321 | ms/batch 4875.63 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   168 /   321 | ms/batch 6752.74 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   170 /   321 | ms/batch 5497.07 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   172 /   321 | ms/batch 5098.70 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   174 /   321 | ms/batch 4446.01 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   176 /   321 | ms/batch 5063.83 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   178 /   321 | ms/batch 6253.13 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   180 /   321 | ms/batch 5064.82 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   182 /   321 | ms/batch 5455.91 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   184 /   321 | ms/batch 4858.12 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   186 /   321 | ms/batch 6205.34 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   188 /   321 | ms/batch 4375.79 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   190 /   321 | ms/batch 4643.43 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   192 /   321 | ms/batch 5877.64 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   194 /   321 | ms/batch 6080.31 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   196 /   321 | ms/batch 6262.44 | Loss 00.61 |\n",
      "[Training]| Epochs  19 | Batch   198 /   321 | ms/batch 5008.90 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   200 /   321 | ms/batch 4191.29 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   202 /   321 | ms/batch 4521.87 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   204 /   321 | ms/batch 4528.16 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   206 /   321 | ms/batch 4636.83 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   208 /   321 | ms/batch 4100.95 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   210 /   321 | ms/batch 3933.73 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   212 /   321 | ms/batch 4469.21 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   214 /   321 | ms/batch 5237.39 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   216 /   321 | ms/batch 4595.59 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   218 /   321 | ms/batch 5320.18 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   220 /   321 | ms/batch 4944.64 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   222 /   321 | ms/batch 4580.19 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   224 /   321 | ms/batch 5375.54 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   226 /   321 | ms/batch 4951.09 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   228 /   321 | ms/batch 4924.45 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   230 /   321 | ms/batch 5577.90 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   232 /   321 | ms/batch 4693.47 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   234 /   321 | ms/batch 5365.04 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   236 /   321 | ms/batch 5361.57 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   238 /   321 | ms/batch 4507.68 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   240 /   321 | ms/batch 3981.59 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   242 /   321 | ms/batch 3539.65 | Loss 00.52 |\n",
      "[Training]| Epochs  19 | Batch   244 /   321 | ms/batch 5597.41 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   246 /   321 | ms/batch 5075.06 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   248 /   321 | ms/batch 5509.05 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   250 /   321 | ms/batch 5064.30 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   252 /   321 | ms/batch 4900.42 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   254 /   321 | ms/batch 4607.58 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   256 /   321 | ms/batch 5479.37 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   258 /   321 | ms/batch 4055.85 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   260 /   321 | ms/batch 4495.42 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   262 /   321 | ms/batch 5892.00 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   264 /   321 | ms/batch 4439.15 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   266 /   321 | ms/batch 4673.22 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   268 /   321 | ms/batch 4865.74 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   270 /   321 | ms/batch 5350.78 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   272 /   321 | ms/batch 5586.36 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   274 /   321 | ms/batch 5601.27 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   276 /   321 | ms/batch 4481.43 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   278 /   321 | ms/batch 5907.50 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   280 /   321 | ms/batch 4602.98 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   282 /   321 | ms/batch 4916.31 | Loss 00.57 |\n",
      "[Training]| Epochs  19 | Batch   284 /   321 | ms/batch 4279.44 | Loss 00.55 |\n",
      "[Training]| Epochs  19 | Batch   286 /   321 | ms/batch 5127.25 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   288 /   321 | ms/batch 5865.32 | Loss 00.60 |\n",
      "[Training]| Epochs  19 | Batch   290 /   321 | ms/batch 5371.56 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   292 /   321 | ms/batch 4062.24 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   294 /   321 | ms/batch 4754.93 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   296 /   321 | ms/batch 3852.64 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   298 /   321 | ms/batch 4943.71 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   300 /   321 | ms/batch 4294.61 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   302 /   321 | ms/batch 4027.54 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   304 /   321 | ms/batch 5575.31 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   306 /   321 | ms/batch 5972.36 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   308 /   321 | ms/batch 4534.95 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  19 | Batch   310 /   321 | ms/batch 4238.38 | Loss 00.56 |\n",
      "[Training]| Epochs  19 | Batch   312 /   321 | ms/batch 6107.45 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   314 /   321 | ms/batch 5842.65 | Loss 00.59 |\n",
      "[Training]| Epochs  19 | Batch   316 /   321 | ms/batch 5902.51 | Loss 00.58 |\n",
      "[Training]| Epochs  19 | Batch   318 /   321 | ms/batch 4784.53 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  19 | Batch   320 /   321 | ms/batch 5003.93 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  19 | Elapsed 1414.48 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.53\n",
      "result= [20, 0.09246902287424494, 0.11872427294427124, 0.17563799815724254, 0.2086066277995299, 0.22959741724717458, 0.11956323280245275, 0.0802418507362657, 0.04068556894850935, 0.029922143426242512, 0.025541140525859495, 0.084440985045578, 0.0788153442477957, 0.058233158530802444, 0.047716564437140736, 0.042607676862090806, 0.08291121249087881, 0.08803762377451, 0.0937447688353211, 0.09590140832457415, 0.09702833829918006, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5846026120362459]\n",
      "[Training]| Epochs  20 | Batch     2 /   321 | ms/batch 6759.86 | Loss 00.85 |\n",
      "[Training]| Epochs  20 | Batch     4 /   321 | ms/batch 4683.30 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch     6 /   321 | ms/batch 6258.71 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch     8 /   321 | ms/batch 5724.30 | Loss 00.61 |\n",
      "[Training]| Epochs  20 | Batch    10 /   321 | ms/batch 5295.95 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    12 /   321 | ms/batch 5638.66 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    14 /   321 | ms/batch 3943.23 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    16 /   321 | ms/batch 4276.18 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    18 /   321 | ms/batch 4518.64 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch    20 /   321 | ms/batch 4564.37 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    22 /   321 | ms/batch 6737.98 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch    24 /   321 | ms/batch 4858.14 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    26 /   321 | ms/batch 4752.47 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    28 /   321 | ms/batch 4906.42 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    30 /   321 | ms/batch 4525.02 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    32 /   321 | ms/batch 4341.79 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    34 /   321 | ms/batch 4292.07 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    36 /   321 | ms/batch 5376.79 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    38 /   321 | ms/batch 5389.57 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    40 /   321 | ms/batch 5227.78 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    42 /   321 | ms/batch 5024.53 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    44 /   321 | ms/batch 5154.12 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    46 /   321 | ms/batch 4392.85 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    48 /   321 | ms/batch 5253.71 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    50 /   321 | ms/batch 4660.21 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    52 /   321 | ms/batch 4863.50 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    54 /   321 | ms/batch 4629.32 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    56 /   321 | ms/batch 5155.50 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    58 /   321 | ms/batch 4697.60 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    60 /   321 | ms/batch 3984.05 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    62 /   321 | ms/batch 4451.12 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    64 /   321 | ms/batch 4592.91 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  20 | Batch    66 /   321 | ms/batch 4357.02 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    68 /   321 | ms/batch 4999.54 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    70 /   321 | ms/batch 4650.26 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    72 /   321 | ms/batch 3746.97 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    74 /   321 | ms/batch 4272.75 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    76 /   321 | ms/batch 4947.49 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    78 /   321 | ms/batch 5526.89 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    80 /   321 | ms/batch 4798.43 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    82 /   321 | ms/batch 5411.62 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    84 /   321 | ms/batch 3994.43 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch    86 /   321 | ms/batch 5924.02 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch    88 /   321 | ms/batch 5549.33 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    90 /   321 | ms/batch 4584.85 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    92 /   321 | ms/batch 5375.24 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch    94 /   321 | ms/batch 4932.00 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch    96 /   321 | ms/batch 4477.57 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch    98 /   321 | ms/batch 5618.58 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   100 /   321 | ms/batch 5841.52 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   102 /   321 | ms/batch 4508.12 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   104 /   321 | ms/batch 5473.38 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   106 /   321 | ms/batch 3908.52 | Loss 00.54 |\n",
      "[Training]| Epochs  20 | Batch   108 /   321 | ms/batch 4313.00 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   110 /   321 | ms/batch 5661.26 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   112 /   321 | ms/batch 3615.09 | Loss 00.54 |\n",
      "[Training]| Epochs  20 | Batch   114 /   321 | ms/batch 4651.54 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   116 /   321 | ms/batch 5394.59 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   118 /   321 | ms/batch 5319.34 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   120 /   321 | ms/batch 4965.44 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   122 /   321 | ms/batch 5307.86 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   124 /   321 | ms/batch 5535.11 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   126 /   321 | ms/batch 4903.64 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   128 /   321 | ms/batch 5230.66 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   130 /   321 | ms/batch 5473.59 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   132 /   321 | ms/batch 3170.71 | Loss 00.54 |\n",
      "[Training]| Epochs  20 | Batch   134 /   321 | ms/batch 5381.23 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   136 /   321 | ms/batch 4613.85 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   138 /   321 | ms/batch 4107.86 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   140 /   321 | ms/batch 4523.35 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   142 /   321 | ms/batch 6221.99 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   144 /   321 | ms/batch 5440.07 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   146 /   321 | ms/batch 4940.87 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   148 /   321 | ms/batch 6343.62 | Loss 00.61 |\n",
      "[Training]| Epochs  20 | Batch   150 /   321 | ms/batch 4138.85 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   152 /   321 | ms/batch 4834.36 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   154 /   321 | ms/batch 4050.29 | Loss 00.54 |\n",
      "[Training]| Epochs  20 | Batch   156 /   321 | ms/batch 5800.84 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   158 /   321 | ms/batch 6018.74 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   160 /   321 | ms/batch 4694.63 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   162 /   321 | ms/batch 4896.42 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   164 /   321 | ms/batch 3318.71 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   166 /   321 | ms/batch 3772.11 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   168 /   321 | ms/batch 6754.39 | Loss 00.61 |\n",
      "[Training]| Epochs  20 | Batch   170 /   321 | ms/batch 5678.87 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   172 /   321 | ms/batch 5001.49 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   174 /   321 | ms/batch 4764.48 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   176 /   321 | ms/batch 5096.79 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   178 /   321 | ms/batch 6046.29 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   180 /   321 | ms/batch 5476.20 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   182 /   321 | ms/batch 5370.14 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   184 /   321 | ms/batch 4745.53 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   186 /   321 | ms/batch 6072.20 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   188 /   321 | ms/batch 4318.54 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   190 /   321 | ms/batch 5022.85 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   192 /   321 | ms/batch 5743.02 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   194 /   321 | ms/batch 6120.38 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   196 /   321 | ms/batch 6307.78 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   198 /   321 | ms/batch 4902.60 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   200 /   321 | ms/batch 4170.77 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   202 /   321 | ms/batch 4628.88 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   204 /   321 | ms/batch 4467.06 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   206 /   321 | ms/batch 4622.81 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   208 /   321 | ms/batch 4068.43 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch   210 /   321 | ms/batch 4112.10 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   212 /   321 | ms/batch 4562.76 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   214 /   321 | ms/batch 5212.71 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   216 /   321 | ms/batch 4765.55 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   218 /   321 | ms/batch 5445.74 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   220 /   321 | ms/batch 5275.66 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   222 /   321 | ms/batch 4343.17 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch   224 /   321 | ms/batch 5352.74 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   226 /   321 | ms/batch 4875.25 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   228 /   321 | ms/batch 5117.90 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   230 /   321 | ms/batch 5796.65 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   232 /   321 | ms/batch 4964.38 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   234 /   321 | ms/batch 5471.54 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   236 /   321 | ms/batch 5220.09 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   238 /   321 | ms/batch 4587.57 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   240 /   321 | ms/batch 3956.66 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   242 /   321 | ms/batch 3282.97 | Loss 00.51 |\n",
      "[Training]| Epochs  20 | Batch   244 /   321 | ms/batch 5605.22 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   246 /   321 | ms/batch 4777.32 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   248 /   321 | ms/batch 5704.73 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   250 /   321 | ms/batch 5251.18 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   252 /   321 | ms/batch 5035.19 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   254 /   321 | ms/batch 4537.11 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   256 /   321 | ms/batch 5358.06 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   258 /   321 | ms/batch 4122.04 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   260 /   321 | ms/batch 4422.74 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   262 /   321 | ms/batch 6020.92 | Loss 00.60 |\n",
      "[Training]| Epochs  20 | Batch   264 /   321 | ms/batch 4507.02 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   266 /   321 | ms/batch 4671.78 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   268 /   321 | ms/batch 5204.94 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   270 /   321 | ms/batch 5176.22 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   272 /   321 | ms/batch 5643.36 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  20 | Batch   274 /   321 | ms/batch 5698.48 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   276 /   321 | ms/batch 4318.37 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   278 /   321 | ms/batch 6034.60 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   280 /   321 | ms/batch 4709.15 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   282 /   321 | ms/batch 4931.25 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   284 /   321 | ms/batch 4396.62 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch   286 /   321 | ms/batch 5236.49 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   288 /   321 | ms/batch 5791.46 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   290 /   321 | ms/batch 5415.35 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   292 /   321 | ms/batch 4197.80 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   294 /   321 | ms/batch 4828.10 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   296 /   321 | ms/batch 4169.18 | Loss 00.57 |\n",
      "[Training]| Epochs  20 | Batch   298 /   321 | ms/batch 5044.22 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   300 /   321 | ms/batch 4288.07 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   302 /   321 | ms/batch 4022.48 | Loss 00.55 |\n",
      "[Training]| Epochs  20 | Batch   304 /   321 | ms/batch 5429.39 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   306 /   321 | ms/batch 6202.04 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   308 /   321 | ms/batch 4403.02 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   310 /   321 | ms/batch 3951.37 | Loss 00.56 |\n",
      "[Training]| Epochs  20 | Batch   312 /   321 | ms/batch 5982.09 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   314 /   321 | ms/batch 5962.19 | Loss 00.59 |\n",
      "[Training]| Epochs  20 | Batch   316 /   321 | ms/batch 5782.44 | Loss 00.58 |\n",
      "[Training]| Epochs  20 | Batch   318 /   321 | ms/batch 4845.97 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  20 | Batch   320 /   321 | ms/batch 4953.46 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  20 | Elapsed 1425.18 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.76\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.87\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.64\n",
      "result= [21, 0.09264867935715851, 0.11979691943245202, 0.1744501555421254, 0.2101154044932531, 0.2282020289360873, 0.11971913885531348, 0.079540362696302, 0.04050371225049323, 0.030109206308913436, 0.025097502829595553, 0.08458129335731988, 0.07853682080192298, 0.057908436514303635, 0.04801758656906877, 0.041903408820612394, 0.08199967105385854, 0.0871406718809085, 0.09273581795589686, 0.09505286216296674, 0.09597911910913937, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5840283808884797]\n",
      "[Training]| Epochs  21 | Batch     2 /   321 | ms/batch 6518.36 | Loss 00.85 |\n",
      "[Training]| Epochs  21 | Batch     4 /   321 | ms/batch 4934.99 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch     6 /   321 | ms/batch 5889.67 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch     8 /   321 | ms/batch 5547.43 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch    10 /   321 | ms/batch 5413.02 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch    12 /   321 | ms/batch 5658.99 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    14 /   321 | ms/batch 3831.16 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    16 /   321 | ms/batch 4321.97 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    18 /   321 | ms/batch 4489.71 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    20 /   321 | ms/batch 4895.52 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    22 /   321 | ms/batch 6461.05 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch    24 /   321 | ms/batch 4909.63 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    26 /   321 | ms/batch 4976.06 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    28 /   321 | ms/batch 4971.00 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  21 | Batch    30 /   321 | ms/batch 4467.18 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    32 /   321 | ms/batch 4440.04 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    34 /   321 | ms/batch 4069.57 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    36 /   321 | ms/batch 5529.08 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    38 /   321 | ms/batch 5519.95 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch    40 /   321 | ms/batch 5569.47 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch    42 /   321 | ms/batch 4928.09 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch    44 /   321 | ms/batch 5059.60 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    46 /   321 | ms/batch 4411.04 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    48 /   321 | ms/batch 5425.47 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    50 /   321 | ms/batch 4451.09 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    52 /   321 | ms/batch 4993.40 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    54 /   321 | ms/batch 4585.42 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    56 /   321 | ms/batch 4736.30 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    58 /   321 | ms/batch 4686.38 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    60 /   321 | ms/batch 4088.61 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    62 /   321 | ms/batch 4380.62 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    64 /   321 | ms/batch 4424.17 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    66 /   321 | ms/batch 4282.00 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    68 /   321 | ms/batch 4368.67 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    70 /   321 | ms/batch 4411.59 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    72 /   321 | ms/batch 3730.37 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    74 /   321 | ms/batch 4391.95 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    76 /   321 | ms/batch 4927.97 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    78 /   321 | ms/batch 5403.42 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    80 /   321 | ms/batch 4822.66 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    82 /   321 | ms/batch 5255.89 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    84 /   321 | ms/batch 4137.52 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch    86 /   321 | ms/batch 6034.75 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch    88 /   321 | ms/batch 5699.72 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    90 /   321 | ms/batch 4344.71 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    92 /   321 | ms/batch 5396.58 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch    94 /   321 | ms/batch 4882.17 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch    96 /   321 | ms/batch 4365.35 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch    98 /   321 | ms/batch 5624.67 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   100 /   321 | ms/batch 5395.48 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   102 /   321 | ms/batch 4318.23 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   104 /   321 | ms/batch 5683.54 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   106 /   321 | ms/batch 3990.40 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   108 /   321 | ms/batch 4225.65 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   110 /   321 | ms/batch 5409.72 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   112 /   321 | ms/batch 3893.94 | Loss 00.53 |\n",
      "[Training]| Epochs  21 | Batch   114 /   321 | ms/batch 4527.42 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   116 /   321 | ms/batch 5130.52 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   118 /   321 | ms/batch 4964.97 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   120 /   321 | ms/batch 4878.23 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   122 /   321 | ms/batch 5204.86 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   124 /   321 | ms/batch 5181.93 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   126 /   321 | ms/batch 4987.16 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   128 /   321 | ms/batch 5325.95 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   130 /   321 | ms/batch 5561.85 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   132 /   321 | ms/batch 3236.61 | Loss 00.54 |\n",
      "[Training]| Epochs  21 | Batch   134 /   321 | ms/batch 5523.22 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   136 /   321 | ms/batch 4642.08 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   138 /   321 | ms/batch 4018.95 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   140 /   321 | ms/batch 4646.48 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   142 /   321 | ms/batch 6035.97 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   144 /   321 | ms/batch 5344.39 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   146 /   321 | ms/batch 5125.77 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   148 /   321 | ms/batch 6294.15 | Loss 00.61 |\n",
      "[Training]| Epochs  21 | Batch   150 /   321 | ms/batch 4250.99 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   152 /   321 | ms/batch 4843.58 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   154 /   321 | ms/batch 3929.24 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   156 /   321 | ms/batch 5561.22 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   158 /   321 | ms/batch 6015.41 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   160 /   321 | ms/batch 4589.52 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   162 /   321 | ms/batch 5002.51 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   164 /   321 | ms/batch 4015.34 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   166 /   321 | ms/batch 4969.15 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   168 /   321 | ms/batch 6709.28 | Loss 00.61 |\n",
      "[Training]| Epochs  21 | Batch   170 /   321 | ms/batch 5504.27 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   172 /   321 | ms/batch 4877.98 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   174 /   321 | ms/batch 4579.53 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   176 /   321 | ms/batch 5286.31 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   178 /   321 | ms/batch 6028.40 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   180 /   321 | ms/batch 5210.72 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   182 /   321 | ms/batch 5471.84 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   184 /   321 | ms/batch 4923.14 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   186 /   321 | ms/batch 5857.72 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   188 /   321 | ms/batch 4377.22 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   190 /   321 | ms/batch 5085.98 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   192 /   321 | ms/batch 5539.97 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   194 /   321 | ms/batch 6199.74 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   196 /   321 | ms/batch 6155.90 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   198 /   321 | ms/batch 5114.82 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   200 /   321 | ms/batch 4183.64 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   202 /   321 | ms/batch 4845.85 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   204 /   321 | ms/batch 4535.40 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   206 /   321 | ms/batch 4797.20 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   208 /   321 | ms/batch 4166.28 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   210 /   321 | ms/batch 4044.81 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   212 /   321 | ms/batch 4447.46 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   214 /   321 | ms/batch 5241.70 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   216 /   321 | ms/batch 4381.26 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   218 /   321 | ms/batch 5408.72 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   220 /   321 | ms/batch 5043.58 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   222 /   321 | ms/batch 4324.20 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   224 /   321 | ms/batch 5279.33 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   226 /   321 | ms/batch 5246.39 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   228 /   321 | ms/batch 4950.97 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   230 /   321 | ms/batch 5887.72 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   232 /   321 | ms/batch 4990.61 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   234 /   321 | ms/batch 5134.92 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   236 /   321 | ms/batch 5200.63 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  21 | Batch   238 /   321 | ms/batch 4646.51 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   240 /   321 | ms/batch 4031.78 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   242 /   321 | ms/batch 3387.85 | Loss 00.52 |\n",
      "[Training]| Epochs  21 | Batch   244 /   321 | ms/batch 5329.17 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   246 /   321 | ms/batch 4920.56 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   248 /   321 | ms/batch 5281.60 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   250 /   321 | ms/batch 5144.44 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   252 /   321 | ms/batch 4994.46 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   254 /   321 | ms/batch 4585.70 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   256 /   321 | ms/batch 5489.26 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   258 /   321 | ms/batch 4077.44 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   260 /   321 | ms/batch 4557.41 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   262 /   321 | ms/batch 5946.46 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   264 /   321 | ms/batch 4423.68 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   266 /   321 | ms/batch 4376.45 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   268 /   321 | ms/batch 4971.74 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   270 /   321 | ms/batch 5156.94 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   272 /   321 | ms/batch 5809.90 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   274 /   321 | ms/batch 5835.55 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   276 /   321 | ms/batch 4373.07 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   278 /   321 | ms/batch 5927.53 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   280 /   321 | ms/batch 4804.02 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   282 /   321 | ms/batch 4872.72 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   284 /   321 | ms/batch 4257.85 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   286 /   321 | ms/batch 5174.06 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   288 /   321 | ms/batch 5782.04 | Loss 00.60 |\n",
      "[Training]| Epochs  21 | Batch   290 /   321 | ms/batch 5043.43 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   292 /   321 | ms/batch 4224.76 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   294 /   321 | ms/batch 4739.05 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   296 /   321 | ms/batch 4133.93 | Loss 00.56 |\n",
      "[Training]| Epochs  21 | Batch   298 /   321 | ms/batch 5113.80 | Loss 00.58 |\n",
      "[Training]| Epochs  21 | Batch   300 /   321 | ms/batch 4316.12 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   302 /   321 | ms/batch 4060.75 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   304 /   321 | ms/batch 5774.01 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   306 /   321 | ms/batch 5867.35 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   308 /   321 | ms/batch 4115.60 | Loss 00.57 |\n",
      "[Training]| Epochs  21 | Batch   310 /   321 | ms/batch 4161.96 | Loss 00.55 |\n",
      "[Training]| Epochs  21 | Batch   312 /   321 | ms/batch 5940.87 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   314 /   321 | ms/batch 5556.45 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   316 /   321 | ms/batch 5715.66 | Loss 00.59 |\n",
      "[Training]| Epochs  21 | Batch   318 /   321 | ms/batch 4754.29 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  21 | Batch   320 /   321 | ms/batch 4878.33 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  21 | Elapsed 1421.52 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.87\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.53\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.45\n",
      "result= [22, 0.0925223097054176, 0.11779560377517902, 0.17431945087197973, 0.206919169853371, 0.2294198539418721, 0.11964121556151963, 0.07782561613634426, 0.0403997877660619, 0.02948568616169835, 0.025253379149819757, 0.0844631001806222, 0.07709474630497065, 0.05780654276894668, 0.04704603996484984, 0.04215487259398748, 0.08299160628512049, 0.08782804853973195, 0.09367593402509009, 0.09579532664263402, 0.09696690077510163, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5834311887069985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  22 | Batch     2 /   321 | ms/batch 6547.58 | Loss 00.85 |\n",
      "[Training]| Epochs  22 | Batch     4 /   321 | ms/batch 4619.44 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch     6 /   321 | ms/batch 6220.50 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch     8 /   321 | ms/batch 5627.01 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch    10 /   321 | ms/batch 5398.04 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    12 /   321 | ms/batch 5160.15 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    14 /   321 | ms/batch 4127.49 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    16 /   321 | ms/batch 4254.53 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    18 /   321 | ms/batch 4425.31 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    20 /   321 | ms/batch 4875.11 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    22 /   321 | ms/batch 6177.47 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch    24 /   321 | ms/batch 4628.82 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    26 /   321 | ms/batch 4723.18 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    28 /   321 | ms/batch 4936.08 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    30 /   321 | ms/batch 4586.30 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    32 /   321 | ms/batch 4497.92 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    34 /   321 | ms/batch 4170.97 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    36 /   321 | ms/batch 5481.94 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    38 /   321 | ms/batch 5436.85 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    40 /   321 | ms/batch 5173.02 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    42 /   321 | ms/batch 5224.60 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    44 /   321 | ms/batch 4947.05 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    46 /   321 | ms/batch 4429.96 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    48 /   321 | ms/batch 5262.54 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    50 /   321 | ms/batch 4228.45 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    52 /   321 | ms/batch 5068.29 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    54 /   321 | ms/batch 4570.29 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    56 /   321 | ms/batch 4905.65 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    58 /   321 | ms/batch 4783.09 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    60 /   321 | ms/batch 4284.93 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    62 /   321 | ms/batch 4299.22 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    64 /   321 | ms/batch 4466.22 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    66 /   321 | ms/batch 4432.41 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    68 /   321 | ms/batch 4612.94 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    70 /   321 | ms/batch 4557.03 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    72 /   321 | ms/batch 3725.91 | Loss 00.55 |\n",
      "[Training]| Epochs  22 | Batch    74 /   321 | ms/batch 4282.95 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    76 /   321 | ms/batch 4692.58 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    78 /   321 | ms/batch 5285.30 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    80 /   321 | ms/batch 4943.08 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    82 /   321 | ms/batch 5295.12 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    84 /   321 | ms/batch 4071.06 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    86 /   321 | ms/batch 5862.21 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch    88 /   321 | ms/batch 5791.12 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    90 /   321 | ms/batch 4405.00 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    92 /   321 | ms/batch 5455.00 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch    94 /   321 | ms/batch 4978.48 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch    96 /   321 | ms/batch 4319.59 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch    98 /   321 | ms/batch 5468.07 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   100 /   321 | ms/batch 5513.43 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   102 /   321 | ms/batch 4251.41 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   104 /   321 | ms/batch 5461.82 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   106 /   321 | ms/batch 3873.09 | Loss 00.54 |\n",
      "[Training]| Epochs  22 | Batch   108 /   321 | ms/batch 4051.90 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   110 /   321 | ms/batch 5274.84 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   112 /   321 | ms/batch 4068.34 | Loss 00.54 |\n",
      "[Training]| Epochs  22 | Batch   114 /   321 | ms/batch 4552.80 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   116 /   321 | ms/batch 5349.90 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   118 /   321 | ms/batch 5231.22 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   120 /   321 | ms/batch 4958.49 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   122 /   321 | ms/batch 4930.33 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   124 /   321 | ms/batch 5275.55 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   126 /   321 | ms/batch 4835.23 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   128 /   321 | ms/batch 5035.77 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   130 /   321 | ms/batch 5388.62 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   132 /   321 | ms/batch 3220.21 | Loss 00.54 |\n",
      "[Training]| Epochs  22 | Batch   134 /   321 | ms/batch 5493.56 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   136 /   321 | ms/batch 4583.71 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   138 /   321 | ms/batch 4022.70 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   140 /   321 | ms/batch 4623.72 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   142 /   321 | ms/batch 5994.35 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   144 /   321 | ms/batch 5443.32 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   146 /   321 | ms/batch 4784.39 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   148 /   321 | ms/batch 6244.53 | Loss 00.61 |\n",
      "[Training]| Epochs  22 | Batch   150 /   321 | ms/batch 4335.06 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   152 /   321 | ms/batch 4849.13 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   154 /   321 | ms/batch 3943.76 | Loss 00.54 |\n",
      "[Training]| Epochs  22 | Batch   156 /   321 | ms/batch 5619.81 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   158 /   321 | ms/batch 6063.20 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   160 /   321 | ms/batch 4404.07 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   162 /   321 | ms/batch 4865.90 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   164 /   321 | ms/batch 3823.01 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   166 /   321 | ms/batch 4880.91 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   168 /   321 | ms/batch 6889.01 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   170 /   321 | ms/batch 5554.69 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   172 /   321 | ms/batch 4920.53 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   174 /   321 | ms/batch 4598.17 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   176 /   321 | ms/batch 5094.51 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   178 /   321 | ms/batch 6223.09 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   180 /   321 | ms/batch 5230.29 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   182 /   321 | ms/batch 5371.73 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   184 /   321 | ms/batch 4742.53 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   186 /   321 | ms/batch 6106.19 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   188 /   321 | ms/batch 4316.24 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   190 /   321 | ms/batch 4951.98 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   192 /   321 | ms/batch 5450.48 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   194 /   321 | ms/batch 6635.30 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   196 /   321 | ms/batch 6178.99 | Loss 00.61 |\n",
      "[Training]| Epochs  22 | Batch   198 /   321 | ms/batch 5073.19 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   200 /   321 | ms/batch 4353.07 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   202 /   321 | ms/batch 4827.53 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   204 /   321 | ms/batch 4573.02 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   206 /   321 | ms/batch 4637.04 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   208 /   321 | ms/batch 4083.96 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  22 | Batch   210 /   321 | ms/batch 3962.61 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   212 /   321 | ms/batch 4686.45 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   214 /   321 | ms/batch 5197.24 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   216 /   321 | ms/batch 4783.20 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   218 /   321 | ms/batch 5415.94 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   220 /   321 | ms/batch 5005.57 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   222 /   321 | ms/batch 4280.03 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   224 /   321 | ms/batch 5413.41 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   226 /   321 | ms/batch 4893.19 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   228 /   321 | ms/batch 4967.99 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   230 /   321 | ms/batch 5731.02 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   232 /   321 | ms/batch 4711.38 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   234 /   321 | ms/batch 5389.09 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   236 /   321 | ms/batch 5178.19 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   238 /   321 | ms/batch 4543.43 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   240 /   321 | ms/batch 3758.26 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   242 /   321 | ms/batch 3580.40 | Loss 00.52 |\n",
      "[Training]| Epochs  22 | Batch   244 /   321 | ms/batch 5514.46 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   246 /   321 | ms/batch 4917.16 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   248 /   321 | ms/batch 5399.42 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   250 /   321 | ms/batch 5314.18 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   252 /   321 | ms/batch 4913.60 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   254 /   321 | ms/batch 4535.71 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   256 /   321 | ms/batch 5468.94 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   258 /   321 | ms/batch 4159.16 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   260 /   321 | ms/batch 4587.97 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   262 /   321 | ms/batch 6235.42 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   264 /   321 | ms/batch 4338.27 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   266 /   321 | ms/batch 4763.02 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   268 /   321 | ms/batch 4753.35 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   270 /   321 | ms/batch 5290.82 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   272 /   321 | ms/batch 5665.16 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   274 /   321 | ms/batch 5431.01 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   276 /   321 | ms/batch 4526.18 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   278 /   321 | ms/batch 5743.83 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   280 /   321 | ms/batch 4657.63 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   282 /   321 | ms/batch 4720.77 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   284 /   321 | ms/batch 4411.63 | Loss 00.55 |\n",
      "[Training]| Epochs  22 | Batch   286 /   321 | ms/batch 5165.24 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   288 /   321 | ms/batch 5572.80 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   290 /   321 | ms/batch 5264.58 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   292 /   321 | ms/batch 4135.33 | Loss 00.57 |\n",
      "[Training]| Epochs  22 | Batch   294 /   321 | ms/batch 4826.03 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   296 /   321 | ms/batch 4109.39 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   298 /   321 | ms/batch 5238.32 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   300 /   321 | ms/batch 4259.70 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   302 /   321 | ms/batch 4031.25 | Loss 00.55 |\n",
      "[Training]| Epochs  22 | Batch   304 /   321 | ms/batch 5550.61 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   306 /   321 | ms/batch 6075.62 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   308 /   321 | ms/batch 4243.56 | Loss 00.56 |\n",
      "[Training]| Epochs  22 | Batch   310 /   321 | ms/batch 4464.99 | Loss 00.55 |\n",
      "[Training]| Epochs  22 | Batch   312 /   321 | ms/batch 5903.01 | Loss 00.58 |\n",
      "[Training]| Epochs  22 | Batch   314 /   321 | ms/batch 5896.34 | Loss 00.60 |\n",
      "[Training]| Epochs  22 | Batch   316 /   321 | ms/batch 5674.02 | Loss 00.59 |\n",
      "[Training]| Epochs  22 | Batch   318 /   321 | ms/batch 4677.71 | Loss 00.57 |\n",
      "32\n",
      "[Training]| Epochs  22 | Batch   320 /   321 | ms/batch 5212.98 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  22 | Elapsed 1430.99 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.81\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.53\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.37\n",
      "result= [23, 0.09196983569316909, 0.11945203274186476, 0.1744319234894108, 0.20799789369245664, 0.2302669486494057, 0.11925145637589511, 0.0791506213502594, 0.040282873092734615, 0.02949348503225777, 0.025451214166702187, 0.08404046264553293, 0.07828307653532675, 0.05766360014560905, 0.04708410071286018, 0.04247838449217989, 0.08381421686043428, 0.08900281976340796, 0.09462539108766974, 0.09678424523823219, 0.0979814524751444, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5853043176509716]\n",
      "[Training]| Epochs  23 | Batch     2 /   321 | ms/batch 6915.95 | Loss 00.85 |\n",
      "[Training]| Epochs  23 | Batch     4 /   321 | ms/batch 4847.60 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch     6 /   321 | ms/batch 6150.43 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch     8 /   321 | ms/batch 5677.36 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch    10 /   321 | ms/batch 5591.30 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch    12 /   321 | ms/batch 5574.41 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    14 /   321 | ms/batch 4317.53 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    16 /   321 | ms/batch 4403.68 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    18 /   321 | ms/batch 4526.45 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    20 /   321 | ms/batch 4788.57 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    22 /   321 | ms/batch 6189.59 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch    24 /   321 | ms/batch 4949.64 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    26 /   321 | ms/batch 4719.94 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    28 /   321 | ms/batch 5124.56 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    30 /   321 | ms/batch 4651.33 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    32 /   321 | ms/batch 4405.24 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    34 /   321 | ms/batch 4190.32 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    36 /   321 | ms/batch 5720.49 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch    38 /   321 | ms/batch 5594.49 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    40 /   321 | ms/batch 5369.87 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch    42 /   321 | ms/batch 4959.98 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch    44 /   321 | ms/batch 5154.29 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    46 /   321 | ms/batch 4465.42 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    48 /   321 | ms/batch 5216.32 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    50 /   321 | ms/batch 4318.92 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    52 /   321 | ms/batch 4795.30 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    54 /   321 | ms/batch 4681.80 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch    56 /   321 | ms/batch 4857.58 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    58 /   321 | ms/batch 4872.34 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    60 /   321 | ms/batch 4408.23 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    62 /   321 | ms/batch 4371.16 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    64 /   321 | ms/batch 4470.57 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    66 /   321 | ms/batch 4406.48 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    68 /   321 | ms/batch 4824.60 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    70 /   321 | ms/batch 4571.65 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    72 /   321 | ms/batch 3591.25 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    74 /   321 | ms/batch 4271.11 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    76 /   321 | ms/batch 4706.73 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    78 /   321 | ms/batch 5435.36 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    80 /   321 | ms/batch 4994.11 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    82 /   321 | ms/batch 5422.11 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    84 /   321 | ms/batch 3782.62 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch    86 /   321 | ms/batch 5732.12 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch    88 /   321 | ms/batch 5925.00 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    90 /   321 | ms/batch 4463.09 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    92 /   321 | ms/batch 5432.14 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch    94 /   321 | ms/batch 4742.12 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch    96 /   321 | ms/batch 4415.75 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch    98 /   321 | ms/batch 5692.23 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   100 /   321 | ms/batch 5601.33 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   102 /   321 | ms/batch 4542.49 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   104 /   321 | ms/batch 5464.04 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   106 /   321 | ms/batch 3731.06 | Loss 00.54 |\n",
      "[Training]| Epochs  23 | Batch   108 /   321 | ms/batch 4087.03 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   110 /   321 | ms/batch 5473.88 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   112 /   321 | ms/batch 3801.73 | Loss 00.54 |\n",
      "[Training]| Epochs  23 | Batch   114 /   321 | ms/batch 4682.96 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   116 /   321 | ms/batch 5411.15 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   118 /   321 | ms/batch 5205.95 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   120 /   321 | ms/batch 4985.44 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   122 /   321 | ms/batch 5197.04 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   124 /   321 | ms/batch 5463.06 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   126 /   321 | ms/batch 5009.08 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   128 /   321 | ms/batch 5232.71 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   130 /   321 | ms/batch 5729.26 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   132 /   321 | ms/batch 3326.33 | Loss 00.53 |\n",
      "[Training]| Epochs  23 | Batch   134 /   321 | ms/batch 5472.73 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   136 /   321 | ms/batch 4663.92 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   138 /   321 | ms/batch 4206.58 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   140 /   321 | ms/batch 4798.92 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   142 /   321 | ms/batch 5989.18 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   144 /   321 | ms/batch 5544.88 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   146 /   321 | ms/batch 4805.65 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   148 /   321 | ms/batch 6164.75 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   150 /   321 | ms/batch 4154.04 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   152 /   321 | ms/batch 4852.91 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   154 /   321 | ms/batch 3807.43 | Loss 00.54 |\n",
      "[Training]| Epochs  23 | Batch   156 /   321 | ms/batch 5623.09 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   158 /   321 | ms/batch 5906.72 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   160 /   321 | ms/batch 4173.29 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   162 /   321 | ms/batch 4954.98 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   164 /   321 | ms/batch 3819.30 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   166 /   321 | ms/batch 4840.97 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   168 /   321 | ms/batch 6524.40 | Loss 00.61 |\n",
      "[Training]| Epochs  23 | Batch   170 /   321 | ms/batch 5431.09 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   172 /   321 | ms/batch 5035.20 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  23 | Batch   174 /   321 | ms/batch 4537.04 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   176 /   321 | ms/batch 5032.64 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   178 /   321 | ms/batch 6018.26 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   180 /   321 | ms/batch 5133.58 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   182 /   321 | ms/batch 5415.81 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   184 /   321 | ms/batch 4961.03 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   186 /   321 | ms/batch 5981.08 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   188 /   321 | ms/batch 4455.61 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   190 /   321 | ms/batch 4684.11 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   192 /   321 | ms/batch 5387.27 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   194 /   321 | ms/batch 6168.47 | Loss 00.61 |\n",
      "[Training]| Epochs  23 | Batch   196 /   321 | ms/batch 6313.77 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   198 /   321 | ms/batch 4947.96 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   200 /   321 | ms/batch 4413.33 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   202 /   321 | ms/batch 4562.04 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   204 /   321 | ms/batch 4455.44 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   206 /   321 | ms/batch 4587.80 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   208 /   321 | ms/batch 4125.66 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch   210 /   321 | ms/batch 4142.17 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch   212 /   321 | ms/batch 4500.71 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   214 /   321 | ms/batch 5306.72 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   216 /   321 | ms/batch 4481.14 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   218 /   321 | ms/batch 5375.21 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   220 /   321 | ms/batch 5094.74 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   222 /   321 | ms/batch 4540.32 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   224 /   321 | ms/batch 5241.34 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   226 /   321 | ms/batch 5074.13 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   228 /   321 | ms/batch 4859.00 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   230 /   321 | ms/batch 5830.00 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   232 /   321 | ms/batch 4693.61 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   234 /   321 | ms/batch 5300.97 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   236 /   321 | ms/batch 5338.03 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   238 /   321 | ms/batch 4618.04 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch   240 /   321 | ms/batch 3862.20 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   242 /   321 | ms/batch 3371.99 | Loss 00.52 |\n",
      "[Training]| Epochs  23 | Batch   244 /   321 | ms/batch 5651.66 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   246 /   321 | ms/batch 5002.70 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   248 /   321 | ms/batch 5362.32 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   250 /   321 | ms/batch 5512.59 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   252 /   321 | ms/batch 4883.63 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   254 /   321 | ms/batch 4657.55 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   256 /   321 | ms/batch 5237.73 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   258 /   321 | ms/batch 3962.74 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   260 /   321 | ms/batch 4651.23 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   262 /   321 | ms/batch 5988.05 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   264 /   321 | ms/batch 4537.12 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   266 /   321 | ms/batch 4586.12 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   268 /   321 | ms/batch 4911.18 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   270 /   321 | ms/batch 5029.89 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   272 /   321 | ms/batch 5757.11 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   274 /   321 | ms/batch 5601.16 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   276 /   321 | ms/batch 4382.10 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   278 /   321 | ms/batch 5771.34 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   280 /   321 | ms/batch 4795.85 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   282 /   321 | ms/batch 4703.77 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   284 /   321 | ms/batch 4390.44 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch   286 /   321 | ms/batch 5203.27 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   288 /   321 | ms/batch 6091.44 | Loss 00.60 |\n",
      "[Training]| Epochs  23 | Batch   290 /   321 | ms/batch 5239.42 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   292 /   321 | ms/batch 4237.59 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   294 /   321 | ms/batch 4903.30 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   296 /   321 | ms/batch 4074.63 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   298 /   321 | ms/batch 5013.10 | Loss 00.58 |\n",
      "[Training]| Epochs  23 | Batch   300 /   321 | ms/batch 4243.50 | Loss 00.57 |\n",
      "[Training]| Epochs  23 | Batch   302 /   321 | ms/batch 4080.65 | Loss 00.55 |\n",
      "[Training]| Epochs  23 | Batch   304 /   321 | ms/batch 5953.70 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   306 /   321 | ms/batch 5966.24 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   308 /   321 | ms/batch 4306.52 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   310 /   321 | ms/batch 4334.78 | Loss 00.56 |\n",
      "[Training]| Epochs  23 | Batch   312 /   321 | ms/batch 5796.89 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   314 /   321 | ms/batch 5845.00 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   316 /   321 | ms/batch 5802.97 | Loss 00.59 |\n",
      "[Training]| Epochs  23 | Batch   318 /   321 | ms/batch 4800.19 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs  23 | Batch   320 /   321 | ms/batch 5479.94 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  23 | Elapsed 1435.82 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.84\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.62\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.58\n",
      "result= [24, 0.09231483536777511, 0.11794156723438717, 0.17618074959734875, 0.20678860789988066, 0.2320238382483681, 0.11956329226772579, 0.0798131596363808, 0.040893417917372006, 0.029727317352175993, 0.025619072739444297, 0.0842626130125621, 0.0783620107387623, 0.058508958467169844, 0.047405302385194914, 0.042757853435893965, 0.08363236026186568, 0.08867975784536808, 0.09454392070667157, 0.09656442052848777, 0.09785159853223889, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5840480327606201]\n",
      "[Training]| Epochs  24 | Batch     2 /   321 | ms/batch 6779.92 | Loss 00.85 |\n",
      "[Training]| Epochs  24 | Batch     4 /   321 | ms/batch 4795.58 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch     6 /   321 | ms/batch 6262.94 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch     8 /   321 | ms/batch 5674.81 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch    10 /   321 | ms/batch 5376.77 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    12 /   321 | ms/batch 5543.38 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    14 /   321 | ms/batch 4005.18 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    16 /   321 | ms/batch 4253.84 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    18 /   321 | ms/batch 4493.33 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    20 /   321 | ms/batch 4876.67 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    22 /   321 | ms/batch 6444.30 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch    24 /   321 | ms/batch 5134.02 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    26 /   321 | ms/batch 4801.91 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    28 /   321 | ms/batch 4925.40 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    30 /   321 | ms/batch 4632.79 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    32 /   321 | ms/batch 4322.39 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    34 /   321 | ms/batch 4172.24 | Loss 00.55 |\n",
      "[Training]| Epochs  24 | Batch    36 /   321 | ms/batch 5502.80 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    38 /   321 | ms/batch 5446.75 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    40 /   321 | ms/batch 5327.93 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    42 /   321 | ms/batch 5115.99 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    44 /   321 | ms/batch 5185.05 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    46 /   321 | ms/batch 4402.79 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    48 /   321 | ms/batch 5434.81 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    50 /   321 | ms/batch 4571.87 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    52 /   321 | ms/batch 5073.24 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    54 /   321 | ms/batch 4716.00 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    56 /   321 | ms/batch 4729.06 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    58 /   321 | ms/batch 4865.30 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    60 /   321 | ms/batch 4457.33 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    62 /   321 | ms/batch 4651.62 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    64 /   321 | ms/batch 4477.61 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    66 /   321 | ms/batch 4120.92 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    68 /   321 | ms/batch 4476.67 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    70 /   321 | ms/batch 4589.96 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    72 /   321 | ms/batch 3744.13 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    74 /   321 | ms/batch 4166.93 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    76 /   321 | ms/batch 4471.41 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    78 /   321 | ms/batch 5550.97 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    80 /   321 | ms/batch 5066.51 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    82 /   321 | ms/batch 5396.55 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    84 /   321 | ms/batch 3952.93 | Loss 00.55 |\n",
      "[Training]| Epochs  24 | Batch    86 /   321 | ms/batch 5887.92 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    88 /   321 | ms/batch 5831.23 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    90 /   321 | ms/batch 4520.34 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch    92 /   321 | ms/batch 5441.34 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch    94 /   321 | ms/batch 4944.61 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch    96 /   321 | ms/batch 4371.21 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch    98 /   321 | ms/batch 5696.49 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   100 /   321 | ms/batch 5608.91 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   102 /   321 | ms/batch 4498.59 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   104 /   321 | ms/batch 5370.55 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   106 /   321 | ms/batch 3781.01 | Loss 00.54 |\n",
      "[Training]| Epochs  24 | Batch   108 /   321 | ms/batch 4171.89 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   110 /   321 | ms/batch 5659.85 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   112 /   321 | ms/batch 3687.00 | Loss 00.54 |\n",
      "[Training]| Epochs  24 | Batch   114 /   321 | ms/batch 4651.92 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   116 /   321 | ms/batch 5345.19 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   118 /   321 | ms/batch 5181.06 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   120 /   321 | ms/batch 4876.66 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   122 /   321 | ms/batch 5158.10 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   124 /   321 | ms/batch 5278.36 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   126 /   321 | ms/batch 4957.74 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   128 /   321 | ms/batch 5457.82 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   130 /   321 | ms/batch 5685.00 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   132 /   321 | ms/batch 3398.28 | Loss 00.54 |\n",
      "[Training]| Epochs  24 | Batch   134 /   321 | ms/batch 5467.64 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   136 /   321 | ms/batch 4686.74 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  24 | Batch   138 /   321 | ms/batch 4119.10 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   140 /   321 | ms/batch 4705.66 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   142 /   321 | ms/batch 6069.13 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   144 /   321 | ms/batch 5619.35 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   146 /   321 | ms/batch 4713.08 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   148 /   321 | ms/batch 6301.01 | Loss 00.61 |\n",
      "[Training]| Epochs  24 | Batch   150 /   321 | ms/batch 4315.07 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   152 /   321 | ms/batch 5037.03 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   154 /   321 | ms/batch 3858.75 | Loss 00.54 |\n",
      "[Training]| Epochs  24 | Batch   156 /   321 | ms/batch 5783.60 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   158 /   321 | ms/batch 6144.22 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   160 /   321 | ms/batch 4453.25 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   162 /   321 | ms/batch 4908.06 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   164 /   321 | ms/batch 4003.94 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   166 /   321 | ms/batch 4813.96 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   168 /   321 | ms/batch 6687.85 | Loss 00.61 |\n",
      "[Training]| Epochs  24 | Batch   170 /   321 | ms/batch 5682.02 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   172 /   321 | ms/batch 4912.74 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   174 /   321 | ms/batch 4643.93 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   176 /   321 | ms/batch 5005.86 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   178 /   321 | ms/batch 6104.90 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   180 /   321 | ms/batch 5250.66 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   182 /   321 | ms/batch 5392.73 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   184 /   321 | ms/batch 4807.14 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   186 /   321 | ms/batch 5973.18 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   188 /   321 | ms/batch 4353.53 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   190 /   321 | ms/batch 4676.09 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   192 /   321 | ms/batch 5584.71 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   194 /   321 | ms/batch 6284.29 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   196 /   321 | ms/batch 6204.77 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   198 /   321 | ms/batch 5190.47 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   200 /   321 | ms/batch 4152.13 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   202 /   321 | ms/batch 4571.24 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   204 /   321 | ms/batch 4333.30 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   206 /   321 | ms/batch 4637.94 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   208 /   321 | ms/batch 4248.20 | Loss 00.55 |\n",
      "[Training]| Epochs  24 | Batch   210 /   321 | ms/batch 4078.33 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   212 /   321 | ms/batch 4439.74 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   214 /   321 | ms/batch 4897.45 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   216 /   321 | ms/batch 4478.46 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   218 /   321 | ms/batch 5365.44 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   220 /   321 | ms/batch 5320.15 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   222 /   321 | ms/batch 4302.92 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   224 /   321 | ms/batch 5333.39 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   226 /   321 | ms/batch 4964.60 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   228 /   321 | ms/batch 4919.42 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   230 /   321 | ms/batch 5740.44 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   232 /   321 | ms/batch 4826.46 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   234 /   321 | ms/batch 5346.35 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   236 /   321 | ms/batch 5292.87 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   238 /   321 | ms/batch 4841.43 | Loss 00.55 |\n",
      "[Training]| Epochs  24 | Batch   240 /   321 | ms/batch 4078.31 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   242 /   321 | ms/batch 3416.91 | Loss 00.53 |\n",
      "[Training]| Epochs  24 | Batch   244 /   321 | ms/batch 5501.17 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   246 /   321 | ms/batch 4909.55 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   248 /   321 | ms/batch 5499.99 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   250 /   321 | ms/batch 5352.77 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   252 /   321 | ms/batch 5037.41 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   254 /   321 | ms/batch 4930.68 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   256 /   321 | ms/batch 5300.01 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   258 /   321 | ms/batch 3948.01 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   260 /   321 | ms/batch 4566.65 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   262 /   321 | ms/batch 6221.49 | Loss 00.60 |\n",
      "[Training]| Epochs  24 | Batch   264 /   321 | ms/batch 4682.85 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   266 /   321 | ms/batch 4457.97 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   268 /   321 | ms/batch 4921.63 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   270 /   321 | ms/batch 5193.34 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   272 /   321 | ms/batch 5777.43 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   274 /   321 | ms/batch 5357.45 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   276 /   321 | ms/batch 4489.93 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   278 /   321 | ms/batch 5904.88 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   280 /   321 | ms/batch 5049.42 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   282 /   321 | ms/batch 4770.79 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   284 /   321 | ms/batch 4195.05 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   286 /   321 | ms/batch 5279.17 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   288 /   321 | ms/batch 5638.38 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   290 /   321 | ms/batch 5382.39 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   292 /   321 | ms/batch 4037.70 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   294 /   321 | ms/batch 4484.16 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   296 /   321 | ms/batch 4018.87 | Loss 00.56 |\n",
      "[Training]| Epochs  24 | Batch   298 /   321 | ms/batch 5089.68 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   300 /   321 | ms/batch 4662.57 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   302 /   321 | ms/batch 4093.18 | Loss 00.54 |\n",
      "[Training]| Epochs  24 | Batch   304 /   321 | ms/batch 5834.74 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   306 /   321 | ms/batch 6144.73 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   308 /   321 | ms/batch 4322.37 | Loss 00.57 |\n",
      "[Training]| Epochs  24 | Batch   310 /   321 | ms/batch 4052.24 | Loss 00.55 |\n",
      "[Training]| Epochs  24 | Batch   312 /   321 | ms/batch 5831.08 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   314 /   321 | ms/batch 5754.01 | Loss 00.59 |\n",
      "[Training]| Epochs  24 | Batch   316 /   321 | ms/batch 5868.18 | Loss 00.58 |\n",
      "[Training]| Epochs  24 | Batch   318 /   321 | ms/batch 4796.04 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  24 | Batch   320 /   321 | ms/batch 4879.01 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  24 | Elapsed 1431.96 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.32\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.48\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.84\n",
      "result= [25, 0.09305353677560832, 0.11881129442484047, 0.1738552530575616, 0.20810495497004092, 0.23080751176746395, 0.11948532140171352, 0.07961833356231428, 0.0404647387105417, 0.029875400748368083, 0.025403255423994058, 0.08465203324613028, 0.07824784336104955, 0.05789101318930242, 0.047664243916655, 0.04241238695889456, 0.08318947297713297, 0.08815422581998009, 0.0937891781499968, 0.09601318152764389, 0.0971596262394868, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.583422957379141]\n",
      "[Training]| Epochs  25 | Batch     2 /   321 | ms/batch 6819.08 | Loss 00.85 |\n",
      "[Training]| Epochs  25 | Batch     4 /   321 | ms/batch 5020.08 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch     6 /   321 | ms/batch 6049.53 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch     8 /   321 | ms/batch 5759.17 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch    10 /   321 | ms/batch 5319.57 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    12 /   321 | ms/batch 5636.77 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    14 /   321 | ms/batch 3994.82 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    16 /   321 | ms/batch 4148.31 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch    18 /   321 | ms/batch 4601.55 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    20 /   321 | ms/batch 4728.57 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    22 /   321 | ms/batch 6136.76 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch    24 /   321 | ms/batch 4956.08 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    26 /   321 | ms/batch 4962.88 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    28 /   321 | ms/batch 4933.15 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    30 /   321 | ms/batch 4532.51 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    32 /   321 | ms/batch 4572.96 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    34 /   321 | ms/batch 4204.63 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    36 /   321 | ms/batch 5883.15 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    38 /   321 | ms/batch 5347.03 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    40 /   321 | ms/batch 5201.56 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    42 /   321 | ms/batch 5051.44 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    44 /   321 | ms/batch 5089.70 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    46 /   321 | ms/batch 4342.93 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    48 /   321 | ms/batch 5417.36 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    50 /   321 | ms/batch 4797.61 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    52 /   321 | ms/batch 4956.69 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    54 /   321 | ms/batch 4579.21 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    56 /   321 | ms/batch 5022.06 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    58 /   321 | ms/batch 4863.01 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    60 /   321 | ms/batch 4372.59 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch    62 /   321 | ms/batch 4243.27 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    64 /   321 | ms/batch 4619.32 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    66 /   321 | ms/batch 4372.99 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    68 /   321 | ms/batch 4646.86 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    70 /   321 | ms/batch 4342.64 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    72 /   321 | ms/batch 3713.84 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    74 /   321 | ms/batch 4422.49 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    76 /   321 | ms/batch 4680.36 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    78 /   321 | ms/batch 5674.13 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    80 /   321 | ms/batch 5040.79 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    82 /   321 | ms/batch 5264.55 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    84 /   321 | ms/batch 4106.55 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch    86 /   321 | ms/batch 5823.48 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    88 /   321 | ms/batch 5902.32 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    90 /   321 | ms/batch 4490.68 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch    92 /   321 | ms/batch 5457.56 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch    94 /   321 | ms/batch 4716.97 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch    96 /   321 | ms/batch 4418.21 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch    98 /   321 | ms/batch 5577.91 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   100 /   321 | ms/batch 5638.23 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  25 | Batch   102 /   321 | ms/batch 4521.56 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   104 /   321 | ms/batch 5382.72 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   106 /   321 | ms/batch 3587.56 | Loss 00.54 |\n",
      "[Training]| Epochs  25 | Batch   108 /   321 | ms/batch 4130.88 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   110 /   321 | ms/batch 5525.45 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   112 /   321 | ms/batch 3789.77 | Loss 00.54 |\n",
      "[Training]| Epochs  25 | Batch   114 /   321 | ms/batch 4974.51 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   116 /   321 | ms/batch 5272.07 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   118 /   321 | ms/batch 5433.10 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   120 /   321 | ms/batch 4834.03 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   122 /   321 | ms/batch 5375.39 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   124 /   321 | ms/batch 5357.36 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   126 /   321 | ms/batch 5021.22 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   128 /   321 | ms/batch 5186.99 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   130 /   321 | ms/batch 5368.17 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   132 /   321 | ms/batch 3392.29 | Loss 00.54 |\n",
      "[Training]| Epochs  25 | Batch   134 /   321 | ms/batch 5873.07 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   136 /   321 | ms/batch 4702.21 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   138 /   321 | ms/batch 4100.07 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   140 /   321 | ms/batch 4552.76 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   142 /   321 | ms/batch 6163.91 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   144 /   321 | ms/batch 5621.14 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   146 /   321 | ms/batch 4876.99 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   148 /   321 | ms/batch 6407.58 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   150 /   321 | ms/batch 4310.46 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   152 /   321 | ms/batch 4793.34 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   154 /   321 | ms/batch 3876.89 | Loss 00.54 |\n",
      "[Training]| Epochs  25 | Batch   156 /   321 | ms/batch 5696.50 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   158 /   321 | ms/batch 5760.05 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   160 /   321 | ms/batch 4440.66 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   162 /   321 | ms/batch 4745.75 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   164 /   321 | ms/batch 4178.05 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   166 /   321 | ms/batch 4914.78 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   168 /   321 | ms/batch 6781.86 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   170 /   321 | ms/batch 5419.28 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   172 /   321 | ms/batch 5106.14 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   174 /   321 | ms/batch 4605.97 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   176 /   321 | ms/batch 5197.63 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   178 /   321 | ms/batch 6101.02 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   180 /   321 | ms/batch 5334.59 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   182 /   321 | ms/batch 5481.69 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   184 /   321 | ms/batch 5003.80 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   186 /   321 | ms/batch 6252.49 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   188 /   321 | ms/batch 4490.37 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   190 /   321 | ms/batch 5209.86 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   192 /   321 | ms/batch 5606.15 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   194 /   321 | ms/batch 6115.93 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   196 /   321 | ms/batch 6003.22 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   198 /   321 | ms/batch 5147.24 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   200 /   321 | ms/batch 4226.41 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   202 /   321 | ms/batch 4551.36 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   204 /   321 | ms/batch 4532.67 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   206 /   321 | ms/batch 4784.90 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   208 /   321 | ms/batch 4116.68 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch   210 /   321 | ms/batch 3875.82 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   212 /   321 | ms/batch 4604.74 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   214 /   321 | ms/batch 5065.36 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   216 /   321 | ms/batch 4504.60 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   218 /   321 | ms/batch 5369.10 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   220 /   321 | ms/batch 5157.65 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   222 /   321 | ms/batch 4301.01 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   224 /   321 | ms/batch 5295.83 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   226 /   321 | ms/batch 5018.38 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   228 /   321 | ms/batch 4884.29 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   230 /   321 | ms/batch 5447.48 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   232 /   321 | ms/batch 4865.42 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   234 /   321 | ms/batch 5530.36 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   236 /   321 | ms/batch 5339.07 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   238 /   321 | ms/batch 4483.58 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   240 /   321 | ms/batch 3972.08 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   242 /   321 | ms/batch 3385.88 | Loss 00.52 |\n",
      "[Training]| Epochs  25 | Batch   244 /   321 | ms/batch 5377.14 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   246 /   321 | ms/batch 4865.28 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   248 /   321 | ms/batch 5516.71 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   250 /   321 | ms/batch 5260.78 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   252 /   321 | ms/batch 4997.37 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   254 /   321 | ms/batch 4732.41 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   256 /   321 | ms/batch 5542.11 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   258 /   321 | ms/batch 4162.75 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   260 /   321 | ms/batch 4474.80 | Loss 00.56 |\n",
      "[Training]| Epochs  25 | Batch   262 /   321 | ms/batch 6048.80 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   264 /   321 | ms/batch 4433.22 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   266 /   321 | ms/batch 4829.99 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   268 /   321 | ms/batch 5076.11 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   270 /   321 | ms/batch 5282.46 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   272 /   321 | ms/batch 5586.60 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   274 /   321 | ms/batch 5482.64 | Loss 00.60 |\n",
      "[Training]| Epochs  25 | Batch   276 /   321 | ms/batch 4562.23 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   278 /   321 | ms/batch 5870.36 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   280 /   321 | ms/batch 4608.63 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   282 /   321 | ms/batch 4930.35 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   284 /   321 | ms/batch 4159.63 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch   286 /   321 | ms/batch 5162.06 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   288 /   321 | ms/batch 5573.74 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   290 /   321 | ms/batch 5387.10 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   292 /   321 | ms/batch 4221.92 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   294 /   321 | ms/batch 4877.83 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   296 /   321 | ms/batch 4131.01 | Loss 00.57 |\n",
      "[Training]| Epochs  25 | Batch   298 /   321 | ms/batch 5189.81 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   300 /   321 | ms/batch 4218.94 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   302 /   321 | ms/batch 3973.18 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch   304 /   321 | ms/batch 5643.81 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   306 /   321 | ms/batch 6073.61 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   308 /   321 | ms/batch 4542.80 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  25 | Batch   310 /   321 | ms/batch 4282.66 | Loss 00.55 |\n",
      "[Training]| Epochs  25 | Batch   312 /   321 | ms/batch 5866.82 | Loss 00.58 |\n",
      "[Training]| Epochs  25 | Batch   314 /   321 | ms/batch 5901.19 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   316 /   321 | ms/batch 5720.16 | Loss 00.59 |\n",
      "[Training]| Epochs  25 | Batch   318 /   321 | ms/batch 4988.37 | Loss 00.59 |\n",
      "32\n",
      "[Training]| Epochs  25 | Batch   320 /   321 | ms/batch 5360.73 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  25 | Elapsed 1427.49 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.47\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.56\n",
      "result= [26, 0.09283785623028608, 0.11908777226532054, 0.174437049395947, 0.20880077002295644, 0.22991538995518315, 0.11940735053570124, 0.07895579527619288, 0.04063360819292552, 0.029898779520464425, 0.025433231868134377, 0.08449365343791103, 0.07801240845202284, 0.058091714432345454, 0.047678250961719966, 0.04245264792200744, 0.08285720120978977, 0.08789401617886924, 0.09360226105542205, 0.09581069703125311, 0.09692243380435406, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847900399455318]\n",
      "[Training]| Epochs  26 | Batch     2 /   321 | ms/batch 6893.99 | Loss 00.86 |\n",
      "[Training]| Epochs  26 | Batch     4 /   321 | ms/batch 4747.24 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch     6 /   321 | ms/batch 5989.00 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch     8 /   321 | ms/batch 5537.08 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch    10 /   321 | ms/batch 5530.69 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    12 /   321 | ms/batch 5404.95 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    14 /   321 | ms/batch 4150.89 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    16 /   321 | ms/batch 4336.27 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    18 /   321 | ms/batch 4553.88 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    20 /   321 | ms/batch 4748.47 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    22 /   321 | ms/batch 6314.75 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch    24 /   321 | ms/batch 4744.18 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    26 /   321 | ms/batch 4684.82 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    28 /   321 | ms/batch 5102.74 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    30 /   321 | ms/batch 4717.64 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    32 /   321 | ms/batch 4391.15 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    34 /   321 | ms/batch 4369.84 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    36 /   321 | ms/batch 5623.91 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch    38 /   321 | ms/batch 5303.30 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    40 /   321 | ms/batch 5439.40 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    42 /   321 | ms/batch 5123.91 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch    44 /   321 | ms/batch 5172.53 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    46 /   321 | ms/batch 4509.75 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    48 /   321 | ms/batch 5009.46 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    50 /   321 | ms/batch 4363.09 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    52 /   321 | ms/batch 5469.00 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    54 /   321 | ms/batch 4662.76 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    56 /   321 | ms/batch 4861.02 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    58 /   321 | ms/batch 4787.37 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    60 /   321 | ms/batch 4084.07 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    62 /   321 | ms/batch 4284.59 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    64 /   321 | ms/batch 4318.27 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  26 | Batch    66 /   321 | ms/batch 4273.52 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    68 /   321 | ms/batch 4456.11 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    70 /   321 | ms/batch 4567.37 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    72 /   321 | ms/batch 3776.18 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch    74 /   321 | ms/batch 4378.39 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    76 /   321 | ms/batch 4716.11 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    78 /   321 | ms/batch 5537.05 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch    80 /   321 | ms/batch 5062.98 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    82 /   321 | ms/batch 5484.68 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch    84 /   321 | ms/batch 3842.11 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch    86 /   321 | ms/batch 5695.37 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch    88 /   321 | ms/batch 5817.55 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch    90 /   321 | ms/batch 4476.58 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    92 /   321 | ms/batch 5406.30 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch    94 /   321 | ms/batch 4794.72 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch    96 /   321 | ms/batch 4389.72 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch    98 /   321 | ms/batch 5665.17 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   100 /   321 | ms/batch 5510.34 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   102 /   321 | ms/batch 4383.57 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   104 /   321 | ms/batch 5178.58 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   106 /   321 | ms/batch 3650.23 | Loss 00.54 |\n",
      "[Training]| Epochs  26 | Batch   108 /   321 | ms/batch 4213.63 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   110 /   321 | ms/batch 5387.67 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   112 /   321 | ms/batch 3836.45 | Loss 00.54 |\n",
      "[Training]| Epochs  26 | Batch   114 /   321 | ms/batch 4749.63 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   116 /   321 | ms/batch 5311.86 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   118 /   321 | ms/batch 5287.66 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   120 /   321 | ms/batch 4831.59 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   122 /   321 | ms/batch 5422.24 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   124 /   321 | ms/batch 5215.00 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   126 /   321 | ms/batch 4834.70 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   128 /   321 | ms/batch 5343.31 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   130 /   321 | ms/batch 5325.10 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   132 /   321 | ms/batch 3420.33 | Loss 00.53 |\n",
      "[Training]| Epochs  26 | Batch   134 /   321 | ms/batch 5292.98 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   136 /   321 | ms/batch 4780.38 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   138 /   321 | ms/batch 3923.00 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   140 /   321 | ms/batch 4680.90 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   142 /   321 | ms/batch 5843.99 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   144 /   321 | ms/batch 5260.02 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   146 /   321 | ms/batch 5035.30 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   148 /   321 | ms/batch 6299.69 | Loss 00.61 |\n",
      "[Training]| Epochs  26 | Batch   150 /   321 | ms/batch 4157.83 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   152 /   321 | ms/batch 4962.17 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   154 /   321 | ms/batch 3876.40 | Loss 00.54 |\n",
      "[Training]| Epochs  26 | Batch   156 /   321 | ms/batch 5755.52 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   158 /   321 | ms/batch 5895.61 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   160 /   321 | ms/batch 4441.39 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   162 /   321 | ms/batch 4949.63 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   164 /   321 | ms/batch 4006.20 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   166 /   321 | ms/batch 4879.63 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   168 /   321 | ms/batch 6764.17 | Loss 00.61 |\n",
      "[Training]| Epochs  26 | Batch   170 /   321 | ms/batch 5514.66 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   172 /   321 | ms/batch 4813.14 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   174 /   321 | ms/batch 4598.34 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   176 /   321 | ms/batch 5045.78 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   178 /   321 | ms/batch 6187.11 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   180 /   321 | ms/batch 5180.41 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   182 /   321 | ms/batch 5370.01 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   184 /   321 | ms/batch 4493.20 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   186 /   321 | ms/batch 6078.09 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   188 /   321 | ms/batch 4553.32 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   190 /   321 | ms/batch 4645.77 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   192 /   321 | ms/batch 5573.50 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   194 /   321 | ms/batch 6044.67 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   196 /   321 | ms/batch 6028.93 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   198 /   321 | ms/batch 5022.64 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   200 /   321 | ms/batch 4127.66 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   202 /   321 | ms/batch 4841.90 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   204 /   321 | ms/batch 4608.11 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   206 /   321 | ms/batch 4779.35 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   208 /   321 | ms/batch 4302.53 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch   210 /   321 | ms/batch 3993.76 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   212 /   321 | ms/batch 4525.53 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   214 /   321 | ms/batch 4981.71 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   216 /   321 | ms/batch 4465.07 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   218 /   321 | ms/batch 5531.67 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   220 /   321 | ms/batch 5230.08 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   222 /   321 | ms/batch 4401.87 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   224 /   321 | ms/batch 5451.84 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   226 /   321 | ms/batch 4904.48 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   228 /   321 | ms/batch 4858.83 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   230 /   321 | ms/batch 6010.04 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   232 /   321 | ms/batch 4630.09 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   234 /   321 | ms/batch 5072.29 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   236 /   321 | ms/batch 5456.17 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   238 /   321 | ms/batch 4455.20 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   240 /   321 | ms/batch 3697.15 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   242 /   321 | ms/batch 3351.18 | Loss 00.52 |\n",
      "[Training]| Epochs  26 | Batch   244 /   321 | ms/batch 5402.26 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   246 /   321 | ms/batch 4679.45 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   248 /   321 | ms/batch 5311.00 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   250 /   321 | ms/batch 5236.97 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   252 /   321 | ms/batch 5045.21 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   254 /   321 | ms/batch 4874.20 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch   256 /   321 | ms/batch 5491.77 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   258 /   321 | ms/batch 4175.30 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   260 /   321 | ms/batch 4410.80 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   262 /   321 | ms/batch 6173.15 | Loss 00.60 |\n",
      "[Training]| Epochs  26 | Batch   264 /   321 | ms/batch 4517.13 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   266 /   321 | ms/batch 4329.22 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   268 /   321 | ms/batch 4992.50 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   270 /   321 | ms/batch 5109.99 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   272 /   321 | ms/batch 5638.95 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  26 | Batch   274 /   321 | ms/batch 5555.73 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   276 /   321 | ms/batch 4539.65 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   278 /   321 | ms/batch 5671.45 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   280 /   321 | ms/batch 4733.67 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   282 /   321 | ms/batch 4974.55 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   284 /   321 | ms/batch 4296.13 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch   286 /   321 | ms/batch 5134.23 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   288 /   321 | ms/batch 5832.71 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   290 /   321 | ms/batch 5092.49 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   292 /   321 | ms/batch 4167.26 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   294 /   321 | ms/batch 4922.53 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   296 /   321 | ms/batch 4299.37 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   298 /   321 | ms/batch 5093.75 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   300 /   321 | ms/batch 4269.59 | Loss 00.57 |\n",
      "[Training]| Epochs  26 | Batch   302 /   321 | ms/batch 3955.83 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch   304 /   321 | ms/batch 5431.67 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   306 /   321 | ms/batch 6236.09 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   308 /   321 | ms/batch 4154.68 | Loss 00.56 |\n",
      "[Training]| Epochs  26 | Batch   310 /   321 | ms/batch 4114.77 | Loss 00.55 |\n",
      "[Training]| Epochs  26 | Batch   312 /   321 | ms/batch 5762.77 | Loss 00.58 |\n",
      "[Training]| Epochs  26 | Batch   314 /   321 | ms/batch 5787.30 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   316 /   321 | ms/batch 5882.35 | Loss 00.59 |\n",
      "[Training]| Epochs  26 | Batch   318 /   321 | ms/batch 4931.34 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  26 | Batch   320 /   321 | ms/batch 5187.83 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  26 | Elapsed 1432.74 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.56\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.75\n",
      "result= [27, 0.09467944816416907, 0.11982845981327334, 0.17333497949066518, 0.20853643499123148, 0.22972714668684235, 0.12455171509264784, 0.07922859816279898, 0.04026988587710231, 0.029657160223041396, 0.025553131698168356, 0.087377671608215, 0.07834798882737906, 0.05764950687589816, 0.047299686113745555, 0.042633062587152305, 0.0742473245982374, 0.07878336722585819, 0.08433096569713092, 0.08656293868587374, 0.08774463666366328, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5844101854312567]\n",
      "[Training]| Epochs  27 | Batch     2 /   321 | ms/batch 6734.72 | Loss 00.86 |\n",
      "[Training]| Epochs  27 | Batch     4 /   321 | ms/batch 4689.00 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch     6 /   321 | ms/batch 6235.38 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch     8 /   321 | ms/batch 5869.61 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    10 /   321 | ms/batch 5431.57 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    12 /   321 | ms/batch 5530.94 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    14 /   321 | ms/batch 4028.23 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    16 /   321 | ms/batch 4248.16 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch    18 /   321 | ms/batch 4695.46 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    20 /   321 | ms/batch 4901.45 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    22 /   321 | ms/batch 6192.96 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch    24 /   321 | ms/batch 4707.72 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    26 /   321 | ms/batch 4562.53 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    28 /   321 | ms/batch 5037.02 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  27 | Batch    30 /   321 | ms/batch 4748.09 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    32 /   321 | ms/batch 4548.82 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    34 /   321 | ms/batch 4247.17 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    36 /   321 | ms/batch 5627.71 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    38 /   321 | ms/batch 5470.39 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    40 /   321 | ms/batch 5297.39 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    42 /   321 | ms/batch 5198.47 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    44 /   321 | ms/batch 5009.94 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    46 /   321 | ms/batch 4460.64 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    48 /   321 | ms/batch 5375.22 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    50 /   321 | ms/batch 4600.35 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    52 /   321 | ms/batch 5235.48 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    54 /   321 | ms/batch 4496.24 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    56 /   321 | ms/batch 4937.53 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    58 /   321 | ms/batch 4883.58 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    60 /   321 | ms/batch 4036.30 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    62 /   321 | ms/batch 4352.27 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    64 /   321 | ms/batch 4372.94 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    66 /   321 | ms/batch 4249.48 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    68 /   321 | ms/batch 4792.90 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    70 /   321 | ms/batch 4497.69 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    72 /   321 | ms/batch 3773.04 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch    74 /   321 | ms/batch 4271.41 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    76 /   321 | ms/batch 4825.39 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    78 /   321 | ms/batch 5316.72 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    80 /   321 | ms/batch 5014.60 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    82 /   321 | ms/batch 5307.34 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    84 /   321 | ms/batch 3734.04 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch    86 /   321 | ms/batch 5709.03 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    88 /   321 | ms/batch 5762.55 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch    90 /   321 | ms/batch 4281.11 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    92 /   321 | ms/batch 5592.78 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch    94 /   321 | ms/batch 4997.34 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch    96 /   321 | ms/batch 4271.67 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch    98 /   321 | ms/batch 5760.54 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   100 /   321 | ms/batch 5506.60 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   102 /   321 | ms/batch 4295.24 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   104 /   321 | ms/batch 5413.86 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   106 /   321 | ms/batch 3829.12 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch   108 /   321 | ms/batch 4179.34 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   110 /   321 | ms/batch 5429.53 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   112 /   321 | ms/batch 3893.37 | Loss 00.54 |\n",
      "[Training]| Epochs  27 | Batch   114 /   321 | ms/batch 4974.52 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   116 /   321 | ms/batch 5356.19 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   118 /   321 | ms/batch 5164.81 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   120 /   321 | ms/batch 4763.21 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   122 /   321 | ms/batch 5079.80 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   124 /   321 | ms/batch 5137.66 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   126 /   321 | ms/batch 4732.41 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   128 /   321 | ms/batch 5211.86 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   130 /   321 | ms/batch 5380.67 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   132 /   321 | ms/batch 3315.18 | Loss 00.53 |\n",
      "[Training]| Epochs  27 | Batch   134 /   321 | ms/batch 5248.79 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   136 /   321 | ms/batch 4658.69 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   138 /   321 | ms/batch 4072.57 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   140 /   321 | ms/batch 4529.36 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   142 /   321 | ms/batch 6019.09 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   144 /   321 | ms/batch 5426.47 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   146 /   321 | ms/batch 4808.04 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   148 /   321 | ms/batch 6182.05 | Loss 00.61 |\n",
      "[Training]| Epochs  27 | Batch   150 /   321 | ms/batch 4362.07 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   152 /   321 | ms/batch 4870.58 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   154 /   321 | ms/batch 3773.55 | Loss 00.54 |\n",
      "[Training]| Epochs  27 | Batch   156 /   321 | ms/batch 5509.71 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   158 /   321 | ms/batch 6058.69 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   160 /   321 | ms/batch 4181.62 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   162 /   321 | ms/batch 4821.44 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   164 /   321 | ms/batch 3781.84 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   166 /   321 | ms/batch 4960.37 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   168 /   321 | ms/batch 6665.24 | Loss 00.61 |\n",
      "[Training]| Epochs  27 | Batch   170 /   321 | ms/batch 5462.16 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   172 /   321 | ms/batch 4915.38 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   174 /   321 | ms/batch 4653.00 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   176 /   321 | ms/batch 5056.78 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   178 /   321 | ms/batch 5934.45 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   180 /   321 | ms/batch 5338.79 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   182 /   321 | ms/batch 5328.48 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   184 /   321 | ms/batch 4901.55 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   186 /   321 | ms/batch 6168.50 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   188 /   321 | ms/batch 3971.37 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   190 /   321 | ms/batch 4841.66 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   192 /   321 | ms/batch 5583.29 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   194 /   321 | ms/batch 6291.33 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   196 /   321 | ms/batch 6230.44 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   198 /   321 | ms/batch 4987.52 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   200 /   321 | ms/batch 4376.65 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   202 /   321 | ms/batch 4647.53 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   204 /   321 | ms/batch 4565.85 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   206 /   321 | ms/batch 4608.87 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   208 /   321 | ms/batch 4045.01 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch   210 /   321 | ms/batch 3954.38 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   212 /   321 | ms/batch 4465.08 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   214 /   321 | ms/batch 5049.32 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   216 /   321 | ms/batch 4606.47 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   218 /   321 | ms/batch 5184.84 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   220 /   321 | ms/batch 5160.78 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   222 /   321 | ms/batch 4474.79 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   224 /   321 | ms/batch 5303.72 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   226 /   321 | ms/batch 5002.30 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   228 /   321 | ms/batch 5023.79 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   230 /   321 | ms/batch 5637.77 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   232 /   321 | ms/batch 4834.09 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   234 /   321 | ms/batch 5390.15 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   236 /   321 | ms/batch 5330.56 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  27 | Batch   238 /   321 | ms/batch 4501.96 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   240 /   321 | ms/batch 4013.15 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   242 /   321 | ms/batch 3385.05 | Loss 00.51 |\n",
      "[Training]| Epochs  27 | Batch   244 /   321 | ms/batch 5691.54 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   246 /   321 | ms/batch 4861.12 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   248 /   321 | ms/batch 5499.46 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   250 /   321 | ms/batch 5271.86 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   252 /   321 | ms/batch 5076.77 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   254 /   321 | ms/batch 4594.70 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   256 /   321 | ms/batch 5508.25 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   258 /   321 | ms/batch 4049.48 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   260 /   321 | ms/batch 4723.89 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   262 /   321 | ms/batch 6295.41 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   264 /   321 | ms/batch 4628.73 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   266 /   321 | ms/batch 4543.92 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   268 /   321 | ms/batch 4968.71 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   270 /   321 | ms/batch 5081.77 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   272 /   321 | ms/batch 5806.88 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   274 /   321 | ms/batch 5776.62 | Loss 00.60 |\n",
      "[Training]| Epochs  27 | Batch   276 /   321 | ms/batch 4506.98 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   278 /   321 | ms/batch 5738.84 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   280 /   321 | ms/batch 4569.86 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   282 /   321 | ms/batch 4878.27 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   284 /   321 | ms/batch 4121.69 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch   286 /   321 | ms/batch 5106.55 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   288 /   321 | ms/batch 5765.96 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   290 /   321 | ms/batch 5287.73 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   292 /   321 | ms/batch 4310.17 | Loss 00.57 |\n",
      "[Training]| Epochs  27 | Batch   294 /   321 | ms/batch 4880.80 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   296 /   321 | ms/batch 4248.20 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   298 /   321 | ms/batch 5188.83 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   300 /   321 | ms/batch 4326.01 | Loss 00.58 |\n",
      "[Training]| Epochs  27 | Batch   302 /   321 | ms/batch 4128.23 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch   304 /   321 | ms/batch 5659.43 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   306 /   321 | ms/batch 6141.55 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   308 /   321 | ms/batch 4428.01 | Loss 00.56 |\n",
      "[Training]| Epochs  27 | Batch   310 /   321 | ms/batch 4403.41 | Loss 00.55 |\n",
      "[Training]| Epochs  27 | Batch   312 /   321 | ms/batch 5730.39 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   314 /   321 | ms/batch 6098.51 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   316 /   321 | ms/batch 5761.73 | Loss 00.59 |\n",
      "[Training]| Epochs  27 | Batch   318 /   321 | ms/batch 4802.80 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  27 | Batch   320 /   321 | ms/batch 5173.18 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  27 | Elapsed 1431.80 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.98\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.58\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.52\n",
      "result= [28, 0.09274545908903388, 0.11809037313364673, 0.17531042775416505, 0.20749990770989624, 0.22906284822863893, 0.11932939156274357, 0.07809840712989576, 0.0408154738107326, 0.02961819262961717, 0.025343302535713416, 0.08437215993855952, 0.07737422714173933, 0.058346963170349646, 0.04726976021508732, 0.0422965129278455, 0.08306962380727241, 0.0879465276106832, 0.09384113441283842, 0.09591606290976859, 0.09705425241006799, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5837076512383826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  28 | Batch     2 /   321 | ms/batch 6639.42 | Loss 00.85 |\n",
      "[Training]| Epochs  28 | Batch     4 /   321 | ms/batch 4842.88 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch     6 /   321 | ms/batch 6047.18 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch     8 /   321 | ms/batch 5680.54 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch    10 /   321 | ms/batch 5532.13 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    12 /   321 | ms/batch 5568.11 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    14 /   321 | ms/batch 4099.99 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    16 /   321 | ms/batch 4488.70 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    18 /   321 | ms/batch 4610.70 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    20 /   321 | ms/batch 4857.89 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    22 /   321 | ms/batch 6363.74 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch    24 /   321 | ms/batch 4496.90 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    26 /   321 | ms/batch 4666.28 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    28 /   321 | ms/batch 5123.93 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    30 /   321 | ms/batch 4782.60 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    32 /   321 | ms/batch 4675.50 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    34 /   321 | ms/batch 4123.42 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    36 /   321 | ms/batch 5573.54 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    38 /   321 | ms/batch 5276.71 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    40 /   321 | ms/batch 5236.56 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    42 /   321 | ms/batch 5181.37 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    44 /   321 | ms/batch 5075.92 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    46 /   321 | ms/batch 4449.78 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    48 /   321 | ms/batch 5450.91 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    50 /   321 | ms/batch 4305.23 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    52 /   321 | ms/batch 4867.60 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    54 /   321 | ms/batch 4730.17 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    56 /   321 | ms/batch 4954.60 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    58 /   321 | ms/batch 4840.12 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    60 /   321 | ms/batch 4279.29 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    62 /   321 | ms/batch 4330.40 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    64 /   321 | ms/batch 4410.94 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    66 /   321 | ms/batch 4314.51 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    68 /   321 | ms/batch 4819.53 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    70 /   321 | ms/batch 4658.03 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    72 /   321 | ms/batch 3735.50 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    74 /   321 | ms/batch 4428.59 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    76 /   321 | ms/batch 4936.60 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    78 /   321 | ms/batch 5458.52 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    80 /   321 | ms/batch 4875.03 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    82 /   321 | ms/batch 5411.16 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    84 /   321 | ms/batch 4179.29 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    86 /   321 | ms/batch 5774.80 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    88 /   321 | ms/batch 6063.00 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    90 /   321 | ms/batch 4318.96 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch    92 /   321 | ms/batch 5241.17 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch    94 /   321 | ms/batch 4741.47 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch    96 /   321 | ms/batch 4391.52 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch    98 /   321 | ms/batch 5569.76 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   100 /   321 | ms/batch 5777.78 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   102 /   321 | ms/batch 4518.56 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   104 /   321 | ms/batch 5531.84 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   106 /   321 | ms/batch 3791.89 | Loss 00.54 |\n",
      "[Training]| Epochs  28 | Batch   108 /   321 | ms/batch 4355.73 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   110 /   321 | ms/batch 5414.31 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   112 /   321 | ms/batch 3724.17 | Loss 00.54 |\n",
      "[Training]| Epochs  28 | Batch   114 /   321 | ms/batch 4676.31 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   116 /   321 | ms/batch 5396.72 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   118 /   321 | ms/batch 5076.15 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   120 /   321 | ms/batch 5135.07 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   122 /   321 | ms/batch 5263.75 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   124 /   321 | ms/batch 5222.07 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   126 /   321 | ms/batch 5023.51 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   128 /   321 | ms/batch 5287.97 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   130 /   321 | ms/batch 5519.38 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   132 /   321 | ms/batch 3412.83 | Loss 00.54 |\n",
      "[Training]| Epochs  28 | Batch   134 /   321 | ms/batch 5549.96 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   136 /   321 | ms/batch 4584.05 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   138 /   321 | ms/batch 4188.59 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   140 /   321 | ms/batch 4790.01 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   142 /   321 | ms/batch 6393.52 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   144 /   321 | ms/batch 5519.63 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   146 /   321 | ms/batch 4907.50 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   148 /   321 | ms/batch 6375.31 | Loss 00.61 |\n",
      "[Training]| Epochs  28 | Batch   150 /   321 | ms/batch 4299.92 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   152 /   321 | ms/batch 4907.94 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   154 /   321 | ms/batch 3810.48 | Loss 00.54 |\n",
      "[Training]| Epochs  28 | Batch   156 /   321 | ms/batch 5695.39 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   158 /   321 | ms/batch 6122.75 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   160 /   321 | ms/batch 4396.27 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   162 /   321 | ms/batch 4811.71 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   164 /   321 | ms/batch 4013.31 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   166 /   321 | ms/batch 4974.65 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   168 /   321 | ms/batch 6754.32 | Loss 00.61 |\n",
      "[Training]| Epochs  28 | Batch   170 /   321 | ms/batch 5364.62 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   172 /   321 | ms/batch 5178.84 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   174 /   321 | ms/batch 4695.51 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   176 /   321 | ms/batch 5077.79 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   178 /   321 | ms/batch 6136.26 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch   180 /   321 | ms/batch 5056.18 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   182 /   321 | ms/batch 5271.19 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   184 /   321 | ms/batch 4753.53 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   186 /   321 | ms/batch 6518.88 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   188 /   321 | ms/batch 4251.92 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   190 /   321 | ms/batch 4918.76 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   192 /   321 | ms/batch 5744.73 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   194 /   321 | ms/batch 6128.34 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch   196 /   321 | ms/batch 6040.92 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch   198 /   321 | ms/batch 4957.25 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   200 /   321 | ms/batch 4166.83 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   202 /   321 | ms/batch 4689.86 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   204 /   321 | ms/batch 4689.14 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   206 /   321 | ms/batch 4693.64 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   208 /   321 | ms/batch 4349.00 | Loss 00.55 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  28 | Batch   210 /   321 | ms/batch 4144.47 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   212 /   321 | ms/batch 4724.67 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   214 /   321 | ms/batch 4923.07 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   216 /   321 | ms/batch 4690.57 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   218 /   321 | ms/batch 5520.11 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   220 /   321 | ms/batch 5407.97 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   222 /   321 | ms/batch 4386.18 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   224 /   321 | ms/batch 5262.62 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   226 /   321 | ms/batch 5184.87 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   228 /   321 | ms/batch 5163.73 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   230 /   321 | ms/batch 5833.63 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   232 /   321 | ms/batch 4793.43 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   234 /   321 | ms/batch 5247.29 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   236 /   321 | ms/batch 5360.31 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   238 /   321 | ms/batch 4571.91 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   240 /   321 | ms/batch 3740.45 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   242 /   321 | ms/batch 3407.33 | Loss 00.51 |\n",
      "[Training]| Epochs  28 | Batch   244 /   321 | ms/batch 5483.79 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch   246 /   321 | ms/batch 5172.57 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   248 /   321 | ms/batch 5243.40 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   250 /   321 | ms/batch 5134.10 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   252 /   321 | ms/batch 4920.91 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   254 /   321 | ms/batch 4693.92 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   256 /   321 | ms/batch 5743.26 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   258 /   321 | ms/batch 4087.44 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   260 /   321 | ms/batch 4654.36 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   262 /   321 | ms/batch 6265.41 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   264 /   321 | ms/batch 4579.57 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   266 /   321 | ms/batch 4924.13 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   268 /   321 | ms/batch 4943.75 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   270 /   321 | ms/batch 5124.46 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   272 /   321 | ms/batch 5930.79 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   274 /   321 | ms/batch 5825.60 | Loss 00.60 |\n",
      "[Training]| Epochs  28 | Batch   276 /   321 | ms/batch 4523.43 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   278 /   321 | ms/batch 5869.42 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   280 /   321 | ms/batch 4754.07 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   282 /   321 | ms/batch 5141.77 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   284 /   321 | ms/batch 4279.32 | Loss 00.55 |\n",
      "[Training]| Epochs  28 | Batch   286 /   321 | ms/batch 5291.53 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   288 /   321 | ms/batch 5965.95 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   290 /   321 | ms/batch 5302.60 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   292 /   321 | ms/batch 4219.75 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   294 /   321 | ms/batch 5037.22 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   296 /   321 | ms/batch 4206.49 | Loss 00.57 |\n",
      "[Training]| Epochs  28 | Batch   298 /   321 | ms/batch 5170.77 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   300 /   321 | ms/batch 4295.96 | Loss 00.58 |\n",
      "[Training]| Epochs  28 | Batch   302 /   321 | ms/batch 4387.45 | Loss 00.55 |\n",
      "[Training]| Epochs  28 | Batch   304 /   321 | ms/batch 5584.30 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   306 /   321 | ms/batch 6008.21 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   308 /   321 | ms/batch 4562.68 | Loss 00.56 |\n",
      "[Training]| Epochs  28 | Batch   310 /   321 | ms/batch 4214.19 | Loss 00.55 |\n",
      "[Training]| Epochs  28 | Batch   312 /   321 | ms/batch 5951.23 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   314 /   321 | ms/batch 6163.21 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   316 /   321 | ms/batch 5954.28 | Loss 00.59 |\n",
      "[Training]| Epochs  28 | Batch   318 /   321 | ms/batch 4715.99 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  28 | Batch   320 /   321 | ms/batch 5260.79 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  28 | Elapsed 1435.78 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.70\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.32\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.55\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.83\n",
      "result= [29, 0.09260503184674591, 0.11888316415383866, 0.17440348719584226, 0.20796751883098694, 0.22737201265497126, 0.11909562168136205, 0.08012493011641113, 0.04088042772847605, 0.029664962066864465, 0.025031555841792306, 0.08421087822501583, 0.0788452969058268, 0.05840128469727324, 0.04732045733361902, 0.04179393622620628, 0.08324125769169927, 0.08839632427742355, 0.09402972323451966, 0.09614194245371657, 0.09715871886798091, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5842128116407512]\n",
      "[Training]| Epochs  29 | Batch     2 /   321 | ms/batch 6695.00 | Loss 00.85 |\n",
      "[Training]| Epochs  29 | Batch     4 /   321 | ms/batch 4959.14 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch     6 /   321 | ms/batch 6119.23 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch     8 /   321 | ms/batch 5710.36 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch    10 /   321 | ms/batch 5342.16 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch    12 /   321 | ms/batch 5546.75 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    14 /   321 | ms/batch 4150.78 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    16 /   321 | ms/batch 4364.12 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    18 /   321 | ms/batch 4467.78 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    20 /   321 | ms/batch 4792.59 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    22 /   321 | ms/batch 6183.83 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch    24 /   321 | ms/batch 4858.21 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    26 /   321 | ms/batch 4897.33 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    28 /   321 | ms/batch 4982.38 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    30 /   321 | ms/batch 4715.36 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    32 /   321 | ms/batch 4595.02 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    34 /   321 | ms/batch 4311.17 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch    36 /   321 | ms/batch 5638.50 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch    38 /   321 | ms/batch 5625.16 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    40 /   321 | ms/batch 5223.14 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    42 /   321 | ms/batch 5100.30 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch    44 /   321 | ms/batch 5086.77 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    46 /   321 | ms/batch 4412.34 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    48 /   321 | ms/batch 5228.19 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch    50 /   321 | ms/batch 4512.06 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    52 /   321 | ms/batch 5209.72 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    54 /   321 | ms/batch 4689.34 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    56 /   321 | ms/batch 5002.33 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    58 /   321 | ms/batch 4771.26 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    60 /   321 | ms/batch 4276.03 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    62 /   321 | ms/batch 4443.15 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    64 /   321 | ms/batch 4513.39 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    66 /   321 | ms/batch 4284.22 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    68 /   321 | ms/batch 4510.31 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    70 /   321 | ms/batch 4618.71 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    72 /   321 | ms/batch 3783.99 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch    74 /   321 | ms/batch 4453.53 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    76 /   321 | ms/batch 4780.23 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    78 /   321 | ms/batch 5520.29 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    80 /   321 | ms/batch 4952.19 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    82 /   321 | ms/batch 5294.46 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch    84 /   321 | ms/batch 3885.33 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch    86 /   321 | ms/batch 5659.10 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch    88 /   321 | ms/batch 6096.43 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    90 /   321 | ms/batch 4453.42 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    92 /   321 | ms/batch 5177.29 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch    94 /   321 | ms/batch 5065.71 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch    96 /   321 | ms/batch 4256.43 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch    98 /   321 | ms/batch 5616.69 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   100 /   321 | ms/batch 5757.74 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   102 /   321 | ms/batch 4471.18 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   104 /   321 | ms/batch 5453.90 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   106 /   321 | ms/batch 3894.39 | Loss 00.54 |\n",
      "[Training]| Epochs  29 | Batch   108 /   321 | ms/batch 4001.38 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   110 /   321 | ms/batch 5703.95 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   112 /   321 | ms/batch 3954.71 | Loss 00.54 |\n",
      "[Training]| Epochs  29 | Batch   114 /   321 | ms/batch 5023.13 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   116 /   321 | ms/batch 5523.93 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   118 /   321 | ms/batch 5171.28 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   120 /   321 | ms/batch 4748.89 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   122 /   321 | ms/batch 5329.92 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   124 /   321 | ms/batch 5301.21 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   126 /   321 | ms/batch 5269.75 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   128 /   321 | ms/batch 5372.42 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   130 /   321 | ms/batch 5613.01 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   132 /   321 | ms/batch 3292.93 | Loss 00.54 |\n",
      "[Training]| Epochs  29 | Batch   134 /   321 | ms/batch 5607.19 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   136 /   321 | ms/batch 4561.18 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   138 /   321 | ms/batch 4041.62 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   140 /   321 | ms/batch 4932.20 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch   142 /   321 | ms/batch 5905.07 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   144 /   321 | ms/batch 5563.42 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   146 /   321 | ms/batch 4787.27 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   148 /   321 | ms/batch 6412.24 | Loss 00.61 |\n",
      "[Training]| Epochs  29 | Batch   150 /   321 | ms/batch 4485.15 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   152 /   321 | ms/batch 5092.73 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   154 /   321 | ms/batch 4101.34 | Loss 00.54 |\n",
      "[Training]| Epochs  29 | Batch   156 /   321 | ms/batch 5738.56 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   158 /   321 | ms/batch 5765.80 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   160 /   321 | ms/batch 4475.29 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   162 /   321 | ms/batch 4848.79 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   164 /   321 | ms/batch 4022.36 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   166 /   321 | ms/batch 4866.82 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   168 /   321 | ms/batch 6886.19 | Loss 00.61 |\n",
      "[Training]| Epochs  29 | Batch   170 /   321 | ms/batch 5434.15 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   172 /   321 | ms/batch 5037.31 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  29 | Batch   174 /   321 | ms/batch 4380.81 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   176 /   321 | ms/batch 5251.78 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   178 /   321 | ms/batch 6357.07 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   180 /   321 | ms/batch 5153.94 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   182 /   321 | ms/batch 5451.63 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   184 /   321 | ms/batch 4765.22 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   186 /   321 | ms/batch 6147.90 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   188 /   321 | ms/batch 4374.08 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   190 /   321 | ms/batch 4626.25 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   192 /   321 | ms/batch 5637.32 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   194 /   321 | ms/batch 6187.90 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   196 /   321 | ms/batch 6162.63 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   198 /   321 | ms/batch 5119.69 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   200 /   321 | ms/batch 4333.52 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   202 /   321 | ms/batch 4705.84 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   204 /   321 | ms/batch 4526.64 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   206 /   321 | ms/batch 4826.30 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   208 /   321 | ms/batch 4149.27 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch   210 /   321 | ms/batch 4212.45 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   212 /   321 | ms/batch 4388.73 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   214 /   321 | ms/batch 5061.95 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   216 /   321 | ms/batch 4517.01 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   218 /   321 | ms/batch 5520.75 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   220 /   321 | ms/batch 5143.43 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   222 /   321 | ms/batch 4459.00 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   224 /   321 | ms/batch 5298.63 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   226 /   321 | ms/batch 4809.40 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   228 /   321 | ms/batch 5290.48 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   230 /   321 | ms/batch 6010.25 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   232 /   321 | ms/batch 4698.24 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   234 /   321 | ms/batch 5414.97 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   236 /   321 | ms/batch 5385.54 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   238 /   321 | ms/batch 4818.29 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   240 /   321 | ms/batch 4001.58 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   242 /   321 | ms/batch 3602.61 | Loss 00.51 |\n",
      "[Training]| Epochs  29 | Batch   244 /   321 | ms/batch 5589.57 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   246 /   321 | ms/batch 4759.79 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   248 /   321 | ms/batch 5509.61 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   250 /   321 | ms/batch 5397.35 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   252 /   321 | ms/batch 4965.46 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   254 /   321 | ms/batch 4560.55 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   256 /   321 | ms/batch 5541.22 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   258 /   321 | ms/batch 3949.91 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   260 /   321 | ms/batch 4464.74 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   262 /   321 | ms/batch 6099.64 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   264 /   321 | ms/batch 4338.61 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   266 /   321 | ms/batch 4724.31 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   268 /   321 | ms/batch 5011.97 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   270 /   321 | ms/batch 5331.90 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   272 /   321 | ms/batch 5690.92 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   274 /   321 | ms/batch 5805.69 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   276 /   321 | ms/batch 4431.00 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   278 /   321 | ms/batch 5778.12 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   280 /   321 | ms/batch 4749.68 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   282 /   321 | ms/batch 4825.06 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   284 /   321 | ms/batch 4185.37 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   286 /   321 | ms/batch 5332.59 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   288 /   321 | ms/batch 5910.07 | Loss 00.60 |\n",
      "[Training]| Epochs  29 | Batch   290 /   321 | ms/batch 5492.74 | Loss 00.57 |\n",
      "[Training]| Epochs  29 | Batch   292 /   321 | ms/batch 4207.31 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   294 /   321 | ms/batch 4876.98 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   296 /   321 | ms/batch 4086.96 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   298 /   321 | ms/batch 4875.69 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   300 /   321 | ms/batch 4355.69 | Loss 00.58 |\n",
      "[Training]| Epochs  29 | Batch   302 /   321 | ms/batch 3991.64 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch   304 /   321 | ms/batch 5741.73 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   306 /   321 | ms/batch 6252.27 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   308 /   321 | ms/batch 4323.92 | Loss 00.56 |\n",
      "[Training]| Epochs  29 | Batch   310 /   321 | ms/batch 4271.16 | Loss 00.55 |\n",
      "[Training]| Epochs  29 | Batch   312 /   321 | ms/batch 5895.67 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   314 /   321 | ms/batch 5958.88 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   316 /   321 | ms/batch 6022.11 | Loss 00.59 |\n",
      "[Training]| Epochs  29 | Batch   318 /   321 | ms/batch 4867.43 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  29 | Batch   320 /   321 | ms/batch 5045.27 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  29 | Elapsed 1433.89 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.30\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.71\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.74\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.84\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.55\n",
      "result= [30, 0.09248013098724912, 0.1195292305593275, 0.17344045899198657, 0.20758879640003897, 0.22854932991676002, 0.11901765081534976, 0.07977418014990197, 0.040399781819534596, 0.029602600835025635, 0.025505172955460224, 0.08409511123145826, 0.07876093352296254, 0.05779290738183822, 0.04723321583153954, 0.042516986374174906, 0.08297064922102498, 0.08819180534679272, 0.09371876899321402, 0.09591141922253497, 0.09706817576458455, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5839056703779433]\n",
      "[Training]| Epochs  30 | Batch     2 /   321 | ms/batch 6677.78 | Loss 00.85 |\n",
      "[Training]| Epochs  30 | Batch     4 /   321 | ms/batch 4683.79 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch     6 /   321 | ms/batch 6200.45 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch     8 /   321 | ms/batch 5852.30 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    10 /   321 | ms/batch 5506.17 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    12 /   321 | ms/batch 5715.62 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    14 /   321 | ms/batch 4087.55 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    16 /   321 | ms/batch 4181.77 | Loss 00.55 |\n",
      "[Training]| Epochs  30 | Batch    18 /   321 | ms/batch 4582.45 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    20 /   321 | ms/batch 4963.28 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    22 /   321 | ms/batch 6331.41 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch    24 /   321 | ms/batch 4857.67 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    26 /   321 | ms/batch 4871.02 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    28 /   321 | ms/batch 4916.47 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    30 /   321 | ms/batch 4792.06 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    32 /   321 | ms/batch 4734.30 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    34 /   321 | ms/batch 4349.68 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    36 /   321 | ms/batch 5723.04 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    38 /   321 | ms/batch 5408.60 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    40 /   321 | ms/batch 5499.64 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    42 /   321 | ms/batch 5050.26 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    44 /   321 | ms/batch 5231.69 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    46 /   321 | ms/batch 4591.54 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    48 /   321 | ms/batch 5500.60 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    50 /   321 | ms/batch 4404.28 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    52 /   321 | ms/batch 5486.41 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    54 /   321 | ms/batch 4693.56 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    56 /   321 | ms/batch 5029.77 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    58 /   321 | ms/batch 4934.26 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    60 /   321 | ms/batch 4434.28 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    62 /   321 | ms/batch 4506.38 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    64 /   321 | ms/batch 4393.92 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    66 /   321 | ms/batch 4241.93 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    68 /   321 | ms/batch 4805.94 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    70 /   321 | ms/batch 4572.43 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    72 /   321 | ms/batch 3829.22 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    74 /   321 | ms/batch 4355.98 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    76 /   321 | ms/batch 4802.12 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    78 /   321 | ms/batch 5503.47 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    80 /   321 | ms/batch 5100.58 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    82 /   321 | ms/batch 5451.39 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    84 /   321 | ms/batch 3937.03 | Loss 00.55 |\n",
      "[Training]| Epochs  30 | Batch    86 /   321 | ms/batch 5876.22 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    88 /   321 | ms/batch 5970.47 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch    90 /   321 | ms/batch 4658.26 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    92 /   321 | ms/batch 5446.35 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch    94 /   321 | ms/batch 4645.42 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch    96 /   321 | ms/batch 4481.41 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch    98 /   321 | ms/batch 5660.29 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   100 /   321 | ms/batch 5603.83 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   102 /   321 | ms/batch 4539.79 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   104 /   321 | ms/batch 5734.39 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   106 /   321 | ms/batch 3833.23 | Loss 00.54 |\n",
      "[Training]| Epochs  30 | Batch   108 /   321 | ms/batch 4555.04 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   110 /   321 | ms/batch 5605.79 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch   112 /   321 | ms/batch 3926.40 | Loss 00.53 |\n",
      "[Training]| Epochs  30 | Batch   114 /   321 | ms/batch 4828.58 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   116 /   321 | ms/batch 5123.99 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   118 /   321 | ms/batch 5112.10 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   120 /   321 | ms/batch 5116.03 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   122 /   321 | ms/batch 5259.26 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   124 /   321 | ms/batch 5308.79 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   126 /   321 | ms/batch 4880.97 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   128 /   321 | ms/batch 5342.78 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   130 /   321 | ms/batch 5490.52 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   132 /   321 | ms/batch 3407.73 | Loss 00.54 |\n",
      "[Training]| Epochs  30 | Batch   134 /   321 | ms/batch 5536.92 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   136 /   321 | ms/batch 4767.08 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  30 | Batch   138 /   321 | ms/batch 4032.97 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   140 /   321 | ms/batch 4575.19 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   142 /   321 | ms/batch 6141.28 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   144 /   321 | ms/batch 5422.04 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   146 /   321 | ms/batch 4818.71 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   148 /   321 | ms/batch 6304.95 | Loss 00.61 |\n",
      "[Training]| Epochs  30 | Batch   150 /   321 | ms/batch 4370.84 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   152 /   321 | ms/batch 4824.11 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   154 /   321 | ms/batch 4130.45 | Loss 00.53 |\n",
      "[Training]| Epochs  30 | Batch   156 /   321 | ms/batch 5821.94 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   158 /   321 | ms/batch 5961.38 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   160 /   321 | ms/batch 4591.95 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   162 /   321 | ms/batch 4926.20 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   164 /   321 | ms/batch 3951.15 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   166 /   321 | ms/batch 5068.11 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   168 /   321 | ms/batch 6792.36 | Loss 00.61 |\n",
      "[Training]| Epochs  30 | Batch   170 /   321 | ms/batch 5603.36 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   172 /   321 | ms/batch 5095.20 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   174 /   321 | ms/batch 4576.10 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   176 /   321 | ms/batch 5227.00 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   178 /   321 | ms/batch 6102.52 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   180 /   321 | ms/batch 5288.82 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   182 /   321 | ms/batch 5326.18 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   184 /   321 | ms/batch 4869.02 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   186 /   321 | ms/batch 6020.43 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch   188 /   321 | ms/batch 4541.92 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   190 /   321 | ms/batch 4954.27 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   192 /   321 | ms/batch 5699.69 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   194 /   321 | ms/batch 6444.50 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch   196 /   321 | ms/batch 5973.05 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch   198 /   321 | ms/batch 5047.00 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   200 /   321 | ms/batch 4415.64 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   202 /   321 | ms/batch 4735.89 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   204 /   321 | ms/batch 4503.94 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   206 /   321 | ms/batch 4660.00 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   208 /   321 | ms/batch 4179.06 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   210 /   321 | ms/batch 4077.34 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   212 /   321 | ms/batch 4394.15 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   214 /   321 | ms/batch 5271.57 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   216 /   321 | ms/batch 4438.46 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   218 /   321 | ms/batch 5354.51 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   220 /   321 | ms/batch 5251.72 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   222 /   321 | ms/batch 4526.63 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   224 /   321 | ms/batch 5518.93 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   226 /   321 | ms/batch 5032.77 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   228 /   321 | ms/batch 5027.73 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   230 /   321 | ms/batch 5821.34 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   232 /   321 | ms/batch 4984.33 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   234 /   321 | ms/batch 5333.74 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   236 /   321 | ms/batch 5313.09 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   238 /   321 | ms/batch 4704.21 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   240 /   321 | ms/batch 3889.96 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   242 /   321 | ms/batch 3430.11 | Loss 00.51 |\n",
      "[Training]| Epochs  30 | Batch   244 /   321 | ms/batch 5539.08 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   246 /   321 | ms/batch 5051.54 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   248 /   321 | ms/batch 5439.92 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   250 /   321 | ms/batch 5294.05 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   252 /   321 | ms/batch 5037.59 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   254 /   321 | ms/batch 4584.38 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   256 /   321 | ms/batch 5694.27 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   258 /   321 | ms/batch 4188.78 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   260 /   321 | ms/batch 4443.83 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   262 /   321 | ms/batch 6332.96 | Loss 00.60 |\n",
      "[Training]| Epochs  30 | Batch   264 /   321 | ms/batch 4443.58 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   266 /   321 | ms/batch 4729.20 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   268 /   321 | ms/batch 4982.17 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   270 /   321 | ms/batch 5235.09 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   272 /   321 | ms/batch 5754.02 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   274 /   321 | ms/batch 5658.92 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   276 /   321 | ms/batch 4589.11 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   278 /   321 | ms/batch 6084.25 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   280 /   321 | ms/batch 4690.66 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   282 /   321 | ms/batch 5071.16 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   284 /   321 | ms/batch 4216.08 | Loss 00.56 |\n",
      "[Training]| Epochs  30 | Batch   286 /   321 | ms/batch 5006.01 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   288 /   321 | ms/batch 5763.88 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   290 /   321 | ms/batch 5258.26 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   292 /   321 | ms/batch 4400.59 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   294 /   321 | ms/batch 4969.28 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   296 /   321 | ms/batch 4296.58 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   298 /   321 | ms/batch 5318.12 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   300 /   321 | ms/batch 4314.19 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   302 /   321 | ms/batch 4223.15 | Loss 00.55 |\n",
      "[Training]| Epochs  30 | Batch   304 /   321 | ms/batch 5798.04 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   306 /   321 | ms/batch 6042.33 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   308 /   321 | ms/batch 4595.55 | Loss 00.57 |\n",
      "[Training]| Epochs  30 | Batch   310 /   321 | ms/batch 4278.20 | Loss 00.55 |\n",
      "[Training]| Epochs  30 | Batch   312 /   321 | ms/batch 6076.07 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   314 /   321 | ms/batch 5667.74 | Loss 00.59 |\n",
      "[Training]| Epochs  30 | Batch   316 /   321 | ms/batch 5787.45 | Loss 00.58 |\n",
      "[Training]| Epochs  30 | Batch   318 /   321 | ms/batch 4945.21 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  30 | Batch   320 /   321 | ms/batch 4988.16 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  30 | Elapsed 1445.74 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.55\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.42\n",
      "result= [31, 0.09254721970829477, 0.11968820502027719, 0.17390783225198508, 0.20882943228456255, 0.22833810926691592, 0.11909559789525283, 0.08109924482909014, 0.04038679757716594, 0.029875406694895388, 0.025535143453073242, 0.08418190079746261, 0.07943688122815484, 0.05777032247113698, 0.04765820521817761, 0.042572422874968034, 0.07343652055236574, 0.07873958203177943, 0.08417990401917844, 0.08644000268163811, 0.08751926410599195, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5842724611729752]\n",
      "[Training]| Epochs  31 | Batch     2 /   321 | ms/batch 6992.30 | Loss 00.85 |\n",
      "[Training]| Epochs  31 | Batch     4 /   321 | ms/batch 4948.25 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch     6 /   321 | ms/batch 6167.86 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch     8 /   321 | ms/batch 5656.16 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch    10 /   321 | ms/batch 5430.10 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    12 /   321 | ms/batch 5885.43 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    14 /   321 | ms/batch 4079.75 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    16 /   321 | ms/batch 4280.98 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    18 /   321 | ms/batch 4508.68 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    20 /   321 | ms/batch 4788.60 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    22 /   321 | ms/batch 6368.08 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch    24 /   321 | ms/batch 4910.72 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    26 /   321 | ms/batch 4851.56 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    28 /   321 | ms/batch 5203.23 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    30 /   321 | ms/batch 4638.81 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    32 /   321 | ms/batch 4597.29 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    34 /   321 | ms/batch 4342.72 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch    36 /   321 | ms/batch 5696.96 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    38 /   321 | ms/batch 5523.11 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    40 /   321 | ms/batch 5355.26 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    42 /   321 | ms/batch 5191.73 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    44 /   321 | ms/batch 5161.88 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    46 /   321 | ms/batch 4346.39 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    48 /   321 | ms/batch 5222.33 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    50 /   321 | ms/batch 4577.65 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    52 /   321 | ms/batch 5237.62 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    54 /   321 | ms/batch 4651.99 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    56 /   321 | ms/batch 4829.62 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    58 /   321 | ms/batch 4987.94 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    60 /   321 | ms/batch 4391.44 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    62 /   321 | ms/batch 4396.71 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    64 /   321 | ms/batch 4552.11 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    66 /   321 | ms/batch 4563.06 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    68 /   321 | ms/batch 4780.66 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    70 /   321 | ms/batch 4659.41 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    72 /   321 | ms/batch 3833.96 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch    74 /   321 | ms/batch 4457.29 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    76 /   321 | ms/batch 5064.26 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    78 /   321 | ms/batch 5791.69 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    80 /   321 | ms/batch 5306.01 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    82 /   321 | ms/batch 5438.85 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    84 /   321 | ms/batch 3999.13 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch    86 /   321 | ms/batch 5902.71 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch    88 /   321 | ms/batch 5849.09 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    90 /   321 | ms/batch 4437.16 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    92 /   321 | ms/batch 5282.26 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch    94 /   321 | ms/batch 4936.41 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch    96 /   321 | ms/batch 4388.51 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch    98 /   321 | ms/batch 5349.75 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   100 /   321 | ms/batch 5585.98 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  31 | Batch   102 /   321 | ms/batch 4450.49 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   104 /   321 | ms/batch 5436.17 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   106 /   321 | ms/batch 3833.29 | Loss 00.54 |\n",
      "[Training]| Epochs  31 | Batch   108 /   321 | ms/batch 4228.04 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   110 /   321 | ms/batch 5581.59 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   112 /   321 | ms/batch 3951.47 | Loss 00.54 |\n",
      "[Training]| Epochs  31 | Batch   114 /   321 | ms/batch 4853.70 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   116 /   321 | ms/batch 5286.22 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   118 /   321 | ms/batch 5287.80 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   120 /   321 | ms/batch 5011.77 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   122 /   321 | ms/batch 5153.88 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   124 /   321 | ms/batch 5124.77 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   126 /   321 | ms/batch 5146.83 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   128 /   321 | ms/batch 5492.76 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   130 /   321 | ms/batch 5557.90 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   132 /   321 | ms/batch 3361.49 | Loss 00.54 |\n",
      "[Training]| Epochs  31 | Batch   134 /   321 | ms/batch 5469.98 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   136 /   321 | ms/batch 4851.96 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   138 /   321 | ms/batch 4032.43 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   140 /   321 | ms/batch 4863.48 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch   142 /   321 | ms/batch 6044.23 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   144 /   321 | ms/batch 5288.32 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   146 /   321 | ms/batch 4959.04 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   148 /   321 | ms/batch 6128.06 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   150 /   321 | ms/batch 4418.35 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   152 /   321 | ms/batch 4901.88 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   154 /   321 | ms/batch 3929.91 | Loss 00.54 |\n",
      "[Training]| Epochs  31 | Batch   156 /   321 | ms/batch 5690.51 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   158 /   321 | ms/batch 6141.89 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   160 /   321 | ms/batch 4769.28 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   162 /   321 | ms/batch 5211.37 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   164 /   321 | ms/batch 3882.97 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   166 /   321 | ms/batch 4840.42 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   168 /   321 | ms/batch 6850.42 | Loss 00.61 |\n",
      "[Training]| Epochs  31 | Batch   170 /   321 | ms/batch 5265.53 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   172 /   321 | ms/batch 5222.40 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   174 /   321 | ms/batch 4765.24 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   176 /   321 | ms/batch 4955.49 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   178 /   321 | ms/batch 6151.82 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   180 /   321 | ms/batch 5526.62 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   182 /   321 | ms/batch 5512.17 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   184 /   321 | ms/batch 4844.65 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   186 /   321 | ms/batch 6305.70 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   188 /   321 | ms/batch 4532.81 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   190 /   321 | ms/batch 5007.29 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   192 /   321 | ms/batch 5561.87 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   194 /   321 | ms/batch 6447.67 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   196 /   321 | ms/batch 6105.52 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   198 /   321 | ms/batch 5092.32 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   200 /   321 | ms/batch 4264.24 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   202 /   321 | ms/batch 4540.67 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   204 /   321 | ms/batch 4319.77 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   206 /   321 | ms/batch 4911.11 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   208 /   321 | ms/batch 4481.33 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch   210 /   321 | ms/batch 4093.19 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   212 /   321 | ms/batch 4642.66 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   214 /   321 | ms/batch 5038.93 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   216 /   321 | ms/batch 4325.84 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   218 /   321 | ms/batch 5434.25 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   220 /   321 | ms/batch 5003.36 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   222 /   321 | ms/batch 4349.93 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   224 /   321 | ms/batch 5436.47 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   226 /   321 | ms/batch 4932.48 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   228 /   321 | ms/batch 4868.57 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   230 /   321 | ms/batch 5747.29 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   232 /   321 | ms/batch 4776.56 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   234 /   321 | ms/batch 5381.64 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   236 /   321 | ms/batch 5175.36 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   238 /   321 | ms/batch 4654.45 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   240 /   321 | ms/batch 3990.49 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   242 /   321 | ms/batch 3534.44 | Loss 00.52 |\n",
      "[Training]| Epochs  31 | Batch   244 /   321 | ms/batch 5516.27 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   246 /   321 | ms/batch 4984.07 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   248 /   321 | ms/batch 5361.92 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   250 /   321 | ms/batch 5284.18 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   252 /   321 | ms/batch 5051.86 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   254 /   321 | ms/batch 4337.29 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   256 /   321 | ms/batch 5582.24 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   258 /   321 | ms/batch 4343.27 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   260 /   321 | ms/batch 4620.46 | Loss 00.56 |\n",
      "[Training]| Epochs  31 | Batch   262 /   321 | ms/batch 6165.45 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   264 /   321 | ms/batch 4360.67 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   266 /   321 | ms/batch 4878.89 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   268 /   321 | ms/batch 5137.36 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   270 /   321 | ms/batch 5135.01 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   272 /   321 | ms/batch 5678.16 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   274 /   321 | ms/batch 5695.61 | Loss 00.60 |\n",
      "[Training]| Epochs  31 | Batch   276 /   321 | ms/batch 4482.60 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   278 /   321 | ms/batch 5738.69 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   280 /   321 | ms/batch 4742.29 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   282 /   321 | ms/batch 4829.99 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   284 /   321 | ms/batch 4213.97 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch   286 /   321 | ms/batch 5286.97 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   288 /   321 | ms/batch 6026.75 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   290 /   321 | ms/batch 5311.41 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   292 /   321 | ms/batch 4135.62 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   294 /   321 | ms/batch 4773.26 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   296 /   321 | ms/batch 4306.99 | Loss 00.57 |\n",
      "[Training]| Epochs  31 | Batch   298 /   321 | ms/batch 5205.70 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   300 /   321 | ms/batch 4303.99 | Loss 00.58 |\n",
      "[Training]| Epochs  31 | Batch   302 /   321 | ms/batch 4100.18 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch   304 /   321 | ms/batch 5803.58 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   306 /   321 | ms/batch 6061.78 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   308 /   321 | ms/batch 4348.35 | Loss 00.56 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  31 | Batch   310 /   321 | ms/batch 4253.99 | Loss 00.55 |\n",
      "[Training]| Epochs  31 | Batch   312 /   321 | ms/batch 5919.81 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   314 /   321 | ms/batch 6002.53 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   316 /   321 | ms/batch 5669.63 | Loss 00.59 |\n",
      "[Training]| Epochs  31 | Batch   318 /   321 | ms/batch 4935.63 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  31 | Batch   320 /   321 | ms/batch 6374.26 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  31 | Elapsed 1451.24 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.33\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.54\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.39\n",
      "result= [32, 0.09257618524279337, 0.11790120220704647, 0.17433008326279958, 0.2062441914321293, 0.2322142460526476, 0.11932943913496201, 0.08074849486258098, 0.040542682817181104, 0.029485695081489308, 0.025715005091178817, 0.08424978040663972, 0.07882855148493825, 0.05795809596382076, 0.04706563377231708, 0.0428976622393423, 0.08305319936964908, 0.08815137616744147, 0.09380099006122035, 0.09588183735283883, 0.09723558151293822, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.584953350049478]\n",
      "[Training]| Epochs  32 | Batch     2 /   321 | ms/batch 6998.40 | Loss 00.85 |\n",
      "[Training]| Epochs  32 | Batch     4 /   321 | ms/batch 4805.94 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch     6 /   321 | ms/batch 6285.47 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch     8 /   321 | ms/batch 5822.48 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch    10 /   321 | ms/batch 5593.04 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    12 /   321 | ms/batch 5759.48 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    14 /   321 | ms/batch 4168.69 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    16 /   321 | ms/batch 4301.84 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch    18 /   321 | ms/batch 4528.38 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    20 /   321 | ms/batch 4816.25 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    22 /   321 | ms/batch 6672.69 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch    24 /   321 | ms/batch 4998.06 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    26 /   321 | ms/batch 4759.05 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    28 /   321 | ms/batch 5399.59 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    30 /   321 | ms/batch 4770.25 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    32 /   321 | ms/batch 4668.73 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    34 /   321 | ms/batch 4394.77 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch    36 /   321 | ms/batch 5632.61 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    38 /   321 | ms/batch 5458.53 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    40 /   321 | ms/batch 5427.85 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    42 /   321 | ms/batch 5182.12 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    44 /   321 | ms/batch 5136.02 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    46 /   321 | ms/batch 4489.64 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    48 /   321 | ms/batch 5295.93 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    50 /   321 | ms/batch 4533.56 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    52 /   321 | ms/batch 5089.68 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    54 /   321 | ms/batch 4542.70 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    56 /   321 | ms/batch 4984.17 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    58 /   321 | ms/batch 4993.29 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    60 /   321 | ms/batch 4306.90 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    62 /   321 | ms/batch 4527.24 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    64 /   321 | ms/batch 4737.66 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  32 | Batch    66 /   321 | ms/batch 4528.18 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    68 /   321 | ms/batch 4885.49 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    70 /   321 | ms/batch 4705.62 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    72 /   321 | ms/batch 4076.19 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch    74 /   321 | ms/batch 4473.64 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    76 /   321 | ms/batch 4984.74 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    78 /   321 | ms/batch 5784.22 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    80 /   321 | ms/batch 5029.56 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    82 /   321 | ms/batch 5411.88 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch    84 /   321 | ms/batch 4012.87 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch    86 /   321 | ms/batch 5943.29 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch    88 /   321 | ms/batch 5838.31 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    90 /   321 | ms/batch 4657.68 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    92 /   321 | ms/batch 5476.76 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch    94 /   321 | ms/batch 4926.02 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch    96 /   321 | ms/batch 4292.57 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch    98 /   321 | ms/batch 5957.43 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   100 /   321 | ms/batch 5894.07 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   102 /   321 | ms/batch 4395.51 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   104 /   321 | ms/batch 5772.71 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   106 /   321 | ms/batch 3825.14 | Loss 00.54 |\n",
      "[Training]| Epochs  32 | Batch   108 /   321 | ms/batch 4294.62 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   110 /   321 | ms/batch 5721.81 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   112 /   321 | ms/batch 3670.29 | Loss 00.54 |\n",
      "[Training]| Epochs  32 | Batch   114 /   321 | ms/batch 4769.65 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   116 /   321 | ms/batch 5327.76 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   118 /   321 | ms/batch 5115.44 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   120 /   321 | ms/batch 4871.46 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   122 /   321 | ms/batch 5190.31 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   124 /   321 | ms/batch 5349.01 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   126 /   321 | ms/batch 5050.86 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   128 /   321 | ms/batch 5535.78 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   130 /   321 | ms/batch 5700.40 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   132 /   321 | ms/batch 3373.10 | Loss 00.54 |\n",
      "[Training]| Epochs  32 | Batch   134 /   321 | ms/batch 5665.61 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   136 /   321 | ms/batch 4904.69 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   138 /   321 | ms/batch 4013.83 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   140 /   321 | ms/batch 4876.74 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   142 /   321 | ms/batch 6289.26 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   144 /   321 | ms/batch 5637.37 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   146 /   321 | ms/batch 4945.06 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   148 /   321 | ms/batch 6336.02 | Loss 00.61 |\n",
      "[Training]| Epochs  32 | Batch   150 /   321 | ms/batch 4539.22 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   152 /   321 | ms/batch 5043.84 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   154 /   321 | ms/batch 3791.84 | Loss 00.54 |\n",
      "[Training]| Epochs  32 | Batch   156 /   321 | ms/batch 6143.81 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   158 /   321 | ms/batch 5966.38 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   160 /   321 | ms/batch 4599.72 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   162 /   321 | ms/batch 4861.78 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   164 /   321 | ms/batch 3942.89 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   166 /   321 | ms/batch 5055.91 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   168 /   321 | ms/batch 6906.57 | Loss 00.61 |\n",
      "[Training]| Epochs  32 | Batch   170 /   321 | ms/batch 5642.53 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   172 /   321 | ms/batch 5276.53 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   174 /   321 | ms/batch 4451.94 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   176 /   321 | ms/batch 5279.18 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   178 /   321 | ms/batch 6156.02 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   180 /   321 | ms/batch 5353.18 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   182 /   321 | ms/batch 5269.54 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   184 /   321 | ms/batch 4769.44 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   186 /   321 | ms/batch 6271.46 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   188 /   321 | ms/batch 4490.37 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   190 /   321 | ms/batch 4918.24 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   192 /   321 | ms/batch 5512.94 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   194 /   321 | ms/batch 6246.69 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   196 /   321 | ms/batch 6338.65 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   198 /   321 | ms/batch 5163.03 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   200 /   321 | ms/batch 4411.96 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   202 /   321 | ms/batch 4715.46 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   204 /   321 | ms/batch 4463.80 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   206 /   321 | ms/batch 4704.51 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   208 /   321 | ms/batch 4221.29 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch   210 /   321 | ms/batch 4197.60 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch   212 /   321 | ms/batch 4529.50 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   214 /   321 | ms/batch 5052.85 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   216 /   321 | ms/batch 4595.67 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   218 /   321 | ms/batch 5599.66 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   220 /   321 | ms/batch 5147.59 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   222 /   321 | ms/batch 4522.54 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   224 /   321 | ms/batch 5246.58 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   226 /   321 | ms/batch 5189.89 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   228 /   321 | ms/batch 5344.20 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   230 /   321 | ms/batch 5995.07 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   232 /   321 | ms/batch 4809.27 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   234 /   321 | ms/batch 5308.96 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   236 /   321 | ms/batch 5279.46 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   238 /   321 | ms/batch 4860.77 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   240 /   321 | ms/batch 4119.57 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   242 /   321 | ms/batch 3610.83 | Loss 00.51 |\n",
      "[Training]| Epochs  32 | Batch   244 /   321 | ms/batch 5386.56 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   246 /   321 | ms/batch 4834.57 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   248 /   321 | ms/batch 5676.54 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   250 /   321 | ms/batch 5201.59 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   252 /   321 | ms/batch 5080.04 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   254 /   321 | ms/batch 4693.92 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   256 /   321 | ms/batch 5351.15 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   258 /   321 | ms/batch 3903.98 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   260 /   321 | ms/batch 4535.65 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   262 /   321 | ms/batch 6269.33 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   264 /   321 | ms/batch 4385.56 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   266 /   321 | ms/batch 4738.78 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   268 /   321 | ms/batch 5233.42 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   270 /   321 | ms/batch 5069.42 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   272 /   321 | ms/batch 5937.94 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  32 | Batch   274 /   321 | ms/batch 5462.47 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   276 /   321 | ms/batch 4571.95 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   278 /   321 | ms/batch 5815.20 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   280 /   321 | ms/batch 4806.07 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   282 /   321 | ms/batch 4991.35 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   284 /   321 | ms/batch 4384.14 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   286 /   321 | ms/batch 5248.17 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   288 /   321 | ms/batch 5861.03 | Loss 00.60 |\n",
      "[Training]| Epochs  32 | Batch   290 /   321 | ms/batch 5293.81 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   292 /   321 | ms/batch 4101.74 | Loss 00.57 |\n",
      "[Training]| Epochs  32 | Batch   294 /   321 | ms/batch 5119.68 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   296 /   321 | ms/batch 4242.13 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   298 /   321 | ms/batch 5104.22 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   300 /   321 | ms/batch 4443.55 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   302 /   321 | ms/batch 4315.56 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch   304 /   321 | ms/batch 5807.79 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   306 /   321 | ms/batch 6128.89 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   308 /   321 | ms/batch 4642.96 | Loss 00.56 |\n",
      "[Training]| Epochs  32 | Batch   310 /   321 | ms/batch 4402.74 | Loss 00.55 |\n",
      "[Training]| Epochs  32 | Batch   312 /   321 | ms/batch 5962.11 | Loss 00.58 |\n",
      "[Training]| Epochs  32 | Batch   314 /   321 | ms/batch 6177.79 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   316 /   321 | ms/batch 5743.45 | Loss 00.59 |\n",
      "[Training]| Epochs  32 | Batch   318 /   321 | ms/batch 4867.34 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  32 | Batch   320 /   321 | ms/batch 5328.22 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  32 | Elapsed 1441.83 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.80\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.86\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.55\n",
      "result= [33, 0.09284628840600338, 0.11910228179194271, 0.17433377010972817, 0.20789154599814888, 0.2289717950025575, 0.11948535708087733, 0.08090441280849632, 0.04064660730161243, 0.02992995418985654, 0.02552915232681429, 0.08445270565089451, 0.07927945881083155, 0.05807398188792442, 0.047744447703669986, 0.042569152284950736, 0.08387891013198477, 0.08908271653627046, 0.09461350332465043, 0.09682038272509608, 0.09793253944619085, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5843595339928145]\n",
      "[Training]| Epochs  33 | Batch     2 /   321 | ms/batch 6852.92 | Loss 00.85 |\n",
      "[Training]| Epochs  33 | Batch     4 /   321 | ms/batch 4876.82 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch     6 /   321 | ms/batch 6324.09 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch     8 /   321 | ms/batch 5738.29 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch    10 /   321 | ms/batch 5537.10 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    12 /   321 | ms/batch 5500.93 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    14 /   321 | ms/batch 3893.39 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    16 /   321 | ms/batch 4344.38 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    18 /   321 | ms/batch 4551.19 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    20 /   321 | ms/batch 4874.37 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    22 /   321 | ms/batch 6367.14 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch    24 /   321 | ms/batch 4946.31 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    26 /   321 | ms/batch 4841.74 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    28 /   321 | ms/batch 5069.93 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  33 | Batch    30 /   321 | ms/batch 4901.29 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    32 /   321 | ms/batch 4526.13 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    34 /   321 | ms/batch 4358.48 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    36 /   321 | ms/batch 5582.00 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch    38 /   321 | ms/batch 5507.76 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    40 /   321 | ms/batch 5453.03 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    42 /   321 | ms/batch 5250.17 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch    44 /   321 | ms/batch 5152.22 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    46 /   321 | ms/batch 4346.60 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    48 /   321 | ms/batch 5200.14 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    50 /   321 | ms/batch 4827.09 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    52 /   321 | ms/batch 5123.12 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    54 /   321 | ms/batch 4617.97 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    56 /   321 | ms/batch 5081.15 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    58 /   321 | ms/batch 4906.85 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    60 /   321 | ms/batch 4348.32 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    62 /   321 | ms/batch 4578.54 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    64 /   321 | ms/batch 4717.15 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    66 /   321 | ms/batch 4297.59 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    68 /   321 | ms/batch 4590.56 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    70 /   321 | ms/batch 4562.17 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    72 /   321 | ms/batch 3821.40 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch    74 /   321 | ms/batch 4452.87 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    76 /   321 | ms/batch 4956.72 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    78 /   321 | ms/batch 5445.33 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch    80 /   321 | ms/batch 5018.86 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    82 /   321 | ms/batch 5397.58 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    84 /   321 | ms/batch 4038.55 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch    86 /   321 | ms/batch 5804.00 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch    88 /   321 | ms/batch 5915.30 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    90 /   321 | ms/batch 4518.95 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch    92 /   321 | ms/batch 5514.53 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch    94 /   321 | ms/batch 4911.29 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch    96 /   321 | ms/batch 4683.30 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch    98 /   321 | ms/batch 5794.98 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   100 /   321 | ms/batch 5723.49 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   102 /   321 | ms/batch 4495.84 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   104 /   321 | ms/batch 5536.35 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   106 /   321 | ms/batch 3804.33 | Loss 00.54 |\n",
      "[Training]| Epochs  33 | Batch   108 /   321 | ms/batch 4282.13 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   110 /   321 | ms/batch 5752.17 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   112 /   321 | ms/batch 3868.07 | Loss 00.54 |\n",
      "[Training]| Epochs  33 | Batch   114 /   321 | ms/batch 4845.15 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   116 /   321 | ms/batch 5383.83 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   118 /   321 | ms/batch 5237.07 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   120 /   321 | ms/batch 4992.79 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   122 /   321 | ms/batch 5218.60 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   124 /   321 | ms/batch 5144.04 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   126 /   321 | ms/batch 4972.67 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   128 /   321 | ms/batch 5232.14 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   130 /   321 | ms/batch 5601.18 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   132 /   321 | ms/batch 3270.82 | Loss 00.53 |\n",
      "[Training]| Epochs  33 | Batch   134 /   321 | ms/batch 5518.73 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   136 /   321 | ms/batch 4596.49 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   138 /   321 | ms/batch 4254.95 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   140 /   321 | ms/batch 4618.36 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch   142 /   321 | ms/batch 6273.43 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   144 /   321 | ms/batch 5460.17 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   146 /   321 | ms/batch 4946.37 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   148 /   321 | ms/batch 6447.67 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   150 /   321 | ms/batch 4462.79 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   152 /   321 | ms/batch 4917.01 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   154 /   321 | ms/batch 3979.67 | Loss 00.54 |\n",
      "[Training]| Epochs  33 | Batch   156 /   321 | ms/batch 5880.86 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   158 /   321 | ms/batch 5915.32 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   160 /   321 | ms/batch 4368.45 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   162 /   321 | ms/batch 4761.73 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   164 /   321 | ms/batch 3823.28 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   166 /   321 | ms/batch 5122.96 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   168 /   321 | ms/batch 6752.22 | Loss 00.61 |\n",
      "[Training]| Epochs  33 | Batch   170 /   321 | ms/batch 5690.31 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   172 /   321 | ms/batch 5114.92 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   174 /   321 | ms/batch 4705.70 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   176 /   321 | ms/batch 5229.01 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   178 /   321 | ms/batch 6070.36 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   180 /   321 | ms/batch 5029.70 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   182 /   321 | ms/batch 5461.82 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   184 /   321 | ms/batch 4896.81 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   186 /   321 | ms/batch 5949.89 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   188 /   321 | ms/batch 4345.00 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   190 /   321 | ms/batch 4888.72 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   192 /   321 | ms/batch 5853.01 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   194 /   321 | ms/batch 6334.07 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   196 /   321 | ms/batch 6322.43 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   198 /   321 | ms/batch 5094.36 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   200 /   321 | ms/batch 4259.20 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   202 /   321 | ms/batch 4616.51 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   204 /   321 | ms/batch 4517.72 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   206 /   321 | ms/batch 4437.24 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   208 /   321 | ms/batch 4388.58 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   210 /   321 | ms/batch 4030.66 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   212 /   321 | ms/batch 4189.36 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   214 /   321 | ms/batch 5035.00 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   216 /   321 | ms/batch 4481.67 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   218 /   321 | ms/batch 5455.43 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   220 /   321 | ms/batch 5311.24 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   222 /   321 | ms/batch 4247.28 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   224 /   321 | ms/batch 5635.70 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   226 /   321 | ms/batch 4972.87 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   228 /   321 | ms/batch 4919.90 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   230 /   321 | ms/batch 5822.72 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   232 /   321 | ms/batch 5107.85 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   234 /   321 | ms/batch 5367.03 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   236 /   321 | ms/batch 5470.23 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  33 | Batch   238 /   321 | ms/batch 4555.06 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   240 /   321 | ms/batch 3936.71 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   242 /   321 | ms/batch 3417.00 | Loss 00.51 |\n",
      "[Training]| Epochs  33 | Batch   244 /   321 | ms/batch 5611.81 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   246 /   321 | ms/batch 5081.62 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   248 /   321 | ms/batch 5568.41 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   250 /   321 | ms/batch 5427.10 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   252 /   321 | ms/batch 5139.84 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   254 /   321 | ms/batch 4649.70 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   256 /   321 | ms/batch 5637.77 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   258 /   321 | ms/batch 4148.74 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   260 /   321 | ms/batch 4501.27 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   262 /   321 | ms/batch 6067.70 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   264 /   321 | ms/batch 4331.33 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   266 /   321 | ms/batch 4704.66 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   268 /   321 | ms/batch 4991.42 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   270 /   321 | ms/batch 4985.00 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   272 /   321 | ms/batch 5834.22 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   274 /   321 | ms/batch 5825.35 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   276 /   321 | ms/batch 4257.17 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   278 /   321 | ms/batch 5861.80 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   280 /   321 | ms/batch 4804.01 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   282 /   321 | ms/batch 4847.73 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   284 /   321 | ms/batch 4403.65 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch   286 /   321 | ms/batch 5229.84 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   288 /   321 | ms/batch 6096.42 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   290 /   321 | ms/batch 5204.15 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   292 /   321 | ms/batch 4096.21 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   294 /   321 | ms/batch 4783.90 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   296 /   321 | ms/batch 4068.81 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   298 /   321 | ms/batch 5113.36 | Loss 00.58 |\n",
      "[Training]| Epochs  33 | Batch   300 /   321 | ms/batch 4291.94 | Loss 00.57 |\n",
      "[Training]| Epochs  33 | Batch   302 /   321 | ms/batch 4107.18 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch   304 /   321 | ms/batch 5898.32 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   306 /   321 | ms/batch 6300.24 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   308 /   321 | ms/batch 4484.73 | Loss 00.56 |\n",
      "[Training]| Epochs  33 | Batch   310 /   321 | ms/batch 4347.56 | Loss 00.55 |\n",
      "[Training]| Epochs  33 | Batch   312 /   321 | ms/batch 5958.43 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   314 /   321 | ms/batch 5863.17 | Loss 00.60 |\n",
      "[Training]| Epochs  33 | Batch   316 /   321 | ms/batch 5870.49 | Loss 00.59 |\n",
      "[Training]| Epochs  33 | Batch   318 /   321 | ms/batch 4842.82 | Loss 00.57 |\n",
      "32\n",
      "[Training]| Epochs  33 | Batch   320 /   321 | ms/batch 5541.25 | Loss 00.61 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  33 | Elapsed 1438.47 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.96\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.78\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.42\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.61\n",
      "result= [34, 0.09206015154986481, 0.1193596593867218, 0.17589698131439374, 0.209373420602348, 0.2302928041501242, 0.11886178044165287, 0.07950140699593239, 0.041179205046346756, 0.029610390785794097, 0.025439228940920634, 0.08390517320283576, 0.0785359942346277, 0.05880775362462247, 0.04727247777806533, 0.042450299043722294, 0.08311078663398559, 0.088343183107497, 0.09414982755791138, 0.09621982579863546, 0.09736242547971709, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5854087935553657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  34 | Batch     2 /   321 | ms/batch 6712.22 | Loss 00.85 |\n",
      "[Training]| Epochs  34 | Batch     4 /   321 | ms/batch 4798.51 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch     6 /   321 | ms/batch 6271.91 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch     8 /   321 | ms/batch 5717.09 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch    10 /   321 | ms/batch 5429.76 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    12 /   321 | ms/batch 5645.27 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    14 /   321 | ms/batch 3748.02 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    16 /   321 | ms/batch 4217.35 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch    18 /   321 | ms/batch 4585.67 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    20 /   321 | ms/batch 5049.03 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    22 /   321 | ms/batch 6130.72 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch    24 /   321 | ms/batch 4906.16 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    26 /   321 | ms/batch 4583.80 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    28 /   321 | ms/batch 4991.68 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    30 /   321 | ms/batch 4611.84 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    32 /   321 | ms/batch 4456.54 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    34 /   321 | ms/batch 4353.20 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    36 /   321 | ms/batch 5623.05 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    38 /   321 | ms/batch 5634.85 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    40 /   321 | ms/batch 5363.62 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    42 /   321 | ms/batch 5104.54 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    44 /   321 | ms/batch 5228.27 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    46 /   321 | ms/batch 4579.63 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    48 /   321 | ms/batch 5262.15 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    50 /   321 | ms/batch 4622.88 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    52 /   321 | ms/batch 5154.20 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    54 /   321 | ms/batch 4777.01 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    56 /   321 | ms/batch 5111.88 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    58 /   321 | ms/batch 4874.92 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    60 /   321 | ms/batch 4317.52 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    62 /   321 | ms/batch 4299.70 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    64 /   321 | ms/batch 4467.47 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    66 /   321 | ms/batch 4208.92 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    68 /   321 | ms/batch 4519.24 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    70 /   321 | ms/batch 4740.36 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    72 /   321 | ms/batch 3818.43 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch    74 /   321 | ms/batch 4342.03 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    76 /   321 | ms/batch 4990.25 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    78 /   321 | ms/batch 5583.80 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    80 /   321 | ms/batch 4859.75 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    82 /   321 | ms/batch 5427.11 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    84 /   321 | ms/batch 3752.44 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch    86 /   321 | ms/batch 5643.51 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    88 /   321 | ms/batch 5805.90 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch    90 /   321 | ms/batch 4465.72 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    92 /   321 | ms/batch 5247.65 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch    94 /   321 | ms/batch 4753.21 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch    96 /   321 | ms/batch 4287.26 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch    98 /   321 | ms/batch 5692.90 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   100 /   321 | ms/batch 5492.42 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   102 /   321 | ms/batch 4325.50 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   104 /   321 | ms/batch 5511.92 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   106 /   321 | ms/batch 3798.59 | Loss 00.54 |\n",
      "[Training]| Epochs  34 | Batch   108 /   321 | ms/batch 4430.99 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   110 /   321 | ms/batch 5583.01 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   112 /   321 | ms/batch 3880.13 | Loss 00.53 |\n",
      "[Training]| Epochs  34 | Batch   114 /   321 | ms/batch 4779.20 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   116 /   321 | ms/batch 5184.45 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   118 /   321 | ms/batch 5255.03 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   120 /   321 | ms/batch 4804.30 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   122 /   321 | ms/batch 5058.88 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   124 /   321 | ms/batch 5403.85 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   126 /   321 | ms/batch 4857.82 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   128 /   321 | ms/batch 5358.38 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   130 /   321 | ms/batch 5687.90 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   132 /   321 | ms/batch 3133.05 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch   134 /   321 | ms/batch 5261.38 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   136 /   321 | ms/batch 4680.63 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   138 /   321 | ms/batch 4153.43 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   140 /   321 | ms/batch 4847.55 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   142 /   321 | ms/batch 6331.82 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   144 /   321 | ms/batch 5468.67 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   146 /   321 | ms/batch 4891.52 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   148 /   321 | ms/batch 6208.46 | Loss 00.61 |\n",
      "[Training]| Epochs  34 | Batch   150 /   321 | ms/batch 4199.36 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   152 /   321 | ms/batch 4974.33 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   154 /   321 | ms/batch 4235.36 | Loss 00.54 |\n",
      "[Training]| Epochs  34 | Batch   156 /   321 | ms/batch 5619.22 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   158 /   321 | ms/batch 5816.92 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   160 /   321 | ms/batch 4353.15 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   162 /   321 | ms/batch 4786.43 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   164 /   321 | ms/batch 4013.80 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch   166 /   321 | ms/batch 4813.32 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   168 /   321 | ms/batch 6928.09 | Loss 00.61 |\n",
      "[Training]| Epochs  34 | Batch   170 /   321 | ms/batch 5598.22 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   172 /   321 | ms/batch 4997.83 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   174 /   321 | ms/batch 4469.88 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   176 /   321 | ms/batch 5296.49 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   178 /   321 | ms/batch 6353.63 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   180 /   321 | ms/batch 5186.45 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   182 /   321 | ms/batch 5207.95 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   184 /   321 | ms/batch 4815.14 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   186 /   321 | ms/batch 6140.06 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   188 /   321 | ms/batch 4346.37 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   190 /   321 | ms/batch 4935.21 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   192 /   321 | ms/batch 5571.38 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   194 /   321 | ms/batch 6383.11 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   196 /   321 | ms/batch 6113.42 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   198 /   321 | ms/batch 5257.45 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   200 /   321 | ms/batch 4456.20 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   202 /   321 | ms/batch 4806.31 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   204 /   321 | ms/batch 4522.12 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   206 /   321 | ms/batch 4712.79 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   208 /   321 | ms/batch 4042.10 | Loss 00.55 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  34 | Batch   210 /   321 | ms/batch 4255.77 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   212 /   321 | ms/batch 4712.12 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   214 /   321 | ms/batch 5334.04 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   216 /   321 | ms/batch 4615.43 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   218 /   321 | ms/batch 5228.05 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   220 /   321 | ms/batch 5088.51 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   222 /   321 | ms/batch 4533.58 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   224 /   321 | ms/batch 5606.15 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   226 /   321 | ms/batch 5032.39 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   228 /   321 | ms/batch 4877.93 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   230 /   321 | ms/batch 5631.76 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   232 /   321 | ms/batch 4655.73 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   234 /   321 | ms/batch 5536.74 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   236 /   321 | ms/batch 5353.04 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   238 /   321 | ms/batch 4772.06 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   240 /   321 | ms/batch 3997.41 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   242 /   321 | ms/batch 3487.32 | Loss 00.51 |\n",
      "[Training]| Epochs  34 | Batch   244 /   321 | ms/batch 5763.19 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   246 /   321 | ms/batch 4886.47 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   248 /   321 | ms/batch 5538.60 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   250 /   321 | ms/batch 5230.00 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   252 /   321 | ms/batch 4934.63 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   254 /   321 | ms/batch 4712.89 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   256 /   321 | ms/batch 5589.90 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   258 /   321 | ms/batch 4046.36 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch   260 /   321 | ms/batch 4482.71 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   262 /   321 | ms/batch 6107.25 | Loss 00.60 |\n",
      "[Training]| Epochs  34 | Batch   264 /   321 | ms/batch 4539.95 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   266 /   321 | ms/batch 4865.15 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   268 /   321 | ms/batch 5136.07 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   270 /   321 | ms/batch 5136.79 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   272 /   321 | ms/batch 5865.70 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   274 /   321 | ms/batch 5557.50 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   276 /   321 | ms/batch 4454.14 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   278 /   321 | ms/batch 5914.94 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   280 /   321 | ms/batch 4887.80 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   282 /   321 | ms/batch 5084.42 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   284 /   321 | ms/batch 4068.37 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch   286 /   321 | ms/batch 5097.15 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   288 /   321 | ms/batch 5814.20 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   290 /   321 | ms/batch 5406.24 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   292 /   321 | ms/batch 4451.02 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   294 /   321 | ms/batch 4893.47 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   296 /   321 | ms/batch 4235.50 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   298 /   321 | ms/batch 5012.11 | Loss 00.58 |\n",
      "[Training]| Epochs  34 | Batch   300 /   321 | ms/batch 4409.04 | Loss 00.57 |\n",
      "[Training]| Epochs  34 | Batch   302 /   321 | ms/batch 3925.05 | Loss 00.55 |\n",
      "[Training]| Epochs  34 | Batch   304 /   321 | ms/batch 5605.62 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   306 /   321 | ms/batch 6313.57 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   308 /   321 | ms/batch 4439.06 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   310 /   321 | ms/batch 4392.51 | Loss 00.56 |\n",
      "[Training]| Epochs  34 | Batch   312 /   321 | ms/batch 5958.44 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   314 /   321 | ms/batch 5834.01 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   316 /   321 | ms/batch 5939.98 | Loss 00.59 |\n",
      "[Training]| Epochs  34 | Batch   318 /   321 | ms/batch 4780.50 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  34 | Batch   320 /   321 | ms/batch 5653.88 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  34 | Elapsed 1454.45 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.89\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.40\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.58\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.45\n",
      "result= [35, 0.09237943249388031, 0.11771548026628263, 0.17379433683185772, 0.20859670899198657, 0.23030219966326482, 0.11917354497515588, 0.07802050762221113, 0.040776512163835675, 0.02973510135641715, 0.02570900207186526, 0.08420222602778826, 0.07723497136533028, 0.058233426124531126, 0.047427042889018965, 0.04286376406044488, 0.07981035138523193, 0.08471861171854815, 0.09055096344911259, 0.09274527097131044, 0.09394180226166582, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847541270432649]\n",
      "[Training]| Epochs  35 | Batch     2 /   321 | ms/batch 6834.25 | Loss 00.85 |\n",
      "[Training]| Epochs  35 | Batch     4 /   321 | ms/batch 4650.57 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch     6 /   321 | ms/batch 6277.23 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch     8 /   321 | ms/batch 5561.34 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch    10 /   321 | ms/batch 5451.88 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    12 /   321 | ms/batch 5779.41 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    14 /   321 | ms/batch 4169.96 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    16 /   321 | ms/batch 4130.27 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch    18 /   321 | ms/batch 4685.59 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    20 /   321 | ms/batch 4823.23 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    22 /   321 | ms/batch 6323.46 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch    24 /   321 | ms/batch 4792.74 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    26 /   321 | ms/batch 4844.67 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    28 /   321 | ms/batch 4903.31 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    30 /   321 | ms/batch 4876.57 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    32 /   321 | ms/batch 4535.57 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    34 /   321 | ms/batch 4224.77 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch    36 /   321 | ms/batch 5491.39 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch    38 /   321 | ms/batch 5514.06 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    40 /   321 | ms/batch 5406.06 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch    42 /   321 | ms/batch 5178.21 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch    44 /   321 | ms/batch 5260.06 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    46 /   321 | ms/batch 4430.98 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    48 /   321 | ms/batch 4988.62 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    50 /   321 | ms/batch 4480.11 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    52 /   321 | ms/batch 5037.85 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    54 /   321 | ms/batch 4761.45 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch    56 /   321 | ms/batch 5307.82 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    58 /   321 | ms/batch 4923.19 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    60 /   321 | ms/batch 4123.64 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch    62 /   321 | ms/batch 4257.82 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    64 /   321 | ms/batch 4325.46 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    66 /   321 | ms/batch 4244.83 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    68 /   321 | ms/batch 4557.68 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    70 /   321 | ms/batch 4566.70 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    72 /   321 | ms/batch 3973.00 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    74 /   321 | ms/batch 4392.37 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    76 /   321 | ms/batch 4938.33 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    78 /   321 | ms/batch 5518.06 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    80 /   321 | ms/batch 4874.59 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    82 /   321 | ms/batch 5476.91 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    84 /   321 | ms/batch 4109.83 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    86 /   321 | ms/batch 5726.66 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch    88 /   321 | ms/batch 6003.66 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    90 /   321 | ms/batch 4456.57 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    92 /   321 | ms/batch 5436.70 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch    94 /   321 | ms/batch 4988.47 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch    96 /   321 | ms/batch 4279.27 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch    98 /   321 | ms/batch 5691.82 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   100 /   321 | ms/batch 5411.73 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   102 /   321 | ms/batch 4565.91 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   104 /   321 | ms/batch 5521.38 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   106 /   321 | ms/batch 3992.24 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch   108 /   321 | ms/batch 4327.38 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   110 /   321 | ms/batch 5392.43 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   112 /   321 | ms/batch 3795.40 | Loss 00.54 |\n",
      "[Training]| Epochs  35 | Batch   114 /   321 | ms/batch 5094.92 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   116 /   321 | ms/batch 5321.20 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   118 /   321 | ms/batch 5130.36 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   120 /   321 | ms/batch 5155.99 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   122 /   321 | ms/batch 5252.65 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   124 /   321 | ms/batch 5113.95 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   126 /   321 | ms/batch 4964.25 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   128 /   321 | ms/batch 5128.61 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   130 /   321 | ms/batch 5671.99 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   132 /   321 | ms/batch 3293.65 | Loss 00.53 |\n",
      "[Training]| Epochs  35 | Batch   134 /   321 | ms/batch 5575.45 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   136 /   321 | ms/batch 4844.59 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   138 /   321 | ms/batch 4198.62 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   140 /   321 | ms/batch 4575.66 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch   142 /   321 | ms/batch 5947.86 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   144 /   321 | ms/batch 5354.54 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   146 /   321 | ms/batch 4941.42 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   148 /   321 | ms/batch 6565.67 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch   150 /   321 | ms/batch 4175.97 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   152 /   321 | ms/batch 5008.15 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   154 /   321 | ms/batch 3838.21 | Loss 00.54 |\n",
      "[Training]| Epochs  35 | Batch   156 /   321 | ms/batch 5835.46 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   158 /   321 | ms/batch 5978.42 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   160 /   321 | ms/batch 4398.04 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   162 /   321 | ms/batch 4893.97 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   164 /   321 | ms/batch 3773.56 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   166 /   321 | ms/batch 4867.71 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   168 /   321 | ms/batch 6864.43 | Loss 00.61 |\n",
      "[Training]| Epochs  35 | Batch   170 /   321 | ms/batch 5550.54 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   172 /   321 | ms/batch 5137.79 | Loss 00.59 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  35 | Batch   174 /   321 | ms/batch 4702.16 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   176 /   321 | ms/batch 5145.35 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   178 /   321 | ms/batch 6217.22 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   180 /   321 | ms/batch 5302.18 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   182 /   321 | ms/batch 5428.01 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   184 /   321 | ms/batch 4964.02 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   186 /   321 | ms/batch 6126.62 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   188 /   321 | ms/batch 4345.52 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   190 /   321 | ms/batch 4893.50 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   192 /   321 | ms/batch 5647.00 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   194 /   321 | ms/batch 6370.66 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch   196 /   321 | ms/batch 6186.22 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch   198 /   321 | ms/batch 5071.74 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   200 /   321 | ms/batch 4276.26 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   202 /   321 | ms/batch 4786.32 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   204 /   321 | ms/batch 4600.30 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   206 /   321 | ms/batch 4805.52 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   208 /   321 | ms/batch 4153.01 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   210 /   321 | ms/batch 3887.23 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   212 /   321 | ms/batch 4589.70 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   214 /   321 | ms/batch 5090.91 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   216 /   321 | ms/batch 4736.11 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   218 /   321 | ms/batch 5457.33 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   220 /   321 | ms/batch 5282.44 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   222 /   321 | ms/batch 4254.78 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   224 /   321 | ms/batch 5515.61 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   226 /   321 | ms/batch 4931.16 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   228 /   321 | ms/batch 5029.53 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   230 /   321 | ms/batch 5537.39 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   232 /   321 | ms/batch 4685.36 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   234 /   321 | ms/batch 5375.66 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   236 /   321 | ms/batch 5393.26 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   238 /   321 | ms/batch 4598.05 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   240 /   321 | ms/batch 3815.48 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   242 /   321 | ms/batch 3449.43 | Loss 00.52 |\n",
      "[Training]| Epochs  35 | Batch   244 /   321 | ms/batch 5437.17 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch   246 /   321 | ms/batch 4912.87 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   248 /   321 | ms/batch 5308.67 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   250 /   321 | ms/batch 5230.65 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   252 /   321 | ms/batch 5280.42 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   254 /   321 | ms/batch 4577.46 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   256 /   321 | ms/batch 5391.97 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   258 /   321 | ms/batch 4087.82 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   260 /   321 | ms/batch 4463.80 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   262 /   321 | ms/batch 5699.14 | Loss 00.60 |\n",
      "[Training]| Epochs  35 | Batch   264 /   321 | ms/batch 4474.77 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   266 /   321 | ms/batch 4662.00 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   268 /   321 | ms/batch 4928.74 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   270 /   321 | ms/batch 5219.10 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   272 /   321 | ms/batch 5896.30 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   274 /   321 | ms/batch 5609.29 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   276 /   321 | ms/batch 4419.62 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   278 /   321 | ms/batch 5865.87 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   280 /   321 | ms/batch 4612.93 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   282 /   321 | ms/batch 4794.42 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   284 /   321 | ms/batch 4539.06 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch   286 /   321 | ms/batch 5424.98 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   288 /   321 | ms/batch 5840.45 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   290 /   321 | ms/batch 5150.22 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   292 /   321 | ms/batch 4316.24 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   294 /   321 | ms/batch 5044.69 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   296 /   321 | ms/batch 4122.49 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   298 /   321 | ms/batch 5185.48 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   300 /   321 | ms/batch 4171.54 | Loss 00.57 |\n",
      "[Training]| Epochs  35 | Batch   302 /   321 | ms/batch 4033.95 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch   304 /   321 | ms/batch 5804.12 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   306 /   321 | ms/batch 6312.53 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   308 /   321 | ms/batch 4232.96 | Loss 00.56 |\n",
      "[Training]| Epochs  35 | Batch   310 /   321 | ms/batch 4334.66 | Loss 00.55 |\n",
      "[Training]| Epochs  35 | Batch   312 /   321 | ms/batch 6041.07 | Loss 00.58 |\n",
      "[Training]| Epochs  35 | Batch   314 /   321 | ms/batch 5903.88 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   316 /   321 | ms/batch 5899.73 | Loss 00.59 |\n",
      "[Training]| Epochs  35 | Batch   318 /   321 | ms/batch 4866.85 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  35 | Batch   320 /   321 | ms/batch 5014.71 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  35 | Elapsed 1448.57 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.72\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.38\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.50\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.41\n",
      "result= [36, 0.09246880285273468, 0.11921632429258208, 0.17588324483632112, 0.20739781772913826, 0.22965785775069417, 0.11878379768258598, 0.07993006836318078, 0.0407505228662528, 0.02982083541382502, 0.025319326137623003, 0.08409021129295961, 0.07874962917455731, 0.0583075436408503, 0.0475306938331943, 0.0422570725855006, 0.08295691161709007, 0.08811192813206722, 0.09378555226985391, 0.09586995682589831, 0.09699798084919603, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5853287091961613]\n",
      "[Training]| Epochs  36 | Batch     2 /   321 | ms/batch 6700.61 | Loss 00.85 |\n",
      "[Training]| Epochs  36 | Batch     4 /   321 | ms/batch 4876.58 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch     6 /   321 | ms/batch 6209.50 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch     8 /   321 | ms/batch 5887.17 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch    10 /   321 | ms/batch 5738.10 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    12 /   321 | ms/batch 5607.34 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    14 /   321 | ms/batch 4274.23 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    16 /   321 | ms/batch 4529.64 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch    18 /   321 | ms/batch 4653.44 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch    20 /   321 | ms/batch 4655.24 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    22 /   321 | ms/batch 6331.23 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch    24 /   321 | ms/batch 4969.75 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    26 /   321 | ms/batch 4860.49 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    28 /   321 | ms/batch 4927.65 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    30 /   321 | ms/batch 4564.63 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    32 /   321 | ms/batch 4728.81 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    34 /   321 | ms/batch 4224.69 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    36 /   321 | ms/batch 5427.82 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    38 /   321 | ms/batch 5599.22 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    40 /   321 | ms/batch 5560.99 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    42 /   321 | ms/batch 5270.16 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    44 /   321 | ms/batch 5138.37 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    46 /   321 | ms/batch 4512.67 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    48 /   321 | ms/batch 5230.25 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    50 /   321 | ms/batch 4308.88 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    52 /   321 | ms/batch 5092.79 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    54 /   321 | ms/batch 4502.81 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    56 /   321 | ms/batch 5117.12 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    58 /   321 | ms/batch 4907.89 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    60 /   321 | ms/batch 4211.66 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    62 /   321 | ms/batch 4553.56 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    64 /   321 | ms/batch 4798.31 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    66 /   321 | ms/batch 4329.35 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    68 /   321 | ms/batch 4560.67 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    70 /   321 | ms/batch 4587.98 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    72 /   321 | ms/batch 3549.89 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    74 /   321 | ms/batch 4446.58 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    76 /   321 | ms/batch 4833.65 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    78 /   321 | ms/batch 5456.32 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    80 /   321 | ms/batch 5201.38 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    82 /   321 | ms/batch 5294.43 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    84 /   321 | ms/batch 4013.77 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch    86 /   321 | ms/batch 5941.61 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    88 /   321 | ms/batch 5865.12 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    90 /   321 | ms/batch 4374.93 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch    92 /   321 | ms/batch 5346.05 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch    94 /   321 | ms/batch 4876.97 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch    96 /   321 | ms/batch 4402.76 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch    98 /   321 | ms/batch 5632.79 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   100 /   321 | ms/batch 5324.83 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   102 /   321 | ms/batch 4393.66 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   104 /   321 | ms/batch 5603.30 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   106 /   321 | ms/batch 3968.90 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   108 /   321 | ms/batch 4039.63 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   110 /   321 | ms/batch 5811.18 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   112 /   321 | ms/batch 3706.83 | Loss 00.54 |\n",
      "[Training]| Epochs  36 | Batch   114 /   321 | ms/batch 4672.96 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   116 /   321 | ms/batch 5592.31 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   118 /   321 | ms/batch 5025.65 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   120 /   321 | ms/batch 4990.13 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   122 /   321 | ms/batch 5141.62 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   124 /   321 | ms/batch 5284.41 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   126 /   321 | ms/batch 4692.31 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   128 /   321 | ms/batch 5137.20 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   130 /   321 | ms/batch 5511.60 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   132 /   321 | ms/batch 3282.43 | Loss 00.53 |\n",
      "[Training]| Epochs  36 | Batch   134 /   321 | ms/batch 5697.21 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   136 /   321 | ms/batch 4657.50 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  36 | Batch   138 /   321 | ms/batch 4005.84 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   140 /   321 | ms/batch 4465.22 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   142 /   321 | ms/batch 6273.83 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   144 /   321 | ms/batch 5370.33 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   146 /   321 | ms/batch 4868.87 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   148 /   321 | ms/batch 6438.26 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   150 /   321 | ms/batch 4311.99 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   152 /   321 | ms/batch 4916.81 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   154 /   321 | ms/batch 3920.38 | Loss 00.54 |\n",
      "[Training]| Epochs  36 | Batch   156 /   321 | ms/batch 5739.73 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   158 /   321 | ms/batch 6047.03 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   160 /   321 | ms/batch 4492.28 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   162 /   321 | ms/batch 4878.86 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   164 /   321 | ms/batch 3968.41 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   166 /   321 | ms/batch 4918.98 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   168 /   321 | ms/batch 6988.16 | Loss 00.61 |\n",
      "[Training]| Epochs  36 | Batch   170 /   321 | ms/batch 5548.33 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   172 /   321 | ms/batch 4857.99 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   174 /   321 | ms/batch 4557.10 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   176 /   321 | ms/batch 5367.08 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   178 /   321 | ms/batch 5833.69 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   180 /   321 | ms/batch 5370.96 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   182 /   321 | ms/batch 5502.95 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   184 /   321 | ms/batch 4890.04 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   186 /   321 | ms/batch 6029.33 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   188 /   321 | ms/batch 4180.07 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   190 /   321 | ms/batch 4915.48 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   192 /   321 | ms/batch 5513.52 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   194 /   321 | ms/batch 6206.81 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   196 /   321 | ms/batch 6519.49 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   198 /   321 | ms/batch 4965.46 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   200 /   321 | ms/batch 4140.13 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   202 /   321 | ms/batch 4608.16 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   204 /   321 | ms/batch 4541.72 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   206 /   321 | ms/batch 4604.83 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   208 /   321 | ms/batch 4236.82 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   210 /   321 | ms/batch 4060.29 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   212 /   321 | ms/batch 4432.77 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   214 /   321 | ms/batch 5230.62 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   216 /   321 | ms/batch 4393.03 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   218 /   321 | ms/batch 5390.37 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   220 /   321 | ms/batch 5219.85 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   222 /   321 | ms/batch 4462.12 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   224 /   321 | ms/batch 5464.46 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   226 /   321 | ms/batch 5000.74 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   228 /   321 | ms/batch 5016.87 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   230 /   321 | ms/batch 5504.99 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   232 /   321 | ms/batch 4780.62 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   234 /   321 | ms/batch 5310.64 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   236 /   321 | ms/batch 5240.14 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   238 /   321 | ms/batch 4759.09 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   240 /   321 | ms/batch 3787.94 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   242 /   321 | ms/batch 3510.64 | Loss 00.51 |\n",
      "[Training]| Epochs  36 | Batch   244 /   321 | ms/batch 5468.15 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   246 /   321 | ms/batch 4858.20 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   248 /   321 | ms/batch 5463.02 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   250 /   321 | ms/batch 5323.22 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   252 /   321 | ms/batch 4942.61 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   254 /   321 | ms/batch 4852.82 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   256 /   321 | ms/batch 5625.05 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   258 /   321 | ms/batch 4137.33 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   260 /   321 | ms/batch 4459.93 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   262 /   321 | ms/batch 6244.19 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   264 /   321 | ms/batch 4216.36 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   266 /   321 | ms/batch 4616.30 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   268 /   321 | ms/batch 5063.44 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   270 /   321 | ms/batch 5069.30 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   272 /   321 | ms/batch 5687.21 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   274 /   321 | ms/batch 5385.67 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   276 /   321 | ms/batch 4352.36 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   278 /   321 | ms/batch 5719.42 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   280 /   321 | ms/batch 4688.73 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   282 /   321 | ms/batch 5315.14 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   284 /   321 | ms/batch 4041.06 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   286 /   321 | ms/batch 5467.70 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   288 /   321 | ms/batch 5559.60 | Loss 00.60 |\n",
      "[Training]| Epochs  36 | Batch   290 /   321 | ms/batch 5536.52 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   292 /   321 | ms/batch 4248.35 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   294 /   321 | ms/batch 4936.44 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   296 /   321 | ms/batch 4056.53 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   298 /   321 | ms/batch 5176.01 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   300 /   321 | ms/batch 4314.53 | Loss 00.57 |\n",
      "[Training]| Epochs  36 | Batch   302 /   321 | ms/batch 3987.59 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   304 /   321 | ms/batch 6020.09 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   306 /   321 | ms/batch 5984.34 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   308 /   321 | ms/batch 4245.14 | Loss 00.56 |\n",
      "[Training]| Epochs  36 | Batch   310 /   321 | ms/batch 4149.09 | Loss 00.55 |\n",
      "[Training]| Epochs  36 | Batch   312 /   321 | ms/batch 5766.35 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   314 /   321 | ms/batch 6026.13 | Loss 00.59 |\n",
      "[Training]| Epochs  36 | Batch   316 /   321 | ms/batch 5991.74 | Loss 00.58 |\n",
      "[Training]| Epochs  36 | Batch   318 /   321 | ms/batch 4748.11 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  36 | Batch   320 /   321 | ms/batch 5531.67 | Loss 00.60 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  36 | Elapsed 1436.01 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.63\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.73\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.50\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.58\n",
      "result= [37, 0.09402806556327029, 0.11914870038407906, 0.17375693317511448, 0.2083687429212539, 0.22957893544031324, 0.12377227997278108, 0.07946240966987164, 0.0406725847061407, 0.02982864023091174, 0.025457211239488443, 0.08675156174835895, 0.07833085688221575, 0.05810834686921522, 0.04755160479645941, 0.042473579698118116, 0.0740023324830822, 0.07858748836348461, 0.08424824729815143, 0.08647085899441406, 0.08759872621434885, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5847807607533019]\n",
      "[Training]| Epochs  37 | Batch     2 /   321 | ms/batch 7034.69 | Loss 00.85 |\n",
      "[Training]| Epochs  37 | Batch     4 /   321 | ms/batch 4750.28 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch     6 /   321 | ms/batch 6304.44 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch     8 /   321 | ms/batch 5802.61 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch    10 /   321 | ms/batch 5709.56 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    12 /   321 | ms/batch 5546.54 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    14 /   321 | ms/batch 4021.44 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    16 /   321 | ms/batch 4149.72 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch    18 /   321 | ms/batch 4419.95 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    20 /   321 | ms/batch 4857.02 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    22 /   321 | ms/batch 6558.88 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch    24 /   321 | ms/batch 4711.39 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    26 /   321 | ms/batch 4783.14 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    28 /   321 | ms/batch 4955.64 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    30 /   321 | ms/batch 4594.53 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    32 /   321 | ms/batch 4274.67 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    34 /   321 | ms/batch 4383.86 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    36 /   321 | ms/batch 5468.86 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    38 /   321 | ms/batch 5665.55 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    40 /   321 | ms/batch 5358.49 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    42 /   321 | ms/batch 5024.34 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch    44 /   321 | ms/batch 5265.83 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    46 /   321 | ms/batch 4251.69 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    48 /   321 | ms/batch 5359.17 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    50 /   321 | ms/batch 4707.71 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    52 /   321 | ms/batch 5057.80 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    54 /   321 | ms/batch 4722.40 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch    56 /   321 | ms/batch 5044.06 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    58 /   321 | ms/batch 4835.06 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    60 /   321 | ms/batch 4163.33 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    62 /   321 | ms/batch 4439.14 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    64 /   321 | ms/batch 4728.70 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    66 /   321 | ms/batch 4399.92 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch    68 /   321 | ms/batch 4765.13 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    70 /   321 | ms/batch 4605.85 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    72 /   321 | ms/batch 3689.89 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch    74 /   321 | ms/batch 4255.94 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    76 /   321 | ms/batch 4876.31 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    78 /   321 | ms/batch 5640.41 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    80 /   321 | ms/batch 5022.44 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    82 /   321 | ms/batch 5469.42 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    84 /   321 | ms/batch 4072.92 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch    86 /   321 | ms/batch 5844.85 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch    88 /   321 | ms/batch 5640.20 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    90 /   321 | ms/batch 4634.27 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    92 /   321 | ms/batch 5506.29 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch    94 /   321 | ms/batch 4947.12 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch    96 /   321 | ms/batch 4563.09 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch    98 /   321 | ms/batch 5457.07 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   100 /   321 | ms/batch 5692.06 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  37 | Batch   102 /   321 | ms/batch 4393.36 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   104 /   321 | ms/batch 5570.43 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   106 /   321 | ms/batch 3689.71 | Loss 00.54 |\n",
      "[Training]| Epochs  37 | Batch   108 /   321 | ms/batch 4259.89 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   110 /   321 | ms/batch 5503.16 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   112 /   321 | ms/batch 3951.55 | Loss 00.54 |\n",
      "[Training]| Epochs  37 | Batch   114 /   321 | ms/batch 4760.66 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   116 /   321 | ms/batch 5429.04 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   118 /   321 | ms/batch 5571.46 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   120 /   321 | ms/batch 5091.51 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   122 /   321 | ms/batch 4952.52 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   124 /   321 | ms/batch 5407.17 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   126 /   321 | ms/batch 4977.41 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   128 /   321 | ms/batch 5504.64 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   130 /   321 | ms/batch 5763.32 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   132 /   321 | ms/batch 3282.79 | Loss 00.53 |\n",
      "[Training]| Epochs  37 | Batch   134 /   321 | ms/batch 5690.54 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   136 /   321 | ms/batch 4570.25 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   138 /   321 | ms/batch 4026.22 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   140 /   321 | ms/batch 4836.41 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   142 /   321 | ms/batch 5980.22 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   144 /   321 | ms/batch 5334.17 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   146 /   321 | ms/batch 4953.33 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   148 /   321 | ms/batch 6367.89 | Loss 00.61 |\n",
      "[Training]| Epochs  37 | Batch   150 /   321 | ms/batch 4279.51 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   152 /   321 | ms/batch 5073.55 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   154 /   321 | ms/batch 4039.40 | Loss 00.54 |\n",
      "[Training]| Epochs  37 | Batch   156 /   321 | ms/batch 5798.82 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   158 /   321 | ms/batch 5916.32 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   160 /   321 | ms/batch 4532.78 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   162 /   321 | ms/batch 4649.97 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   164 /   321 | ms/batch 4149.07 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   166 /   321 | ms/batch 5117.78 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   168 /   321 | ms/batch 6944.03 | Loss 00.61 |\n",
      "[Training]| Epochs  37 | Batch   170 /   321 | ms/batch 5505.91 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch   172 /   321 | ms/batch 4932.77 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   174 /   321 | ms/batch 4621.20 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   176 /   321 | ms/batch 5380.06 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   178 /   321 | ms/batch 6073.31 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   180 /   321 | ms/batch 5271.45 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   182 /   321 | ms/batch 5608.30 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   184 /   321 | ms/batch 4786.62 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   186 /   321 | ms/batch 6097.84 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   188 /   321 | ms/batch 4377.38 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   190 /   321 | ms/batch 4936.56 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   192 /   321 | ms/batch 5841.49 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   194 /   321 | ms/batch 6176.15 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch   196 /   321 | ms/batch 6101.75 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch   198 /   321 | ms/batch 5267.32 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   200 /   321 | ms/batch 4150.28 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   202 /   321 | ms/batch 4737.53 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   204 /   321 | ms/batch 4516.19 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   206 /   321 | ms/batch 4844.82 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   208 /   321 | ms/batch 4152.18 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   210 /   321 | ms/batch 3822.23 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   212 /   321 | ms/batch 4466.59 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   214 /   321 | ms/batch 5130.54 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   216 /   321 | ms/batch 4636.40 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   218 /   321 | ms/batch 5394.23 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   220 /   321 | ms/batch 5189.47 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   222 /   321 | ms/batch 4360.00 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   224 /   321 | ms/batch 5265.38 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   226 /   321 | ms/batch 5000.51 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   228 /   321 | ms/batch 4877.42 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   230 /   321 | ms/batch 5589.06 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   232 /   321 | ms/batch 4837.57 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   234 /   321 | ms/batch 5509.45 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   236 /   321 | ms/batch 5330.05 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   238 /   321 | ms/batch 4751.81 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   240 /   321 | ms/batch 4041.38 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   242 /   321 | ms/batch 3569.63 | Loss 00.52 |\n",
      "[Training]| Epochs  37 | Batch   244 /   321 | ms/batch 5603.72 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch   246 /   321 | ms/batch 4921.49 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   248 /   321 | ms/batch 5544.05 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   250 /   321 | ms/batch 5467.39 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   252 /   321 | ms/batch 5050.39 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   254 /   321 | ms/batch 4548.11 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   256 /   321 | ms/batch 5581.97 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   258 /   321 | ms/batch 4080.50 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   260 /   321 | ms/batch 4447.60 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   262 /   321 | ms/batch 6157.67 | Loss 00.60 |\n",
      "[Training]| Epochs  37 | Batch   264 /   321 | ms/batch 4606.62 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   266 /   321 | ms/batch 4579.61 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   268 /   321 | ms/batch 4843.60 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   270 /   321 | ms/batch 5172.12 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   272 /   321 | ms/batch 5730.57 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   274 /   321 | ms/batch 5678.04 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   276 /   321 | ms/batch 4377.79 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   278 /   321 | ms/batch 5814.37 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   280 /   321 | ms/batch 4763.80 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   282 /   321 | ms/batch 4981.88 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   284 /   321 | ms/batch 4302.93 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   286 /   321 | ms/batch 4886.68 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   288 /   321 | ms/batch 5980.99 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   290 /   321 | ms/batch 5388.61 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   292 /   321 | ms/batch 4144.98 | Loss 00.57 |\n",
      "[Training]| Epochs  37 | Batch   294 /   321 | ms/batch 5088.82 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   296 /   321 | ms/batch 4091.74 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   298 /   321 | ms/batch 5028.36 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   300 /   321 | ms/batch 4441.03 | Loss 00.58 |\n",
      "[Training]| Epochs  37 | Batch   302 /   321 | ms/batch 4020.61 | Loss 00.55 |\n",
      "[Training]| Epochs  37 | Batch   304 /   321 | ms/batch 5554.74 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   306 /   321 | ms/batch 6397.23 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   308 /   321 | ms/batch 4365.93 | Loss 00.55 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  37 | Batch   310 /   321 | ms/batch 4080.07 | Loss 00.56 |\n",
      "[Training]| Epochs  37 | Batch   312 /   321 | ms/batch 5752.25 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   314 /   321 | ms/batch 5844.44 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   316 /   321 | ms/batch 5699.62 | Loss 00.59 |\n",
      "[Training]| Epochs  37 | Batch   318 /   321 | ms/batch 4865.47 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  37 | Batch   320 /   321 | ms/batch 5218.69 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  37 | Elapsed 1443.05 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.36\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.75\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.35\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 1.00\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.48\n",
      "result= [38, 0.09194711401233985, 0.11876269940371079, 0.17454432474851422, 0.2064387380194125, 0.2274802870241256, 0.1187838095756406, 0.07825431318275648, 0.0405426768706538, 0.029563624321810453, 0.02521141153337058, 0.08383109731220771, 0.07746209897571305, 0.057970375542703866, 0.047164756435950285, 0.0420607123073896, 0.08156690775073443, 0.08663244877084625, 0.09237043714602293, 0.09445968482135966, 0.09556644375614579, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5840598624429585]\n",
      "[Training]| Epochs  38 | Batch     2 /   321 | ms/batch 6687.42 | Loss 00.86 |\n",
      "[Training]| Epochs  38 | Batch     4 /   321 | ms/batch 4949.32 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch     6 /   321 | ms/batch 6127.92 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch     8 /   321 | ms/batch 5592.42 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    10 /   321 | ms/batch 5463.37 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    12 /   321 | ms/batch 5549.86 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    14 /   321 | ms/batch 4146.43 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    16 /   321 | ms/batch 4181.35 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    18 /   321 | ms/batch 4448.07 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    20 /   321 | ms/batch 4794.14 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    22 /   321 | ms/batch 6500.14 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch    24 /   321 | ms/batch 4937.75 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    26 /   321 | ms/batch 4813.17 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    28 /   321 | ms/batch 5001.68 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    30 /   321 | ms/batch 4736.85 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    32 /   321 | ms/batch 4730.37 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    34 /   321 | ms/batch 4255.89 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    36 /   321 | ms/batch 5687.58 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    38 /   321 | ms/batch 5433.19 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    40 /   321 | ms/batch 5392.65 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    42 /   321 | ms/batch 5095.96 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    44 /   321 | ms/batch 5137.50 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    46 /   321 | ms/batch 4351.73 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    48 /   321 | ms/batch 5523.31 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    50 /   321 | ms/batch 4627.11 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    52 /   321 | ms/batch 4998.97 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    54 /   321 | ms/batch 4755.91 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch    56 /   321 | ms/batch 5102.75 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    58 /   321 | ms/batch 4763.76 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    60 /   321 | ms/batch 4271.96 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    62 /   321 | ms/batch 4590.07 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    64 /   321 | ms/batch 4310.72 | Loss 00.57 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  38 | Batch    66 /   321 | ms/batch 4350.87 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    68 /   321 | ms/batch 4833.63 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    70 /   321 | ms/batch 4577.68 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    72 /   321 | ms/batch 3638.92 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    74 /   321 | ms/batch 4492.97 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    76 /   321 | ms/batch 4766.02 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch    78 /   321 | ms/batch 5594.92 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    80 /   321 | ms/batch 5039.83 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    82 /   321 | ms/batch 5573.65 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch    84 /   321 | ms/batch 4096.04 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch    86 /   321 | ms/batch 5800.39 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch    88 /   321 | ms/batch 5871.32 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    90 /   321 | ms/batch 4470.66 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    92 /   321 | ms/batch 5296.36 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch    94 /   321 | ms/batch 4754.77 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch    96 /   321 | ms/batch 4470.69 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch    98 /   321 | ms/batch 5693.42 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   100 /   321 | ms/batch 5829.09 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   102 /   321 | ms/batch 4429.97 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   104 /   321 | ms/batch 5433.93 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   106 /   321 | ms/batch 3864.98 | Loss 00.54 |\n",
      "[Training]| Epochs  38 | Batch   108 /   321 | ms/batch 4264.50 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   110 /   321 | ms/batch 5240.49 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   112 /   321 | ms/batch 3771.25 | Loss 00.54 |\n",
      "[Training]| Epochs  38 | Batch   114 /   321 | ms/batch 4901.20 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   116 /   321 | ms/batch 5167.28 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   118 /   321 | ms/batch 5353.68 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   120 /   321 | ms/batch 5183.12 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   122 /   321 | ms/batch 5140.06 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   124 /   321 | ms/batch 5454.63 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   126 /   321 | ms/batch 5016.06 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   128 /   321 | ms/batch 5278.96 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   130 /   321 | ms/batch 5427.88 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   132 /   321 | ms/batch 3230.57 | Loss 00.54 |\n",
      "[Training]| Epochs  38 | Batch   134 /   321 | ms/batch 5637.59 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   136 /   321 | ms/batch 4802.08 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   138 /   321 | ms/batch 4172.26 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   140 /   321 | ms/batch 4786.65 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   142 /   321 | ms/batch 6069.74 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   144 /   321 | ms/batch 5709.13 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   146 /   321 | ms/batch 4929.54 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   148 /   321 | ms/batch 6571.36 | Loss 00.61 |\n",
      "[Training]| Epochs  38 | Batch   150 /   321 | ms/batch 4236.89 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   152 /   321 | ms/batch 4901.04 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   154 /   321 | ms/batch 3904.44 | Loss 00.54 |\n",
      "[Training]| Epochs  38 | Batch   156 /   321 | ms/batch 5592.24 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   158 /   321 | ms/batch 5944.49 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   160 /   321 | ms/batch 4355.01 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   162 /   321 | ms/batch 4924.55 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   164 /   321 | ms/batch 3975.55 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   166 /   321 | ms/batch 4700.97 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   168 /   321 | ms/batch 6629.90 | Loss 00.61 |\n",
      "[Training]| Epochs  38 | Batch   170 /   321 | ms/batch 5843.82 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   172 /   321 | ms/batch 5069.98 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   174 /   321 | ms/batch 4471.52 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   176 /   321 | ms/batch 5458.75 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   178 /   321 | ms/batch 6106.77 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   180 /   321 | ms/batch 5064.58 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   182 /   321 | ms/batch 5404.19 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   184 /   321 | ms/batch 4966.64 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   186 /   321 | ms/batch 6368.43 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   188 /   321 | ms/batch 4357.16 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   190 /   321 | ms/batch 4944.58 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   192 /   321 | ms/batch 5610.24 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   194 /   321 | ms/batch 6297.42 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   196 /   321 | ms/batch 5877.15 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   198 /   321 | ms/batch 5056.08 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   200 /   321 | ms/batch 4197.20 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   202 /   321 | ms/batch 4664.99 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   204 /   321 | ms/batch 4732.45 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   206 /   321 | ms/batch 4897.15 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   208 /   321 | ms/batch 4287.17 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch   210 /   321 | ms/batch 3993.61 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   212 /   321 | ms/batch 4565.18 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   214 /   321 | ms/batch 5223.43 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   216 /   321 | ms/batch 4485.35 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   218 /   321 | ms/batch 5318.65 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   220 /   321 | ms/batch 5080.27 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   222 /   321 | ms/batch 4488.24 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch   224 /   321 | ms/batch 5160.27 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   226 /   321 | ms/batch 4983.61 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   228 /   321 | ms/batch 4844.23 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   230 /   321 | ms/batch 5954.40 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   232 /   321 | ms/batch 4603.22 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   234 /   321 | ms/batch 5280.71 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   236 /   321 | ms/batch 5549.96 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   238 /   321 | ms/batch 4589.09 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   240 /   321 | ms/batch 3636.11 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   242 /   321 | ms/batch 3481.32 | Loss 00.52 |\n",
      "[Training]| Epochs  38 | Batch   244 /   321 | ms/batch 5309.46 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   246 /   321 | ms/batch 5063.38 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   248 /   321 | ms/batch 5586.69 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   250 /   321 | ms/batch 5353.01 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   252 /   321 | ms/batch 4917.13 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   254 /   321 | ms/batch 4621.66 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   256 /   321 | ms/batch 5462.02 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   258 /   321 | ms/batch 4087.20 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   260 /   321 | ms/batch 4301.27 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch   262 /   321 | ms/batch 6191.39 | Loss 00.60 |\n",
      "[Training]| Epochs  38 | Batch   264 /   321 | ms/batch 4610.61 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   266 /   321 | ms/batch 4521.81 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   268 /   321 | ms/batch 5199.14 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   270 /   321 | ms/batch 5200.06 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   272 /   321 | ms/batch 5488.09 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  38 | Batch   274 /   321 | ms/batch 5731.17 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   276 /   321 | ms/batch 4463.85 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   278 /   321 | ms/batch 5906.90 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   280 /   321 | ms/batch 4853.65 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   282 /   321 | ms/batch 4788.95 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   284 /   321 | ms/batch 4388.77 | Loss 00.55 |\n",
      "[Training]| Epochs  38 | Batch   286 /   321 | ms/batch 5186.89 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   288 /   321 | ms/batch 5782.58 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   290 /   321 | ms/batch 5252.18 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   292 /   321 | ms/batch 4259.22 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   294 /   321 | ms/batch 4910.15 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   296 /   321 | ms/batch 4100.63 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   298 /   321 | ms/batch 5333.69 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   300 /   321 | ms/batch 4583.49 | Loss 00.57 |\n",
      "[Training]| Epochs  38 | Batch   302 /   321 | ms/batch 4052.61 | Loss 00.54 |\n",
      "[Training]| Epochs  38 | Batch   304 /   321 | ms/batch 5716.32 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   306 /   321 | ms/batch 6312.16 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   308 /   321 | ms/batch 4362.19 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   310 /   321 | ms/batch 4383.41 | Loss 00.56 |\n",
      "[Training]| Epochs  38 | Batch   312 /   321 | ms/batch 5887.81 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   314 /   321 | ms/batch 5833.58 | Loss 00.59 |\n",
      "[Training]| Epochs  38 | Batch   316 /   321 | ms/batch 5908.91 | Loss 00.58 |\n",
      "[Training]| Epochs  38 | Batch   318 /   321 | ms/batch 5203.09 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  38 | Batch   320 /   321 | ms/batch 5443.43 | Loss 00.59 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  38 | Elapsed 1463.21 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.69\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.64\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.51\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.43\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.61\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.68\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.34\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.52\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.55\n",
      "result= [39, 0.0925750672956602, 0.11979109183569393, 0.17477152966375195, 0.20894624586692567, 0.22969199081742012, 0.1197191745344773, 0.07934553067570818, 0.04085444735068413, 0.02989098659643231, 0.025667040401943383, 0.08449240466717715, 0.07841710531423543, 0.05838791095736616, 0.04767849476933944, 0.04280143850776987, 0.083165421444612, 0.08828002500724533, 0.09393845990246462, 0.0961333882491542, 0.09727228353226837, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5844198958373364]\n",
      "[Training]| Epochs  39 | Batch     2 /   321 | ms/batch 6674.23 | Loss 00.84 |\n",
      "[Training]| Epochs  39 | Batch     4 /   321 | ms/batch 4801.06 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch     6 /   321 | ms/batch 6141.00 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch     8 /   321 | ms/batch 5668.95 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch    10 /   321 | ms/batch 5420.76 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch    12 /   321 | ms/batch 5273.31 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    14 /   321 | ms/batch 3862.75 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    16 /   321 | ms/batch 4390.86 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    18 /   321 | ms/batch 4465.78 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    20 /   321 | ms/batch 4703.99 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    22 /   321 | ms/batch 6375.98 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch    24 /   321 | ms/batch 4867.38 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    26 /   321 | ms/batch 4836.60 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    28 /   321 | ms/batch 5258.95 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  39 | Batch    30 /   321 | ms/batch 4569.23 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    32 /   321 | ms/batch 4350.47 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    34 /   321 | ms/batch 4451.05 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    36 /   321 | ms/batch 5566.35 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    38 /   321 | ms/batch 5649.18 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    40 /   321 | ms/batch 5285.54 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    42 /   321 | ms/batch 5079.47 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch    44 /   321 | ms/batch 4988.71 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    46 /   321 | ms/batch 4372.72 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    48 /   321 | ms/batch 5411.99 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    50 /   321 | ms/batch 4493.23 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    52 /   321 | ms/batch 4740.18 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    54 /   321 | ms/batch 4575.05 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    56 /   321 | ms/batch 4866.76 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    58 /   321 | ms/batch 4892.26 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    60 /   321 | ms/batch 4212.92 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    62 /   321 | ms/batch 4452.22 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    64 /   321 | ms/batch 4433.15 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    66 /   321 | ms/batch 4138.39 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    68 /   321 | ms/batch 4819.94 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    70 /   321 | ms/batch 4544.04 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    72 /   321 | ms/batch 3862.27 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    74 /   321 | ms/batch 4314.48 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    76 /   321 | ms/batch 4759.95 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    78 /   321 | ms/batch 5496.86 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    80 /   321 | ms/batch 4919.82 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    82 /   321 | ms/batch 5258.96 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    84 /   321 | ms/batch 3908.94 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch    86 /   321 | ms/batch 5597.78 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch    88 /   321 | ms/batch 5953.88 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    90 /   321 | ms/batch 4375.81 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    92 /   321 | ms/batch 5289.47 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch    94 /   321 | ms/batch 4730.04 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch    96 /   321 | ms/batch 4303.12 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch    98 /   321 | ms/batch 5503.12 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   100 /   321 | ms/batch 5410.08 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   102 /   321 | ms/batch 4492.49 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   104 /   321 | ms/batch 5230.35 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   106 /   321 | ms/batch 3534.05 | Loss 00.53 |\n",
      "[Training]| Epochs  39 | Batch   108 /   321 | ms/batch 4432.72 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   110 /   321 | ms/batch 5343.22 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   112 /   321 | ms/batch 3648.79 | Loss 00.53 |\n",
      "[Training]| Epochs  39 | Batch   114 /   321 | ms/batch 4663.43 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   116 /   321 | ms/batch 5268.15 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   118 /   321 | ms/batch 5042.06 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   120 /   321 | ms/batch 4934.30 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   122 /   321 | ms/batch 5221.31 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   124 /   321 | ms/batch 5226.86 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   126 /   321 | ms/batch 4773.74 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   128 /   321 | ms/batch 5302.99 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   130 /   321 | ms/batch 5592.56 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   132 /   321 | ms/batch 3310.73 | Loss 00.54 |\n",
      "[Training]| Epochs  39 | Batch   134 /   321 | ms/batch 5492.60 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   136 /   321 | ms/batch 4648.24 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   138 /   321 | ms/batch 4129.12 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   140 /   321 | ms/batch 4509.46 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   142 /   321 | ms/batch 5912.17 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   144 /   321 | ms/batch 5461.36 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   146 /   321 | ms/batch 4678.94 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   148 /   321 | ms/batch 6337.46 | Loss 00.61 |\n",
      "[Training]| Epochs  39 | Batch   150 /   321 | ms/batch 4269.93 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   152 /   321 | ms/batch 4870.41 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   154 /   321 | ms/batch 3841.25 | Loss 00.54 |\n",
      "[Training]| Epochs  39 | Batch   156 /   321 | ms/batch 5602.45 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   158 /   321 | ms/batch 5782.51 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   160 /   321 | ms/batch 4327.14 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   162 /   321 | ms/batch 4910.73 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   164 /   321 | ms/batch 3814.62 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   166 /   321 | ms/batch 4865.14 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   168 /   321 | ms/batch 6494.16 | Loss 00.61 |\n",
      "[Training]| Epochs  39 | Batch   170 /   321 | ms/batch 5481.49 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   172 /   321 | ms/batch 5118.71 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   174 /   321 | ms/batch 4497.16 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   176 /   321 | ms/batch 5121.71 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   178 /   321 | ms/batch 6068.70 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   180 /   321 | ms/batch 5127.49 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   182 /   321 | ms/batch 5285.99 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   184 /   321 | ms/batch 4891.74 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   186 /   321 | ms/batch 5864.93 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   188 /   321 | ms/batch 4457.12 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   190 /   321 | ms/batch 4810.37 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   192 /   321 | ms/batch 5422.70 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   194 /   321 | ms/batch 5900.01 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   196 /   321 | ms/batch 5867.92 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   198 /   321 | ms/batch 5089.87 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   200 /   321 | ms/batch 4175.33 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   202 /   321 | ms/batch 4591.90 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   204 /   321 | ms/batch 4795.02 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   206 /   321 | ms/batch 4723.24 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   208 /   321 | ms/batch 4357.16 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   210 /   321 | ms/batch 4100.48 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   212 /   321 | ms/batch 4429.24 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   214 /   321 | ms/batch 4917.11 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   216 /   321 | ms/batch 4420.00 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   218 /   321 | ms/batch 5432.39 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   220 /   321 | ms/batch 4971.72 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   222 /   321 | ms/batch 4199.59 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   224 /   321 | ms/batch 5227.39 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   226 /   321 | ms/batch 4964.00 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   228 /   321 | ms/batch 4811.98 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   230 /   321 | ms/batch 5510.00 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   232 /   321 | ms/batch 4488.54 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   234 /   321 | ms/batch 5387.40 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   236 /   321 | ms/batch 5254.01 | Loss 00.58 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training]| Epochs  39 | Batch   238 /   321 | ms/batch 4705.48 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   240 /   321 | ms/batch 3848.12 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   242 /   321 | ms/batch 3521.17 | Loss 00.51 |\n",
      "[Training]| Epochs  39 | Batch   244 /   321 | ms/batch 5448.51 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   246 /   321 | ms/batch 4736.73 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   248 /   321 | ms/batch 5510.90 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   250 /   321 | ms/batch 5096.93 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   252 /   321 | ms/batch 4789.09 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   254 /   321 | ms/batch 4953.67 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   256 /   321 | ms/batch 5402.64 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   258 /   321 | ms/batch 3816.76 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   260 /   321 | ms/batch 4487.68 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   262 /   321 | ms/batch 6047.07 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   264 /   321 | ms/batch 4194.64 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   266 /   321 | ms/batch 4643.78 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   268 /   321 | ms/batch 4938.85 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   270 /   321 | ms/batch 5173.00 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   272 /   321 | ms/batch 5538.00 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   274 /   321 | ms/batch 5628.01 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   276 /   321 | ms/batch 4271.82 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   278 /   321 | ms/batch 5761.86 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   280 /   321 | ms/batch 4662.15 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   282 /   321 | ms/batch 4828.91 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   284 /   321 | ms/batch 4019.86 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   286 /   321 | ms/batch 5192.91 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   288 /   321 | ms/batch 5748.12 | Loss 00.60 |\n",
      "[Training]| Epochs  39 | Batch   290 /   321 | ms/batch 5344.25 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   292 /   321 | ms/batch 4113.19 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   294 /   321 | ms/batch 4721.22 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   296 /   321 | ms/batch 3936.55 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   298 /   321 | ms/batch 4992.59 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   300 /   321 | ms/batch 4479.28 | Loss 00.57 |\n",
      "[Training]| Epochs  39 | Batch   302 /   321 | ms/batch 4063.59 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   304 /   321 | ms/batch 5673.42 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   306 /   321 | ms/batch 6092.76 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   308 /   321 | ms/batch 4363.35 | Loss 00.56 |\n",
      "[Training]| Epochs  39 | Batch   310 /   321 | ms/batch 4276.15 | Loss 00.55 |\n",
      "[Training]| Epochs  39 | Batch   312 /   321 | ms/batch 5626.53 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   314 /   321 | ms/batch 5643.68 | Loss 00.59 |\n",
      "[Training]| Epochs  39 | Batch   316 /   321 | ms/batch 5901.70 | Loss 00.58 |\n",
      "[Training]| Epochs  39 | Batch   318 /   321 | ms/batch 4805.93 | Loss 00.58 |\n",
      "32\n",
      "[Training]| Epochs  39 | Batch   320 /   321 | ms/batch 4888.77 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "32\n",
      "[Evaluation]| Epochs  39 | Elapsed 1425.83 | Loss 00.58 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "[Predicting]| Batch     0 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch     1 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch     2 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     3 /    81 | seconds/batch 0.59\n",
      "[Predicting]| Batch     4 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch     5 /    81 | seconds/batch 0.46\n",
      "[Predicting]| Batch     6 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch     7 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch     8 /    81 | seconds/batch 0.60\n",
      "[Predicting]| Batch     9 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    10 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    11 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    12 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    13 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    14 /    81 | seconds/batch 0.54\n",
      "[Predicting]| Batch    15 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    16 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    17 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    18 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    19 /    81 | seconds/batch 0.31\n",
      "[Predicting]| Batch    20 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    21 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    22 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    23 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    24 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    25 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    26 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    27 /    81 | seconds/batch 0.62\n",
      "[Predicting]| Batch    28 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    29 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    30 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    31 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    32 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    33 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    34 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    35 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    36 /    81 | seconds/batch 0.49\n",
      "[Predicting]| Batch    37 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    38 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    39 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    40 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    41 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    42 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    43 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    44 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    45 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    46 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    47 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    48 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    49 /    81 | seconds/batch 0.44\n",
      "[Predicting]| Batch    50 /    81 | seconds/batch 0.56\n",
      "[Predicting]| Batch    51 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    52 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    53 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    54 /    81 | seconds/batch 0.58\n",
      "[Predicting]| Batch    55 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    56 /    81 | seconds/batch 0.57\n",
      "[Predicting]| Batch    57 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    58 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    59 /    81 | seconds/batch 0.65\n",
      "[Predicting]| Batch    60 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    61 /    81 | seconds/batch 0.66\n",
      "[Predicting]| Batch    62 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    63 /    81 | seconds/batch 0.53\n",
      "[Predicting]| Batch    64 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    65 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    66 /    81 | seconds/batch 0.52\n",
      "[Predicting]| Batch    67 /    81 | seconds/batch 0.67\n",
      "[Predicting]| Batch    68 /    81 | seconds/batch 0.55\n",
      "[Predicting]| Batch    69 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    70 /    81 | seconds/batch 0.39\n",
      "[Predicting]| Batch    71 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    72 /    81 | seconds/batch 0.50\n",
      "[Predicting]| Batch    73 /    81 | seconds/batch 0.37\n",
      "[Predicting]| Batch    74 /    81 | seconds/batch 0.42\n",
      "[Predicting]| Batch    75 /    81 | seconds/batch 0.41\n",
      "[Predicting]| Batch    76 /    81 | seconds/batch 0.48\n",
      "[Predicting]| Batch    77 /    81 | seconds/batch 0.47\n",
      "[Predicting]| Batch    78 /    81 | seconds/batch 0.45\n",
      "[Predicting]| Batch    79 /    81 | seconds/batch 0.53\n",
      "32\n",
      "[Predicting]| Batch    80 /    81 | seconds/batch 0.52\n",
      "result= [40, 0.09097096182272384, 0.11967588381570295, 0.17510376214423592, 0.20850515625761157, 0.22955198577857072, 0.11488659829714293, 0.0805146298367626, 0.040594631679710336, 0.029774080842895984, 0.02522340567894309, 0.08206477057441361, 0.07925097494504457, 0.05812993870985666, 0.04754332723045201, 0.042107808803638624, 0.07318841947557655, 0.07890433882270952, 0.08453780065914174, 0.08671974498286425, 0.08779504373981759, 5.302026500389712, 6.48207326578332, 22.347233047544815, 42.214731098986746, 57.214731098986746, 0.5839858562858017]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# import constants\n",
    "# from config import Config\n",
    "# from dream import DreamModel\n",
    "# from data import Dataset, BasketConstructor\n",
    "# from utils import batchify, repackage_hidden, get_grad_norm, get_ratio_update, get_weight_update\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 測試模型\n",
    "# from eval import eval_batch\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# BPR損失函數\n",
    "def bpr_loss(x, dynamic_user, item_embedding, config):\n",
    "    '''\n",
    "        bayesian personalized ranking loss for implicit feedback\n",
    "        parameters:\n",
    "        - x: batch of users' baskets\n",
    "        - dynamic_user: batch of users' dynamic representations\n",
    "        - item_embedding: item_embedding matrix\n",
    "        - config: model configuration\n",
    "    '''\n",
    "    nll = 0\n",
    "    ub_seqs = []\n",
    "    for u, du in zip(x, dynamic_user):\n",
    "        du_p_product = torch.mm(du, item_embedding.t())  # shape: max_len, num_item\n",
    "        nll_u = []  # nll for user\n",
    "        for t, basket_t in enumerate(u):\n",
    "            if basket_t[0] != 0 and t != 0:\n",
    "                pos_idx = torch.tensor(basket_t, dtype=torch.long).to(device) if config.cuda else torch.tensor(basket_t, dtype=torch.long)\n",
    "                # Sample negative products\n",
    "                neg = [random.choice(range(1, config.num_product)) for _ in range(len(basket_t))]  # replacement\n",
    "                neg_idx = torch.tensor(neg).to(device) if config.cuda else torch.tensor(neg)\n",
    "                # Score p(u, t, v > v')\n",
    "                score = du_p_product[t - 1][pos_idx] - du_p_product[t - 1][neg_idx]\n",
    "                # Average Negative log likelihood for basket_t\n",
    "                nll_u.append( (- torch.mean(torch.nn.LogSigmoid()(score)) ).unsqueeze(0))\n",
    "        nll += torch.mean(torch.cat(nll_u))\n",
    "    return nll\n",
    "\n",
    "# 訓練模型\n",
    "def train_dream():\n",
    "    dr_model.train()  # turn on training mode for dropout\n",
    "    dr_hidden = dr_model.init_hidden(dr_config.batch_size)\n",
    "    total_loss = 0\n",
    "    start_time = time()\n",
    "    num_batchs = ceil(len(train_ub) / dr_config.batch_size)\n",
    "    for i, x in enumerate(batchify(train_ub, dr_config.batch_size)):\n",
    "        baskets, lens, _ = x\n",
    "        dr_hidden = repackage_hidden(dr_hidden)  # repackage hidden state for RNN\n",
    "        dr_model.zero_grad()  # optim.zero_grad()\n",
    "        dynamic_user, _ = dr_model(baskets, lens, dr_hidden)\n",
    "        loss = bpr_loss(baskets, dynamic_user, dr_model.encode.weight, dr_config)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid gradient exploding\n",
    "        torch.nn.utils.clip_grad_norm_(dr_model.parameters(), dr_config.clip)\n",
    "\n",
    "        # Parameter updating\n",
    "        # manual SGD\n",
    "        # for p in dr_model.parameters(): # Update parameters by -lr*grad\n",
    "        #    p.data.add_(- dr_config.learning_rate, p.grad.data)\n",
    "        # adam\n",
    "        grad_norm = get_grad_norm(dr_model)\n",
    "        previous_params = deepcopy(list(dr_model.parameters()))\n",
    "        optim.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        params = deepcopy(list(dr_model.parameters()))\n",
    "        delta = get_weight_update(previous_params, params)\n",
    "        weight_update_ratio = get_ratio_update(delta, params)\n",
    "\n",
    "        # Logging\n",
    "        if i % dr_config.log_interval == 0 and i > 0:\n",
    "            elapsed = (time() - start_time) * 1000 / dr_config.log_interval\n",
    "            cur_loss = total_loss / dr_config.log_interval / dr_config.batch_size  # turn tensor into float\n",
    "            total_loss = 0\n",
    "            start_time = time()\n",
    "            print(\n",
    "                '[Training]| Epochs {:3d} | Batch {:5d} / {:5d} | ms/batch {:02.2f} | Loss {:05.2f} |'.format(epoch, i,\n",
    "                                                                                                              num_batchs,\n",
    "                                                                                                              elapsed,\n",
    "                                                                                                              cur_loss))\n",
    "            writer.add_scalar('model/train_loss', cur_loss, epoch * num_batchs + i)\n",
    "            writer.add_scalar('model/grad_norm', grad_norm, epoch * num_batchs + i)\n",
    "            writer.add_scalar('model/weight_update_ratio', weight_update_ratio, epoch * num_batchs + i)\n",
    "\n",
    "# 驗證模型\n",
    "def evaluate_dream():\n",
    "    dr_model.eval()\n",
    "    dr_hidden = dr_model.init_hidden(dr_config.batch_size)\n",
    "\n",
    "    total_loss = 0\n",
    "    start_time = time()\n",
    "    num_batchs = ceil(len(test_ub) / dr_config.batch_size)\n",
    "    for i, x in enumerate(batchify(test_ub, dr_config.batch_size)):\n",
    "        baskets, lens, _ = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, dr_hidden)\n",
    "        loss = bpr_loss(baskets, dynamic_user, dr_model.encode.weight, dr_config)\n",
    "        dr_hidden = repackage_hidden(dr_hidden)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    elapsed = (time() - start_time) * 1000 / num_batchs\n",
    "    total_loss = total_loss / num_batchs / dr_config.batch_size\n",
    "    writer.add_scalar('model/eval_loss', total_loss, (epoch + 1) * num_batchs)\n",
    "    writer.add_scalar('model/eval_loss', total_loss, (epoch + 1) * num_batchs)\n",
    "    print('[Evaluation]| Epochs {:3d} | Elapsed {:02.2f} | Loss {:05.2f} |'.format(epoch, elapsed, total_loss))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "# 測試模型\n",
    "def test_dream(dr_model, data_type, epoch, val_loss):\n",
    "    \n",
    "    dr_model.eval()\n",
    "\n",
    "    # 測試集&其標籤\n",
    "    with open( FEAT_DATA_DIR + f'./{data_type}/test_ub.pkl', 'rb') as f:\n",
    "        test_ub = pickle.load(f)\n",
    "    with open( FEAT_DATA_DIR + f'./{data_type}/test_ub_label.pkl', 'rb') as f:\n",
    "        test_ub_label = pickle.load(f)\n",
    "\n",
    "    # print(\"test_ub_label=\",test_ub_label)\n",
    "\n",
    "    bc = BasketConstructor( RAW_DATA_DIR, FEAT_DATA_DIR )\n",
    "    # print(\"ub_basket=\\n\",test_ub[:5])\n",
    "    ub = Dataset(test_ub) \n",
    "    #up = bc.get_users_products(test_ub, data_type)\n",
    "    # print(\"users_products=\\n\",up)\n",
    "    res = model_test(dr_model, ub)\n",
    "\n",
    "    dream_final = res.sort_values(['user_id','dream_score_next'],ascending=[True,False])[['user_id','product_id','dream_score_next']]\n",
    "    dream_final_list = dream_final.groupby(['user_id'])['product_id'].apply(list)\n",
    "    dream_final_list.columns = ['user_id', 'product_id']\n",
    "    k_list = [5,10,30,50,65]\n",
    "    f1_score_at_k_eval, recall_at_k_eval, precision_at_k_eval = calculate_f1_score_at_k(dream_final_list, k_list)\n",
    "    ndcg_at_k_eval = calculate_ndcg_at_k(dream_final_list, k_list)\n",
    "    mae_at_k_eval = calculate_mae_at_k(dream_final_list, k_list)\n",
    "\n",
    "    f1_5_rec = f1_score_at_k_eval['F1-score@5']\n",
    "    f1_10_rec = f1_score_at_k_eval['F1-score@10']\n",
    "    f1_30_rec = f1_score_at_k_eval['F1-score@30']\n",
    "    f1_50_rec = f1_score_at_k_eval['F1-score@50']\n",
    "    f1_65_rec = f1_score_at_k_eval['F1-score@65']\n",
    "    f1_list = [f1_5_rec, f1_10_rec, f1_30_rec, f1_50_rec, f1_65_rec]\n",
    "    \n",
    "    recall_5_rec = recall_at_k_eval['Recall@5']\n",
    "    recall_10_rec = recall_at_k_eval['Recall@10']\n",
    "    recall_30_rec = recall_at_k_eval['Recall@30']\n",
    "    recall_50_rec = recall_at_k_eval['Recall@50']\n",
    "    recall_65_rec = recall_at_k_eval['Recall@65']\n",
    "    recall_list = [recall_5_rec, recall_10_rec, recall_30_rec, recall_50_rec, recall_65_rec]\n",
    "    \n",
    "    precision_5_rec = precision_at_k_eval['Precision@5']\n",
    "    precision_10_rec = precision_at_k_eval['Precision@10']\n",
    "    precision_30_rec = precision_at_k_eval['Precision@30']\n",
    "    precision_50_rec = precision_at_k_eval['Precision@50']\n",
    "    precision_65_rec = precision_at_k_eval['Precision@65']\n",
    "    precision_list = [precision_5_rec, precision_10_rec, precision_30_rec, precision_50_rec, precision_65_rec]\n",
    "    \n",
    "    ndcg_5_rec = ndcg_at_k_eval['NDCG@5']\n",
    "    ndcg_10_rec = ndcg_at_k_eval['NDCG@10']\n",
    "    ndcg_30_rec = ndcg_at_k_eval['NDCG@30']\n",
    "    ndcg_50_rec = ndcg_at_k_eval['NDCG@50']\n",
    "    ndcg_65_rec = ndcg_at_k_eval['NDCG@65']\n",
    "    ndcg_list = [ndcg_5_rec, ndcg_10_rec, ndcg_30_rec, ndcg_50_rec, ndcg_65_rec]\n",
    "    \n",
    "    mae_5_rec = mae_at_k_eval['MAE@5']\n",
    "    mae_10_rec = mae_at_k_eval['MAE@10']\n",
    "    mae_30_rec = mae_at_k_eval['MAE@30']\n",
    "    mae_50_rec = mae_at_k_eval['MAE@50']\n",
    "    mae_65_rec = mae_at_k_eval['MAE@65']\n",
    "    mae_list = [mae_5_rec, mae_10_rec, mae_30_rec, mae_50_rec, mae_65_rec]\n",
    "    \n",
    "    result = [epoch+1] + recall_list + precision_list + f1_list + ndcg_list + mae_list + [val_loss]\n",
    "    return result\n",
    "\n",
    "\n",
    "def model_test(dr_model, ub):\n",
    "    dr_model.config.cuda = True\n",
    "    id_u, item_u, score_u, dynamic_u = eval_batch(dr_model, ub,  dr_model.config.batch_size , is_reordered = False)\n",
    "    len_u = [i.shape[0] for i in item_u] # number of products for each user\n",
    "    flatten_id = np.repeat(id_u, len_u) # repeat id_u to ensure the same length as item_u\n",
    "    flatten_item = [i for u in item_u for i in u]\n",
    "    flatten_score = [s for u in score_u for s in u[:-1]]\n",
    "    res = pd.DataFrame({'user_id': flatten_id, 'product_id': flatten_item, 'dream_score': flatten_score})\n",
    "    res.drop_duplicates(inplace=True)\n",
    "    res = res.groupby(['user_id', 'product_id'])['dream_score'].mean().reset_index()\n",
    "    res.columns = ['user_id', 'product_id', 'dream_score_next']\n",
    "    return res\n",
    "\n",
    "def calculate_f1_score_at_k( dream_final_list, k_list ):\n",
    "    f1_score_at_k_eval = dict()\n",
    "    recall_at_k_eval = dict()\n",
    "    precision_at_k_eval = dict()\n",
    "    for k in k_list:\n",
    "        f1_score_sum = 0.0\n",
    "        recall_sum = 0.0\n",
    "        precision_sum = 0.0\n",
    "        for i,user_pred_items in enumerate(dream_final_list):\n",
    "            user_id = dream_final_list.index[i]\n",
    "            # 預測的TopK結果\n",
    "            top_k_item_indices = user_pred_items[:k]\n",
    "            top_k_item_indices = torch.from_numpy(np.array(top_k_item_indices, dtype=np.int64))\n",
    "            # label_list\n",
    "            label_list = test_ub_label[test_ub_label['user_id']==user_id]['label'].values[0]\n",
    "            labels = torch.from_numpy(np.array(label_list, dtype=np.int64))\n",
    "            # 計算用戶 i 的真實標籤和預測標籤的交集。\n",
    "            true_positives = torch.sum(torch.sum(torch.eq(top_k_item_indices, labels.unsqueeze(1)).to(torch.float32), dim=1))\n",
    "            # print(\"true_positives=\",true_positives)\n",
    "            # 計算用戶 i 的真實標籤和預測標籤的並集。\n",
    "            predicted_positives = k\n",
    "            actual_positives = len(labels)\n",
    "            if actual_positives == 0:\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "            else:\n",
    "                precision = true_positives / predicted_positives\n",
    "                recall = true_positives / actual_positives\n",
    "            # 計算 F1-score。\n",
    "            if precision + recall == 0:\n",
    "                f1_score = 0.0\n",
    "            else:\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "            f1_score_sum += f1_score\n",
    "            recall_sum += recall\n",
    "            precision_sum += precision\n",
    "        # 計算平均 F1-score@K 分數。\n",
    "        f1_score_at_k = float(f1_score_sum) / float(len(dream_final_list))\n",
    "        recall_at_k = float(recall_sum) / float(len(dream_final_list))\n",
    "        precision_at_k = float(precision_sum) / float(len(dream_final_list))\n",
    "        key = '{}@{}'.format('F1-score',k)\n",
    "        f1_score_at_k_eval[key]=f1_score_at_k\n",
    "        key = '{}@{}'.format('Recall',k)\n",
    "        recall_at_k_eval[key]=recall_at_k\n",
    "        key = '{}@{}'.format('Precision',k)\n",
    "        precision_at_k_eval[key]=precision_at_k\n",
    "    return f1_score_at_k_eval, recall_at_k_eval, precision_at_k_eval\n",
    "\n",
    "def calculate_ndcg_at_k( dream_final_list, k_list ):\n",
    "    ndcg_at_k_eval = dict()\n",
    "    for k in k_list:\n",
    "        ndcg_sum = 0.0\n",
    "        for i,user_pred_items in enumerate(dream_final_list):\n",
    "            user_id = dream_final_list.index[i]\n",
    "            # 預測的TopK結果\n",
    "            top_k_item_indices = user_pred_items[:k]\n",
    "            top_k_item_indices = torch.from_numpy(np.array(top_k_item_indices, dtype=np.int64))\n",
    "            # label_list\n",
    "            label_list = test_ub_label[test_ub_label['user_id']==user_id]['label'].values[0]\n",
    "            labels = torch.from_numpy(np.array(label_list, dtype=np.int64))\n",
    "            \n",
    "            # 計算 DCG@K。\n",
    "            if k>len(top_k_item_indices):\n",
    "                k_ = len(top_k_item_indices)\n",
    "            else:\n",
    "                k_ = k\n",
    "\n",
    "            dcg_at_k = torch.sum(torch.div(1.0, torch.log2(torch.arange(k_, dtype=torch.float32) + 2)) * (torch.eq(top_k_item_indices, labels.unsqueeze(1)).to(torch.float32) / torch.log2(torch.arange(k_, dtype=torch.float32) + 2)))\n",
    "            # 計算 IDCG@K。\n",
    "            idcg_at_k = torch.sum(torch.div(1.0, torch.log2(torch.arange(len(labels), dtype=torch.float32) + 2)))\n",
    "            \n",
    "            # 計算 NDCG@K。\n",
    "            ndcg_at_k = dcg_at_k / idcg_at_k\n",
    "            ndcg_sum += ndcg_at_k.item()\n",
    "\n",
    "        # 計算平均 NDCG@K 分數。\n",
    "        ndcg_at_k = ndcg_sum / float(len(dream_final_list))\n",
    "        key = '{}@{}'.format('NDCG',k)\n",
    "        ndcg_at_k_eval[key]=ndcg_at_k\n",
    "    return ndcg_at_k_eval\n",
    "\n",
    "# MAE\n",
    "def calculate_mae_at_k(dream_final_list, k_list):\n",
    "    mae_eval = dict()\n",
    "    for k in k_list:\n",
    "        mae_sum = 0.0\n",
    "        for i,user_pred_items in enumerate(dream_final_list):\n",
    "            user_id = dream_final_list.index[i]\n",
    "            # label_list\n",
    "            label_list = test_ub_label[test_ub_label['user_id']==user_id]['label'].values[0]\n",
    "            labels = torch.from_numpy(np.array(label_list, dtype=np.int64))\n",
    "            mae_sum += abs(k - len(labels))\n",
    "        key = \"{}@{}\".format(\"MAE\", k)\n",
    "        mae_eval[key] = mae_sum / float(len(dream_final_list))\n",
    "\n",
    "    return mae_eval\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUS\n",
    "\n",
    "dr_config = Config(DREAM_CONFIG)\n",
    "\n",
    "rebuild = True # 重建訓練集、測試集\n",
    "\n",
    "\n",
    "# ---------------------------- #\n",
    "#       訓練 & 驗證 & 測試      #\n",
    "# ---------------------------- #\n",
    "\n",
    "for train_num in range( 1, dr_config.train_times+1 ):   # 跑10次實驗\n",
    "\n",
    "    # Prepare input\n",
    "    bc = BasketConstructor(RAW_DATA_DIR, FEAT_DATA_DIR)\n",
    "    # Users' baskets\n",
    "    ub_basket = bc.get_baskets( dr_config.data_name, reconstruct=True)\n",
    "    print(\"ub_basket=\\n\",ub_basket)\n",
    "\n",
    "    # 測試集標籤\n",
    "    filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/test_ub_label.pkl'\n",
    "    if  not rebuild and os.path.exists(filepath) :\n",
    "        with open(filepath, 'rb') as f:\n",
    "            test_ub_label = pickle.load(f)\n",
    "        # 測試集\n",
    "        filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/test_ub.pkl'\n",
    "        with open(filepath, 'rb') as f:\n",
    "            test_ub = pickle.load(f)\n",
    "        # 訓練集\n",
    "        filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/train_ub.pkl' \n",
    "        with open(filepath, 'rb') as f:\n",
    "            train_ub = pickle.load(f)\n",
    "    else:\n",
    "        train_ub, test_ub = train_test_split(ub_basket, test_size=0.2, shuffle=True)\n",
    "        test_ub = test_ub.reset_index(drop=True)\n",
    "        #  test_ub : 測試資料，不包含用戶的最後一個購物籃\n",
    "        #  test_ub_label : 測試資料的標籤(用戶的最後一個購物籃項目ID)\n",
    "        test_ub_label = test_ub.loc[:, ['user_id','basket']]\n",
    "        test_ub_label.columns = ['user_id','label']\n",
    "\n",
    "        for i,bsk in enumerate(test_ub[\"basket\"]):\n",
    "            test_ub_label[\"label\"][i] = test_ub[ \"basket\"][i][-1]\n",
    "            test_ub[\"basket\"][i] = test_ub[\"basket\"][i][:-1]\n",
    "        \n",
    "        filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/test_ub_label.pkl'\n",
    "        # 測試集標籤\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(test_ub_label, f, pickle.HIGHEST_PROTOCOL)\n",
    "        # 測試集\n",
    "        filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/test_ub.pkl' \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(test_ub, f, pickle.HIGHEST_PROTOCOL)\n",
    "        # 訓練集\n",
    "        filepath = FEAT_DATA_DIR + f'/{dr_config.data_name}/train_ub.pkl' \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(train_ub, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"test_ub_label=\",test_ub_label)\n",
    "    print(\"ub_basket=\\n\",test_ub[:5])\n",
    "        \n",
    "    del ub_basket\n",
    "\n",
    "    dir_path = os.getcwd() + DREAM_MODEL_DIR + f'/{dr_config.data_name}'\n",
    "    train_ub, test_ub = Dataset(train_ub), Dataset(test_ub)\n",
    "\n",
    "    # Model config\n",
    "    dr_model = DreamModel(dr_config)\n",
    "    if dr_config.cuda:\n",
    "        dr_model.cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optim = torch.optim.Adam(dr_model.parameters(), lr = dr_config.learning_rate)\n",
    "    # optim = torch.optim.Adadelta(dr_model.parameters())\n",
    "    # optim = torch.optim.SGD(dr_model.parameters(), lr=dr_config.learning_rate, momentum=0.9)\n",
    "    writer = SummaryWriter(log_dir= 'runs/{}/{}'.format(dr_config.data_name, dr_config.alias))  # tensorboard writer\n",
    "    writer.add_text('config', str(dr_config))\n",
    "    best_val_loss = None\n",
    "\n",
    "    try:\n",
    "        for k,v in DREAM_CONFIG.items():\n",
    "            print(k,v)\n",
    "        # training\n",
    "        results = []\n",
    "        for epoch in range(dr_config.epochs):\n",
    "            train_dream()\n",
    "            print('-' * 89)\n",
    "            val_loss = evaluate_dream()\n",
    "            print('-' * 89)\n",
    "            # checkpoint\n",
    "            path = DREAM_MODEL_DIR + f'/{dr_config.data_name}/'\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "            # with open(dr_config.checkpoint_dir.format( df = dr_config.df_num, train_num = train_num ,epoch = epoch, loss = val_loss), 'wb') as f:\n",
    "            #     torch.save(dr_model, f)\n",
    "            \n",
    "            # 測試模型\n",
    "            result = test_dream(dr_model, dr_config.data_name , epoch, val_loss)\n",
    "            results.append(result)\n",
    "            print(\"result=\", result)\n",
    "            record_df = pd.DataFrame(results,columns=['Epoch','Recall@5', 'Recall@10', 'Recall@30', 'Recall@50', 'Recall@65',\n",
    "                                                      'Precision@5', 'Precision@10', 'Precision@30', 'Precision@50', 'Precision@65',\n",
    "                                                      'F1-score@5', 'F1-score@10', 'F1-score@30', 'F1-score@50', 'F1-score@65',\n",
    "                                                      'NDCG@5', 'NDCG@10', 'NDCG@30', 'NDCG@50', 'NDCG@65',\n",
    "                                                      'MAE@5', 'MAE@10', 'MAE@30', 'MAE@50', 'MAE@65', 'loss'])\n",
    "            path = dir_path + f'/output_{train_num}.csv'\n",
    "            record_df.to_csv(path, index=False)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('*' * 89)\n",
    "        print('Early Stopping!')\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# 執行時，當前目錄須移至\"\\程式碼\\ASBRec_Code\\baseline\\DREAM\"\n",
    "# python src\\train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
