{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835eb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"TaFeng\"\n",
    "# DATASET_NAME = \"Dunnhumby\"\n",
    "# DATASET_NAME = \"Instacart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648e4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dictionary generation...\n",
      "{'MATERIAL_NUMBER': 12085}\n",
      "# dimensions of final vector: 12085 | 521\n",
      "finish dictionary generation*****\n",
      "num of vectors having entries more than 1: 46123\n",
      "num of vectors having entries more than 1: 44383\n",
      "data_chunk;) <class 'list'>\n",
      "Number of training instances: 9860\n",
      "Number of valid instances: 1232\n",
      "Number of test instances: 1233\n",
      "Epoch:  1\n",
      "k = 5\n",
      "average Recall: 0.021121644359136778\n",
      "average Precision: 0.014772727272727274\n",
      "average F score: 0.013521263155725894\n",
      "average NDCG: 0.017474400496634252\n",
      "average Recall: 0.01721125044461924\n",
      "average Precision: 0.013787510137875101\n",
      "average F score: 0.011538814207142901\n",
      "average NDCG: 0.014473399652113538\n",
      "k = 10\n",
      "average Recall: 0.027651811547409158\n",
      "average Precision: 0.011282467532467534\n",
      "average F score: 0.013088914125398202\n",
      "average NDCG: 0.02098856179428205\n",
      "average Recall: 0.022016355637966348\n",
      "average Precision: 0.009732360097323603\n",
      "average F score: 0.010700606679151362\n",
      "average NDCG: 0.017034427489999504\n",
      "k = 15\n",
      "average Recall: 0.03289590905664164\n",
      "average Precision: 0.010119047619047621\n",
      "average F score: 0.013036338860587129\n",
      "average NDCG: 0.023534311823833955\n",
      "average Recall: 0.026183203824164312\n",
      "average Precision: 0.008867261422005947\n",
      "average F score: 0.01089246230114524\n",
      "average NDCG: 0.019186487155713597\n",
      "k = 40\n",
      "average Recall: 0.06085998478795719\n",
      "average Precision: 0.008015422077922078\n",
      "average F score: 0.012849147417235184\n",
      "average NDCG: 0.03347236481363587\n",
      "average Recall: 0.050412648658069895\n",
      "average Precision: 0.0065085158150851585\n",
      "average F score: 0.01041168634181465\n",
      "average NDCG: 0.027496107250758725\n",
      "Epoch:  2\n",
      "k = 5\n",
      "average Recall: 0.03268034089947582\n",
      "average Precision: 0.018993506493506493\n",
      "average F score: 0.01881146062252337\n",
      "average NDCG: 0.02472191419533972\n",
      "average Recall: 0.03909521695786919\n",
      "average Precision: 0.021735604217356045\n",
      "average F score: 0.022177677511157762\n",
      "average NDCG: 0.029164222900977515\n",
      "k = 10\n",
      "average Recall: 0.044596161395130764\n",
      "average Precision: 0.015178571428571428\n",
      "average F score: 0.018035281639138905\n",
      "average NDCG: 0.030614152572330237\n",
      "average Recall: 0.04928569865547122\n",
      "average Precision: 0.01654501216545012\n",
      "average F score: 0.020025591830300244\n",
      "average NDCG: 0.03460673588992303\n",
      "k = 15\n",
      "average Recall: 0.05027549778212177\n",
      "average Precision: 0.013149350649350649\n",
      "average F score: 0.01726169117394476\n",
      "average NDCG: 0.03354902176768474\n",
      "average Recall: 0.0547132015003828\n",
      "average Precision: 0.013192754798594214\n",
      "average F score: 0.017773581182490835\n",
      "average NDCG: 0.03703275336214105\n",
      "k = 40\n",
      "average Recall: 0.08311214113092498\n",
      "average Precision: 0.010369318181818183\n",
      "average F score: 0.016653396928985566\n",
      "average NDCG: 0.04554306922689371\n",
      "average Recall: 0.0918222572946291\n",
      "average Precision: 0.010117599351175994\n",
      "average F score: 0.01631013253095007\n",
      "average NDCG: 0.04938444722772186\n",
      "Epoch:  3\n",
      "k = 5\n",
      "average Recall: 0.027199252047851254\n",
      "average Precision: 0.01818181818181818\n",
      "average F score: 0.016946568660228808\n",
      "average NDCG: 0.027964851012031756\n",
      "average Recall: 0.03302504166484758\n",
      "average Precision: 0.019789132197891326\n",
      "average F score: 0.019786865376734145\n",
      "average NDCG: 0.0356189035755501\n",
      "k = 10\n",
      "average Recall: 0.045084040239155444\n",
      "average Precision: 0.015827922077922076\n",
      "average F score: 0.01866761715722694\n",
      "average NDCG: 0.03620811015399343\n",
      "average Recall: 0.053515373777275846\n",
      "average Precision: 0.01824817518248175\n",
      "average F score: 0.022112292711432203\n",
      "average NDCG: 0.045271809966787036\n",
      "k = 15\n",
      "average Recall: 0.06563336357680404\n",
      "average Precision: 0.01590909090909091\n",
      "average F score: 0.021194890742732744\n",
      "average NDCG: 0.043827563276882434\n",
      "average Recall: 0.0803787120712027\n",
      "average Precision: 0.01730197350635307\n",
      "average F score: 0.023894792574963408\n",
      "average NDCG: 0.05434001352382325\n",
      "k = 40\n",
      "average Recall: 0.10302321008080868\n",
      "average Precision: 0.012256493506493507\n",
      "average F score: 0.01974244654950676\n",
      "average NDCG: 0.05751809564962359\n",
      "average Recall: 0.11701140159721467\n",
      "average Precision: 0.011901865369018653\n",
      "average F score: 0.019402530285539135\n",
      "average NDCG: 0.06703108210837354\n",
      "Epoch:  4\n",
      "k = 5\n",
      "average Recall: 0.026666605670493306\n",
      "average Precision: 0.017857142857142856\n",
      "average F score: 0.01656944813756657\n",
      "average NDCG: 0.02804936518338618\n",
      "average Recall: 0.03335528917248857\n",
      "average Precision: 0.021086780210867802\n",
      "average F score: 0.02066014410609342\n",
      "average NDCG: 0.036192962700185426\n",
      "k = 10\n",
      "average Recall: 0.05866368898452152\n",
      "average Precision: 0.018993506493506493\n",
      "average F score: 0.02310019679311142\n",
      "average NDCG: 0.04210128925837631\n",
      "average Recall: 0.07621383087620055\n",
      "average Precision: 0.022384428223844285\n",
      "average F score: 0.028459785898611818\n",
      "average NDCG: 0.05406422015946287\n",
      "k = 15\n",
      "average Recall: 0.06594641931184948\n",
      "average Precision: 0.01607142857142857\n",
      "average F score: 0.021333146866203433\n",
      "average NDCG: 0.045529836460348645\n",
      "average Recall: 0.08154868534691202\n",
      "average Precision: 0.01762638550959719\n",
      "average F score: 0.024337867515467876\n",
      "average NDCG: 0.056669974781459305\n",
      "k = 40\n",
      "average Recall: 0.11176306281584163\n",
      "average Precision: 0.013839285714285715\n",
      "average F score: 0.022203894749035506\n",
      "average NDCG: 0.06268097623997093\n",
      "average Recall: 0.12768187653249266\n",
      "average Precision: 0.013199513381995135\n",
      "average F score: 0.021534915327068874\n",
      "average NDCG: 0.07263718043489521\n",
      "Epoch:  5\n",
      "k = 5\n",
      "average Recall: 0.026694695065465825\n",
      "average Precision: 0.01931818181818182\n",
      "average F score: 0.01722371527286905\n",
      "average NDCG: 0.028834972713759746\n",
      "average Recall: 0.03832643061542739\n",
      "average Precision: 0.023682076236820764\n",
      "average F score: 0.023172121947969648\n",
      "average NDCG: 0.03887626812348164\n",
      "k = 10\n",
      "average Recall: 0.06333740058139413\n",
      "average Precision: 0.021996753246753247\n",
      "average F score: 0.02619836844563918\n",
      "average NDCG: 0.04510953152779538\n",
      "average Recall: 0.0781067155629769\n",
      "average Precision: 0.02368207623682076\n",
      "average F score: 0.029937849568617943\n",
      "average NDCG: 0.056139702799659734\n",
      "k = 15\n",
      "average Recall: 0.07200925381146668\n",
      "average Precision: 0.01904761904761905\n",
      "average F score: 0.024905430087818963\n",
      "average NDCG: 0.04935101446999309\n",
      "average Recall: 0.08627594789413623\n",
      "average Precision: 0.01941065152743985\n",
      "average F score: 0.026746336068497956\n",
      "average NDCG: 0.060005953933115995\n",
      "k = 40\n",
      "average Recall: 0.11949919214406482\n",
      "average Precision: 0.01495535714285714\n",
      "average F score: 0.023982465179517222\n",
      "average NDCG: 0.0669651629267379\n",
      "average Recall: 0.13642623324766315\n",
      "average Precision: 0.014213300892133008\n",
      "average F score: 0.02313968431619613\n",
      "average NDCG: 0.07725091450698197\n",
      "Epoch:  6\n",
      "k = 5\n",
      "average Recall: 0.04165776706177907\n",
      "average Precision: 0.027922077922077928\n",
      "average F score: 0.02580786832812569\n",
      "average NDCG: 0.03874740216037789\n",
      "average Recall: 0.058736590030325636\n",
      "average Precision: 0.03260340632603406\n",
      "average F score: 0.033249527571549274\n",
      "average NDCG: 0.0531317717326595\n",
      "k = 10\n",
      "average Recall: 0.07069933178822643\n",
      "average Precision: 0.026704545454545453\n",
      "average F score: 0.03127610917805263\n",
      "average NDCG: 0.052705033348334165\n",
      "average Recall: 0.08429216907647782\n",
      "average Precision: 0.026682887266828877\n",
      "average F score: 0.0333656133934515\n",
      "average NDCG: 0.06515610992986051\n",
      "k = 15\n",
      "average Recall: 0.07912370492565618\n",
      "average Precision: 0.022186147186147184\n",
      "average F score: 0.02875593122504336\n",
      "average NDCG: 0.05687368224413107\n",
      "average Recall: 0.09473767102316075\n",
      "average Precision: 0.02195187888618546\n",
      "average F score: 0.030022421445253265\n",
      "average NDCG: 0.06973891474548982\n",
      "k = 40\n",
      "average Recall: 0.1323442391348036\n",
      "average Precision: 0.01635551948051948\n",
      "average F score: 0.02631118364777523\n",
      "average NDCG: 0.07569763439010947\n",
      "average Recall: 0.14980220346271805\n",
      "average Precision: 0.015409570154095702\n",
      "average F score: 0.02511198434740649\n",
      "average NDCG: 0.08798136438699392\n",
      "Epoch:  7\n",
      "k = 5\n",
      "average Recall: 0.04757113091610607\n",
      "average Precision: 0.032792207792207796\n",
      "average F score: 0.03038972959593497\n",
      "average NDCG: 0.04429667804077003\n",
      "average Recall: 0.06185601599387176\n",
      "average Precision: 0.03600973236009733\n",
      "average F score: 0.03642575237388513\n",
      "average NDCG: 0.05715535525631672\n",
      "k = 10\n",
      "average Recall: 0.07593054990615533\n",
      "average Precision: 0.029951298701298704\n",
      "average F score: 0.034675557398615324\n",
      "average NDCG: 0.058105987469174905\n",
      "average Recall: 0.08865119666014452\n",
      "average Precision: 0.028710462287104627\n",
      "average F score: 0.03571884601704731\n",
      "average NDCG: 0.06965005829355522\n",
      "k = 15\n",
      "average Recall: 0.0866650160559204\n",
      "average Precision: 0.025162337662337664\n",
      "average F score: 0.03236423948833843\n",
      "average NDCG: 0.06321980175017765\n",
      "average Recall: 0.10583776236010799\n",
      "average Precision: 0.024547174912138415\n",
      "average F score: 0.03360219829927773\n",
      "average NDCG: 0.07650950911661736\n",
      "k = 40\n",
      "average Recall: 0.14750821028502314\n",
      "average Precision: 0.018060064935064936\n",
      "average F score: 0.029126501913070604\n",
      "average NDCG: 0.08417186625428472\n",
      "average Recall: 0.16316705172799126\n",
      "average Precision: 0.016747769667477694\n",
      "average F score: 0.027321431046126367\n",
      "average NDCG: 0.09535772359441166\n",
      "Epoch:  8\n",
      "k = 5\n",
      "average Recall: 0.05052036308618234\n",
      "average Precision: 0.03847402597402597\n",
      "average F score: 0.03427303615818612\n",
      "average NDCG: 0.048036669353038776\n",
      "average Recall: 0.06834691482411509\n",
      "average Precision: 0.04120032441200325\n",
      "average F score: 0.04128149875758647\n",
      "average NDCG: 0.061699185671207336\n",
      "k = 10\n",
      "average Recall: 0.08214627387189123\n",
      "average Precision: 0.033279220779220776\n",
      "average F score: 0.038436971686539374\n",
      "average NDCG: 0.06294266236551245\n",
      "average Recall: 0.09820537093444387\n",
      "average Precision: 0.03179237631792377\n",
      "average F score: 0.03951708552505085\n",
      "average NDCG: 0.07484689421976198\n",
      "k = 15\n",
      "average Recall: 0.09355013104108162\n",
      "average Precision: 0.02835497835497836\n",
      "average F score: 0.03631102697132767\n",
      "average NDCG: 0.0687118457365611\n",
      "average Recall: 0.11468305152962621\n",
      "average Precision: 0.026926196269261966\n",
      "average F score: 0.036965044069534075\n",
      "average NDCG: 0.08180323358681668\n",
      "k = 40\n",
      "average Recall: 0.15433571239646826\n",
      "average Precision: 0.018750000000000003\n",
      "average F score: 0.030281659264577876\n",
      "average NDCG: 0.08930031188376403\n",
      "average Recall: 0.17169256857496684\n",
      "average Precision: 0.01759935117599351\n",
      "average F score: 0.028744315869176428\n",
      "average NDCG: 0.10047736585517893\n",
      "Epoch:  9\n",
      "k = 5\n",
      "average Recall: 0.05721245992253289\n",
      "average Precision: 0.045292207792207786\n",
      "average F score: 0.039858295159018224\n",
      "average NDCG: 0.05371608977982744\n",
      "average Recall: 0.07567982978552654\n",
      "average Precision: 0.04736415247364153\n",
      "average F score: 0.04658514169388569\n",
      "average NDCG: 0.06688019079970975\n",
      "k = 10\n",
      "average Recall: 0.0851168584947042\n",
      "average Precision: 0.036850649350649356\n",
      "average F score: 0.041762125814305676\n",
      "average NDCG: 0.06750925773924542\n",
      "average Recall: 0.10278348805284371\n",
      "average Precision: 0.03471208434712084\n",
      "average F score: 0.042674633624802255\n",
      "average NDCG: 0.0793373887625139\n",
      "k = 15\n",
      "average Recall: 0.10262621689081952\n",
      "average Precision: 0.03051948051948052\n",
      "average F score: 0.039361231880945785\n",
      "average NDCG: 0.07469884842268122\n",
      "average Recall: 0.11825469989835266\n",
      "average Precision: 0.02822384428223844\n",
      "average F score: 0.03858685844363812\n",
      "average NDCG: 0.0857788805946955\n",
      "k = 40\n",
      "average Recall: 0.1643156153526724\n",
      "average Precision: 0.019926948051948053\n",
      "average F score: 0.03226982710467562\n",
      "average NDCG: 0.09585490947918576\n",
      "average Recall: 0.1779792759910359\n",
      "average Precision: 0.01843065693430657\n",
      "average F score: 0.03009362597115676\n",
      "average NDCG: 0.10561106927458995\n",
      "Epoch:  10\n",
      "k = 5\n",
      "average Recall: 0.060453870719210115\n",
      "average Precision: 0.04983766233766234\n",
      "average F score: 0.04345900114841744\n",
      "average NDCG: 0.05808850432509587\n",
      "average Recall: 0.08112810284725565\n",
      "average Precision: 0.05093268450932685\n",
      "average F score: 0.05006459488564991\n",
      "average NDCG: 0.07157883696174594\n",
      "k = 10\n",
      "average Recall: 0.09048595763669047\n",
      "average Precision: 0.04001623376623376\n",
      "average F score: 0.04524352543829612\n",
      "average NDCG: 0.07291913984552358\n",
      "average Recall: 0.1060393421682548\n",
      "average Precision: 0.0373073803730738\n",
      "average F score: 0.045635169994797296\n",
      "average NDCG: 0.08371724854666428\n",
      "k = 15\n",
      "average Recall: 0.1096156825932755\n",
      "average Precision: 0.033279220779220776\n",
      "average F score: 0.0427803532432556\n",
      "average NDCG: 0.0808156535533668\n",
      "average Recall: 0.12176427539674044\n",
      "average Precision: 0.030170316301703165\n",
      "average F score: 0.04083727260727092\n",
      "average NDCG: 0.09013583926022374\n",
      "k = 40\n",
      "average Recall: 0.17277801319658456\n",
      "average Precision: 0.021022727272727273\n",
      "average F score: 0.03405525080738372\n",
      "average NDCG: 0.10235185419471463\n",
      "average Recall: 0.1845873635583354\n",
      "average Precision: 0.019343065693430656\n",
      "average F score: 0.031578772117010316\n",
      "average NDCG: 0.11095021695659396\n",
      "Epoch:  11\n",
      "k = 5\n",
      "average Recall: 0.06740974307354192\n",
      "average Precision: 0.0577922077922078\n",
      "average F score: 0.050038024208473156\n",
      "average NDCG: 0.06423302976109788\n",
      "average Recall: 0.08520701304646068\n",
      "average Precision: 0.054825628548256296\n",
      "average F score: 0.05401884676016601\n",
      "average NDCG: 0.0761042695274484\n",
      "k = 10\n",
      "average Recall: 0.09305776463479368\n",
      "average Precision: 0.042288961038961036\n",
      "average F score: 0.04762312977393024\n",
      "average NDCG: 0.07699010361578555\n",
      "average Recall: 0.10783232577310402\n",
      "average Precision: 0.038199513381995134\n",
      "average F score: 0.04675983713746653\n",
      "average NDCG: 0.08727670569105744\n",
      "k = 15\n",
      "average Recall: 0.11617012279044944\n",
      "average Precision: 0.036038961038961044\n",
      "average F score: 0.046494856799192345\n",
      "average NDCG: 0.08660138123181244\n",
      "average Recall: 0.12709151740239427\n",
      "average Precision: 0.032765612327656124\n",
      "average F score: 0.04399632971624076\n",
      "average NDCG: 0.09554163052117699\n",
      "k = 40\n",
      "average Recall: 0.17673516172186976\n",
      "average Precision: 0.02159090909090909\n",
      "average F score: 0.03496327952106497\n",
      "average NDCG: 0.10718746241628023\n",
      "average Recall: 0.1918401866133524\n",
      "average Precision: 0.020072992700729927\n",
      "average F score: 0.032749326068266735\n",
      "average NDCG: 0.11646816171252318\n",
      "Epoch:  12\n",
      "k = 5\n",
      "average Recall: 0.0697741549437223\n",
      "average Precision: 0.06120129870129871\n",
      "average F score: 0.052883668200745054\n",
      "average NDCG: 0.06756317088171587\n",
      "average Recall: 0.08815788800520642\n",
      "average Precision: 0.05660989456609895\n",
      "average F score: 0.05603663844617612\n",
      "average NDCG: 0.07704684312620709\n",
      "k = 10\n",
      "average Recall: 0.09823830247205899\n",
      "average Precision: 0.045698051948051946\n",
      "average F score: 0.051540694043265896\n",
      "average NDCG: 0.08200441585147171\n",
      "average Recall: 0.10930101962643504\n",
      "average Precision: 0.0407948094079481\n",
      "average F score: 0.04924794076687756\n",
      "average NDCG: 0.08842522821306968\n",
      "k = 15\n",
      "average Recall: 0.12333419523714927\n",
      "average Precision: 0.037554112554112555\n",
      "average F score: 0.04866734971176474\n",
      "average NDCG: 0.09157836763288987\n",
      "average Recall: 0.13604077130213027\n",
      "average Precision: 0.03471208434712085\n",
      "average F score: 0.046749515513188775\n",
      "average NDCG: 0.09856190383262199\n",
      "k = 40\n",
      "average Recall: 0.17936931648414625\n",
      "average Precision: 0.02207792207792208\n",
      "average F score: 0.035744203545270394\n",
      "average NDCG: 0.11131193569107577\n",
      "average Recall: 0.19456294641603356\n",
      "average Precision: 0.020397404703974047\n",
      "average F score: 0.03326968097463306\n",
      "average NDCG: 0.11776616106064607\n",
      "Epoch:  13\n",
      "k = 5\n",
      "average Recall: 0.0695715848369745\n",
      "average Precision: 0.062337662337662345\n",
      "average F score: 0.05346162523248009\n",
      "average NDCG: 0.06895668351001337\n",
      "average Recall: 0.08592351903034946\n",
      "average Precision: 0.05709651257096513\n",
      "average F score: 0.05578752208671938\n",
      "average NDCG: 0.07698036335970188\n",
      "k = 10\n",
      "average Recall: 0.10415283333526248\n",
      "average Precision: 0.0476461038961039\n",
      "average F score: 0.0540437437240423\n",
      "average NDCG: 0.08573589224072778\n",
      "average Recall: 0.1143968337660956\n",
      "average Precision: 0.04257907542579076\n",
      "average F score: 0.051461703204868356\n",
      "average NDCG: 0.09096514610664176\n",
      "k = 15\n",
      "average Recall: 0.13087351887327558\n",
      "average Precision: 0.039502164502164504\n",
      "average F score: 0.05145295278465612\n",
      "average NDCG: 0.0961141620650629\n",
      "average Recall: 0.14068432721722593\n",
      "average Precision: 0.03584752635847526\n",
      "average F score: 0.048501442771648556\n",
      "average NDCG: 0.10105994930354927\n",
      "k = 40\n",
      "average Recall: 0.1830697015421423\n",
      "average Precision: 0.022625811688311688\n",
      "average F score: 0.03663188756167217\n",
      "average NDCG: 0.11467779583044516\n",
      "average Recall: 0.19827234016153011\n",
      "average Precision: 0.0209448499594485\n",
      "average F score: 0.03409939067975971\n",
      "average NDCG: 0.12007873575198036\n",
      "Epoch:  14\n",
      "k = 5\n",
      "average Recall: 0.06806253701203038\n",
      "average Precision: 0.06217532467532467\n",
      "average F score: 0.05284662162042418\n",
      "average NDCG: 0.06742490118903412\n",
      "average Recall: 0.07968826582656156\n",
      "average Precision: 0.05660989456609895\n",
      "average F score: 0.0537725121789856\n",
      "average NDCG: 0.07286816140064278\n",
      "k = 10\n",
      "average Recall: 0.10394759317977309\n",
      "average Precision: 0.04845779220779221\n",
      "average F score: 0.05498634930848814\n",
      "average NDCG: 0.08507715084103987\n",
      "average Recall: 0.11435444579354692\n",
      "average Precision: 0.04347120843471208\n",
      "average F score: 0.05226130477816256\n",
      "average NDCG: 0.08906186731434976\n",
      "k = 15\n",
      "average Recall: 0.12739131499140346\n",
      "average Precision: 0.04004329004329004\n",
      "average F score: 0.052026876808921624\n",
      "average NDCG: 0.09469871756190192\n",
      "average Recall: 0.13781655953945784\n",
      "average Precision: 0.03595566369288997\n",
      "average F score: 0.0484903138851859\n",
      "average NDCG: 0.09824326143302499\n",
      "k = 40\n",
      "average Recall: 0.1838932652316097\n",
      "average Precision: 0.022666396103896102\n",
      "average F score: 0.03668406485729462\n",
      "average NDCG: 0.1138949477705539\n",
      "average Recall: 0.19764429693872668\n",
      "average Precision: 0.02086374695863747\n",
      "average F score: 0.034000657589708175\n",
      "average NDCG: 0.11782207384481302\n",
      "Epoch:  15\n",
      "k = 5\n",
      "average Recall: 0.06662706557507003\n",
      "average Precision: 0.06266233766233767\n",
      "average F score: 0.053289892240806956\n",
      "average NDCG: 0.06773796928436011\n",
      "average Recall: 0.07293012492067803\n",
      "average Precision: 0.05482562854825628\n",
      "average F score: 0.05112888049003652\n",
      "average NDCG: 0.06900664318850044\n",
      "k = 10\n",
      "average Recall: 0.10229659158075848\n",
      "average Precision: 0.048944805194805194\n",
      "average F score: 0.055271682358390126\n",
      "average NDCG: 0.08538650879447962\n",
      "average Recall: 0.1118655659457108\n",
      "average Precision: 0.04282238442822384\n",
      "average F score: 0.05142086291804708\n",
      "average NDCG: 0.08666154402904269\n",
      "k = 15\n",
      "average Recall: 0.12521687171920828\n",
      "average Precision: 0.04015151515151515\n",
      "average F score: 0.05210281018789448\n",
      "average NDCG: 0.09489029091784255\n",
      "average Recall: 0.13638854454369495\n",
      "average Precision: 0.03606380102730468\n",
      "average F score: 0.04847793365055673\n",
      "average NDCG: 0.09638968279953329\n",
      "k = 40\n",
      "average Recall: 0.18320141911656734\n",
      "average Precision: 0.022544642857142857\n",
      "average F score: 0.03651360272555716\n",
      "average NDCG: 0.11414624804816811\n",
      "average Recall: 0.19902744221337343\n",
      "average Precision: 0.02092457420924574\n",
      "average F score: 0.03413401234788766\n",
      "average NDCG: 0.11652955055019035\n",
      "Epoch:  16\n",
      "k = 5\n",
      "average Recall: 0.0688034101880002\n",
      "average Precision: 0.0650974025974026\n",
      "average F score: 0.055522007644075444\n",
      "average NDCG: 0.06827418291685579\n",
      "average Recall: 0.0700706953415133\n",
      "average Precision: 0.054176804541768056\n",
      "average F score: 0.050009446369904356\n",
      "average NDCG: 0.06673618058014899\n",
      "k = 10\n",
      "average Recall: 0.101416997755185\n",
      "average Precision: 0.04967532467532468\n",
      "average F score: 0.056104799308936366\n",
      "average NDCG: 0.08487512705248947\n",
      "average Recall: 0.1101960969802823\n",
      "average Precision: 0.04274128142741282\n",
      "average F score: 0.05096163224408481\n",
      "average NDCG: 0.0850227588347019\n",
      "k = 15\n",
      "average Recall: 0.12702826041168463\n",
      "average Precision: 0.040530303030303035\n",
      "average F score: 0.052691076159789976\n",
      "average NDCG: 0.09489311086166279\n",
      "average Recall: 0.13617130305951114\n",
      "average Precision: 0.0363882130305488\n",
      "average F score: 0.04888119802162653\n",
      "average NDCG: 0.09541196639705037\n",
      "k = 40\n",
      "average Recall: 0.18501147586312774\n",
      "average Precision: 0.022544642857142857\n",
      "average F score: 0.036539980815591726\n",
      "average NDCG: 0.11384636826102029\n",
      "average Recall: 0.20093745996888993\n",
      "average Precision: 0.021289537712895375\n",
      "average F score: 0.03468955179891495\n",
      "average NDCG: 0.11608223951523076\n",
      "Epoch:  17\n",
      "k = 5\n",
      "average Recall: 0.0695101286271241\n",
      "average Precision: 0.06493506493506493\n",
      "average F score: 0.05552316619867299\n",
      "average NDCG: 0.06774298385708927\n",
      "average Recall: 0.06956081760786285\n",
      "average Precision: 0.05287915652879157\n",
      "average F score: 0.049351984164551506\n",
      "average NDCG: 0.0658974196033494\n",
      "k = 10\n",
      "average Recall: 0.10068712146993074\n",
      "average Precision: 0.04943181818181819\n",
      "average F score: 0.05610646484469791\n",
      "average NDCG: 0.08409817650445928\n",
      "average Recall: 0.11178451463184758\n",
      "average Precision: 0.043227899432279\n",
      "average F score: 0.05152834693112193\n",
      "average NDCG: 0.08501548256995668\n",
      "k = 15\n",
      "average Recall: 0.12721763632581934\n",
      "average Precision: 0.04063852813852814\n",
      "average F score: 0.052848964173327975\n",
      "average NDCG: 0.09437365427810632\n",
      "average Recall: 0.13703101914969493\n",
      "average Precision: 0.03660448769937821\n",
      "average F score: 0.04912784116104737\n",
      "average NDCG: 0.09513390218858435\n",
      "k = 40\n",
      "average Recall: 0.18473367679355632\n",
      "average Precision: 0.02270698051948052\n",
      "average F score: 0.03675468153235705\n",
      "average NDCG: 0.11321529092979815\n",
      "average Recall: 0.2019996869721503\n",
      "average Precision: 0.02145174371451744\n",
      "average F score: 0.03495282981432128\n",
      "average NDCG: 0.11584387373177306\n",
      "Epoch:  18\n",
      "k = 5\n",
      "average Recall: 0.06719576428345074\n",
      "average Precision: 0.0633116883116883\n",
      "average F score: 0.0542856454269877\n",
      "average NDCG: 0.06638633595086818\n",
      "average Recall: 0.0681522692725258\n",
      "average Precision: 0.05190592051905921\n",
      "average F score: 0.04792806918783546\n",
      "average NDCG: 0.06353437011836606\n",
      "k = 10\n",
      "average Recall: 0.10150578469028984\n",
      "average Precision: 0.049512987012987016\n",
      "average F score: 0.056487234931860746\n",
      "average NDCG: 0.08396036690667195\n",
      "average Recall: 0.10916739218504143\n",
      "average Precision: 0.04282238442822385\n",
      "average F score: 0.051023928772143554\n",
      "average NDCG: 0.0822867360876325\n",
      "k = 15\n",
      "average Recall: 0.12540578250503695\n",
      "average Precision: 0.040584415584415584\n",
      "average F score: 0.052700830829625206\n",
      "average NDCG: 0.09344675412265134\n",
      "average Recall: 0.13776836006522392\n",
      "average Precision: 0.036766693701000275\n",
      "average F score: 0.04930131386222009\n",
      "average NDCG: 0.09330342161758758\n",
      "k = 40\n",
      "average Recall: 0.1859695834626668\n",
      "average Precision: 0.022828733766233768\n",
      "average F score: 0.03699536209041064\n",
      "average NDCG: 0.11308585845411594\n",
      "average Recall: 0.202930913495452\n",
      "average Precision: 0.0216139497161395\n",
      "average F score: 0.035210177371234413\n",
      "average NDCG: 0.11413843023523884\n",
      "Epoch:  19\n",
      "k = 5\n",
      "average Recall: 0.0660975013199161\n",
      "average Precision: 0.06217532467532468\n",
      "average F score: 0.05359058274212013\n",
      "average NDCG: 0.06550908054352751\n",
      "average Recall: 0.06777935811756075\n",
      "average Precision: 0.05158150851581508\n",
      "average F score: 0.047457930647935784\n",
      "average NDCG: 0.06350995925676929\n",
      "k = 10\n",
      "average Recall: 0.10191938455092703\n",
      "average Precision: 0.049107142857142856\n",
      "average F score: 0.0562020528221355\n",
      "average NDCG: 0.08369215854884017\n",
      "average Recall: 0.11111145040730054\n",
      "average Precision: 0.04274128142741282\n",
      "average F score: 0.05109776619358167\n",
      "average NDCG: 0.08303150427587427\n",
      "k = 15\n",
      "average Recall: 0.12661024359130404\n",
      "average Precision: 0.040584415584415584\n",
      "average F score: 0.05285825731453457\n",
      "average NDCG: 0.09348813170286356\n",
      "average Recall: 0.1391729579999884\n",
      "average Precision: 0.03671262503379292\n",
      "average F score: 0.04941393592347079\n",
      "average NDCG: 0.0939630100036054\n",
      "k = 40\n",
      "average Recall: 0.18627047888438908\n",
      "average Precision: 0.02301136363636364\n",
      "average F score: 0.03730297289511015\n",
      "average NDCG: 0.11312142715947328\n",
      "average Recall: 0.20195657699989478\n",
      "average Precision: 0.021634225466342256\n",
      "average F score: 0.0351945390845666\n",
      "average NDCG: 0.114315553274105\n",
      "Epoch:  20\n",
      "k = 5\n",
      "average Recall: 0.06566536708646079\n",
      "average Precision: 0.060389610389610396\n",
      "average F score: 0.05255424488259336\n",
      "average NDCG: 0.06522476894487664\n",
      "average Recall: 0.06781542615173639\n",
      "average Precision: 0.05060827250608273\n",
      "average F score: 0.04714988688101457\n",
      "average NDCG: 0.06439046056591803\n",
      "k = 10\n",
      "average Recall: 0.1017040887527674\n",
      "average Precision: 0.048782467532467524\n",
      "average F score: 0.056093277026540725\n",
      "average NDCG: 0.08379623440965263\n",
      "average Recall: 0.11151576808080135\n",
      "average Precision: 0.04282238442822384\n",
      "average F score: 0.051298833408137595\n",
      "average NDCG: 0.08413296872345684\n",
      "k = 15\n",
      "average Recall: 0.12524383831665067\n",
      "average Precision: 0.040259740259740266\n",
      "average F score: 0.052586032024484544\n",
      "average NDCG: 0.09327673716746755\n",
      "average Recall: 0.14001061261213402\n",
      "average Precision: 0.03703703703703704\n",
      "average F score: 0.04984795702227846\n",
      "average NDCG: 0.0953165146621528\n",
      "k = 40\n",
      "average Recall: 0.1843796411129577\n",
      "average Precision: 0.022869318181818185\n",
      "average F score: 0.03706879818411806\n",
      "average NDCG: 0.11282877907486606\n",
      "average Recall: 0.20219601797215808\n",
      "average Precision: 0.021695052716950526\n",
      "average F score: 0.035282183646434706\n",
      "average NDCG: 0.11534747705717252\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "past_chunk = 0\n",
    "future_chunk = 1\n",
    "hidden_size = 4   \n",
    "num_layers = 1    # 1\n",
    "\n",
    "# only one can be set 1\n",
    "use_embedding = 1         \n",
    "use_linear_reduction = 0  \n",
    "###\n",
    "atten_decoder = 1\n",
    "use_dropout = 0\n",
    "use_average_embedding = 1\n",
    "\n",
    "weight = 10\n",
    "labmda = 0\n",
    "topk_labels = 3\n",
    "\n",
    "# It should be the same as the reductioned input in decoder's cat function\n",
    "\n",
    "teacher_forcing_ratio = 0\n",
    "MAX_LENGTH = 1000\n",
    "learning_rate = 0.0001\n",
    "optimizer_option = 2\n",
    "print_val = 3000\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class EncoderRNN_new(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(EncoderRNN_new, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reduction = nn.Linear(input_size, hidden_size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.time_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.time_weight = nn.Linear(input_size, input_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        else:\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        if use_embedding:\n",
    "            list = Variable(torch.LongTensor(input).view(-1, 1))\n",
    "            if use_cuda:\n",
    "                list = list.cuda()\n",
    "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
    "            # sum_embedding = Variable(torch.zeros(hidden_size)).view(1,1,-1)\n",
    "            vectorized_input = Variable(torch.zeros(self.input_size)).view(-1)\n",
    "            if use_cuda:\n",
    "                average_embedding = average_embedding.cuda()\n",
    "                # sum_embedding = sum_embedding.cuda()\n",
    "                vectorized_input = vectorized_input.cuda()\n",
    "\n",
    "            for ele in list:\n",
    "                embedded = self.embedding(ele).view(1, 1, -1)\n",
    "                tmp = average_embedding.clone()\n",
    "                average_embedding = tmp + embedded\n",
    "                # embedded = self.time_embedding(ele).view(1, 1, -1)\n",
    "                # tmp = sum_embedding.clone()\n",
    "                # sum_embedding = tmp + embedded\n",
    "                vectorized_input[ele] = 1\n",
    "\n",
    "            # normalize_length = Variable(torch.LongTensor(len(idx_list)))\n",
    "            # if use_cuda:\n",
    "            #     normalize_length = normalize_length.cuda()\n",
    "            if use_average_embedding:\n",
    "                tmp = [1] * hidden_size\n",
    "                length = Variable(torch.FloatTensor(tmp))\n",
    "                if use_cuda:\n",
    "                    length = length.cuda()\n",
    "                # for idx in range(hidden_size):\n",
    "                real_ave = average_embedding.view(-1) / length\n",
    "                average_embedding = real_ave.view(1, 1, -1)\n",
    "\n",
    "            embedding = average_embedding\n",
    "        else:\n",
    "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
    "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            if use_linear_reduction == 1:\n",
    "                reduced_input = self.reduction(inputs)\n",
    "            else:\n",
    "                reduced_input = inputs\n",
    "\n",
    "            embedding = torch.unsqueeze(reduced_input, 0)\n",
    "\n",
    "        output, hidden = self.gru(embedding, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "class AttnDecoderRNN_new(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout_p=0.2, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN_new, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "            self.attn1 = nn.Linear(self.hidden_size + output_size, self.hidden_size)\n",
    "        else:\n",
    "            self.attn = nn.Linear(self.hidden_size + self.output_size, self.output_size)\n",
    "\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.attn_combine3 = nn.Linear(self.hidden_size * 2 + output_size, self.hidden_size)\n",
    "        else:\n",
    "            self.attn_combine = nn.Linear(self.hidden_size + self.output_size, self.hidden_size)\n",
    "        self.attn_combine5 = nn.Linear(self.output_size, self.output_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.reduction = nn.Linear(self.output_size, self.hidden_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        else:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, history_record, last_hidden):\n",
    "        if use_embedding:\n",
    "            list = Variable(torch.LongTensor(input).view(-1, 1))\n",
    "            if use_cuda:\n",
    "                list = list.cuda()\n",
    "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
    "            if use_cuda:\n",
    "                average_embedding = average_embedding.cuda()\n",
    "\n",
    "            for ele in list:\n",
    "                embedded = self.embedding(ele).view(1, 1, -1)\n",
    "                tmp = average_embedding.clone()\n",
    "                average_embedding = tmp + embedded\n",
    "\n",
    "            if use_average_embedding:\n",
    "                tmp = [1] * hidden_size\n",
    "                length = Variable(torch.FloatTensor(tmp))\n",
    "                if use_cuda:\n",
    "                    length = length.cuda()\n",
    "                # for idx in range(hidden_size):\n",
    "                real_ave = average_embedding.view(-1) / length\n",
    "                average_embedding = real_ave.view(1, 1, -1)\n",
    "\n",
    "            embedding = average_embedding\n",
    "        else:\n",
    "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
    "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            if use_linear_reduction == 1:\n",
    "                reduced_input = self.reduction(inputs)\n",
    "            else:\n",
    "                reduced_input = inputs\n",
    "\n",
    "            embedding = torch.unsqueeze(reduced_input, 0)\n",
    "\n",
    "        if use_dropout:\n",
    "            droped_ave_embedded = self.dropout(embedding)\n",
    "        else:\n",
    "            droped_ave_embedded = embedding\n",
    "\n",
    "        history_context = Variable(torch.FloatTensor(history_record).view(1, -1))\n",
    "        if use_cuda:\n",
    "            history_context = history_context.cuda()\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((droped_ave_embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        element_attn_weights = F.softmax(\n",
    "            self.attn1(torch.cat((history_context, hidden[0]), 1)), dim=1)\n",
    "\n",
    "        # attn_applied = torch.bmm(element_attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # attn_embedd = element_attn_weights * droped_ave_embedded[0]\n",
    "\n",
    "        output = torch.cat((droped_ave_embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # output = torch.cat((droped_ave_embedded[0], attn_applied[0], time_coef.unsqueeze(0)), 1)\n",
    "        # output = self.attn_combine3(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        linear_output = self.out(output[0])\n",
    "        # output_user_item = F.softmax(linear_output)\n",
    "\n",
    "        value = torch.sigmoid(self.attn_combine5(history_context).unsqueeze(0))\n",
    "\n",
    "        one_vec = Variable(torch.ones(self.output_size).view(1, -1))\n",
    "        if use_cuda:\n",
    "            one_vec = one_vec.cuda()\n",
    "\n",
    "        # ones_set = torch.index_select(value[0,0], 1, ones_idx_set[:, 1])\n",
    "        res = history_context.clone()\n",
    "        res[history_context != 0] = 1\n",
    "\n",
    "        output = F.softmax(linear_output * (one_vec - res * value[0]) + history_context * value[0], dim=1)\n",
    "\n",
    "        return output.view(1, -1), hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "class custom_MultiLabelLoss_torch(nn.modules.loss._Loss):\n",
    "    def __init__(self):\n",
    "        super(custom_MultiLabelLoss_torch, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, weights):\n",
    "        mseloss = torch.sum(weights * torch.pow((pred - target), 2))\n",
    "        pred = pred.data\n",
    "        target = target.data\n",
    "        #\n",
    "        ones_idx_set = (target == 1).nonzero()\n",
    "        zeros_idx_set = (target == 0).nonzero()\n",
    "        # zeros_idx_set = (target == -1).nonzero()\n",
    "        \n",
    "        ones_set = torch.index_select(pred, 1, ones_idx_set[:, 1])\n",
    "        zeros_set = torch.index_select(pred, 1, zeros_idx_set[:, 1])\n",
    "        \n",
    "        repeat_ones = ones_set.repeat(1, zeros_set.shape[1])\n",
    "        repeat_zeros_set = torch.transpose(zeros_set.repeat(ones_set.shape[1], 1), 0, 1).clone()\n",
    "        repeat_zeros = repeat_zeros_set.contiguous().view(1, -1)\n",
    "        difference_val = -(repeat_ones - repeat_zeros)\n",
    "        exp_val = torch.exp(difference_val)\n",
    "        exp_loss = torch.sum(exp_val)\n",
    "        normalized_loss = exp_loss / (zeros_set.shape[1] * ones_set.shape[1])\n",
    "        set_loss = Variable(torch.FloatTensor([labmda * normalized_loss]), requires_grad=True)\n",
    "        if use_cuda:\n",
    "            set_loss = set_loss.cuda()\n",
    "        loss = mseloss + set_loss\n",
    "        #loss = mseloss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def generate_dictionary_BA(path, files, attributes_list):\n",
    "    # path = '../Minnemudac/'\n",
    "    # files = ['Coborn_history_order.csv','Coborn_future_order.csv']\n",
    "    # files = ['BA_history_order.csv', 'BA_future_order.csv']\n",
    "    # attributes_list = ['MATERIAL_NUMBER']\n",
    "    dictionary_table = {}\n",
    "    counter_table = {}\n",
    "    for attr in attributes_list:\n",
    "        dictionary = {}\n",
    "        dictionary_table[attr] = dictionary\n",
    "        counter_table[attr] = 0\n",
    "\n",
    "    #csv.field_size_limit(sys.maxsize)\n",
    "    csv.field_size_limit(2**31-1)\n",
    "    for filename in files:\n",
    "        count = 0\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                key = attributes_list[0]\n",
    "                if row[2] not in dictionary_table[key]:\n",
    "                    dictionary_table[key][row[2]] = counter_table[key]\n",
    "                    counter_table[key] = counter_table[key] + 1\n",
    "                    count += 1\n",
    "\n",
    "    print(counter_table)\n",
    "\n",
    "    total = 0\n",
    "    for key in counter_table.keys():\n",
    "        total = total + counter_table[key]\n",
    "\n",
    "    print('# dimensions of final vector: ' + str(total) + ' | ' + str(count - 1))\n",
    "\n",
    "    return dictionary_table, total, counter_table\n",
    "\n",
    "\n",
    "def read_claim2vector_embedding_file_no_vector(path, files):\n",
    "    # attributes_list = ['DRG', 'PROVCAT ', 'RVNU_CD', 'DIAG', 'PROC']\n",
    "    attributes_list = ['MATERIAL_NUMBER']\n",
    "    # path = '../Minnemudac/'\n",
    "    print('start dictionary generation...')\n",
    "    dictionary_table, num_dim, counter_table = generate_dictionary_BA(path, files, attributes_list)\n",
    "    print('finish dictionary generation*****')\n",
    "    usr_attr = 'CUSTOMER_ID'\n",
    "    ord_attr = 'ORDER_NUMBER'\n",
    "\n",
    "    # dictionary_table, num_dim, counter_table = GDF.generate_dictionary(attributes_list)\n",
    "\n",
    "    freq_max = 200\n",
    "    ## all the follow three ways array. First index is patient, second index is the time step, third is the feature vector\n",
    "    data_chunk = []\n",
    "    day_gap_counter = []\n",
    "    claims_counter = 0\n",
    "    num_claim = 0\n",
    "    code_freq_at_first_claim = np.zeros(num_dim + 2)\n",
    "\n",
    "    for file_id in range(len(files)):\n",
    "\n",
    "        count = 0\n",
    "        data_chunk.append({})\n",
    "        filename = files[file_id]\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            # gap_within_one_year = np.zeros(365)\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            last_pid_date = '*'\n",
    "            last_pid = '-1'\n",
    "            last_days = -1\n",
    "            # 2 more elements in the end for start and end states\n",
    "            feature_vector = []\n",
    "            for row in reader:\n",
    "                cur_pid_date = row[usr_attr] + '_' + row[ord_attr]\n",
    "                cur_pid = row[usr_attr]\n",
    "                # cur_days = int(row[ord_attr])\n",
    "\n",
    "                if cur_pid != last_pid:\n",
    "                    # start state\n",
    "                    tmp = [-1]\n",
    "                    data_chunk[file_id][cur_pid] = []\n",
    "                    data_chunk[file_id][cur_pid].append(tmp)\n",
    "                    num_claim = 0\n",
    "\n",
    "                if cur_pid_date not in last_pid_date:\n",
    "                    if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                        sorted_feature_vector = np.sort(feature_vector)\n",
    "                        data_chunk[file_id][last_pid].append(sorted_feature_vector)\n",
    "                        if len(sorted_feature_vector) > 0:\n",
    "                            count = count + 1\n",
    "                        # data_chunk[file_id][last_pid].append(feature_vector)\n",
    "                    feature_vector = []\n",
    "\n",
    "                    claims_counter = 0\n",
    "                if cur_pid != last_pid:\n",
    "                    # end state\n",
    "                    if last_pid not in '-1':\n",
    "                        tmp = [-1]\n",
    "                        data_chunk[file_id][last_pid].append(tmp)\n",
    "\n",
    "                key = attributes_list[0]\n",
    "\n",
    "                within_idx = dictionary_table[key][row[key]]\n",
    "                previous_idx = 0\n",
    "\n",
    "                for j in range(attributes_list.index(key)):\n",
    "                    previous_idx = previous_idx + counter_table[attributes_list[j]]\n",
    "                idx = within_idx + previous_idx\n",
    "\n",
    "                # set corresponding dimention to 1\n",
    "                if idx not in feature_vector:\n",
    "                    feature_vector.append(idx)\n",
    "\n",
    "                last_pid_date = cur_pid_date\n",
    "                last_pid = cur_pid\n",
    "                # last_days = cur_days\n",
    "                if file_id == 1:\n",
    "                    claims_counter = claims_counter + 1\n",
    "\n",
    "            if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                data_chunk[file_id][last_pid].append(np.sort(feature_vector))\n",
    "        print('num of vectors having entries more than 1: ' + str(count))\n",
    "\n",
    "    return data_chunk, num_dim + 2, code_freq_at_first_claim\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, codes_inverse_freq, encoder_optimizer, decoder_optimizer,\n",
    "          criterion, output_size, next_k_step, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(input_variable)\n",
    "    target_length = len(target_variable)\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    history_record = np.zeros(output_size)\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        for ele in input_variable[ei]:\n",
    "            history_record[ele] += 1 / (input_length - 2)\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
    "\n",
    "    last_input = input_variable[input_length - 2]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    last_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    num_str = 0\n",
    "    topk = 1\n",
    "    max_len = 5\n",
    "\n",
    "    if next_k_step > 0:\n",
    "        if next_k_step <= target_length - 2:\n",
    "            max_step = next_k_step\n",
    "        else:\n",
    "            max_step = target_length - 2\n",
    "    else:\n",
    "        max_step = target_length - 1\n",
    "        max_step = min(target_length - 2, max_len)\n",
    "    decoder_input = last_input\n",
    "\n",
    "    for di in range(max_step):\n",
    "\n",
    "        if atten_decoder:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(topk)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        # activation_bound\n",
    "        # topk_labels\n",
    "        # target_neg = zero2neg(target_variable[di])\n",
    "\n",
    "        vectorized_target = np.zeros(output_size)\n",
    "        for idx in target_variable[di + 1]:\n",
    "            vectorized_target[idx] = 1\n",
    "\n",
    "        target = Variable(torch.FloatTensor(vectorized_target).view(1, -1))\n",
    "        if use_cuda:\n",
    "            target = target.cuda()\n",
    "        weights = Variable(torch.FloatTensor(codes_inverse_freq).view(1, -1))\n",
    "        if use_cuda:\n",
    "            weights = weights.cuda()\n",
    "\n",
    "        tt = criterion(decoder_output, target, weights)\n",
    "        # tt = torch.sum(weights*torch.pow((decoder_output - target),2))\n",
    "        loss += tt\n",
    "\n",
    "        decoder_input = target_variable[di + 1]\n",
    "        # loss += multilable_loss(decoder_output, target)\n",
    "\n",
    "    # encoder_optimizer.zero_grad()\n",
    "    # decoder_optimizer.zero_grad()\n",
    "    loss = torch.tensor(0.0, requires_grad=True) if not loss else loss\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "#\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def trainIters(data_chunk, output_size, encoder, decoder, model_id, training_key_set, codes_inverse_freq, next_k_step,\n",
    "               n_iters, print_every=300):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    encoder_pathes = []\n",
    "    decoder_pathes = []\n",
    "    # elem_wise_connection.initWeight()\n",
    "\n",
    "    # sum_history = add_history(data_chunk[past_chunk],training_key_set,output_size)\n",
    "    # KNN_history = KNN_history_record1(sum_history, output_size, num_nearest_neighbors)\n",
    "    KNN_history = []\n",
    "\n",
    "    if optimizer_option == 1:\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    elif optimizer_option == 2:\n",
    "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-09, weight_decay=0)\n",
    "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.88, 0.95), eps=1e-08, weight_decay=0)\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
    "                                             weight_decay=0)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
    "                                             weight_decay=0)\n",
    "    elif optimizer_option == 3:\n",
    "        encoder_optimizer = torch.optim.RMSprop(encoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
    "                                                weight_decay=0, momentum=0, centered=False)\n",
    "        decoder_optimizer = torch.optim.RMSprop(decoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
    "                                                weight_decay=0, momentum=0, centered=False)\n",
    "    elif optimizer_option == 4:\n",
    "        encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
    "                                                 weight_decay=0)\n",
    "        decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
    "                                                 weight_decay=0)\n",
    "\n",
    "    # training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "    #                  for i in range(n_iters)]\n",
    "    # criterion = nn.NLLLoss()\n",
    "    total_iter = 0\n",
    "    criterion = custom_MultiLabelLoss_torch()\n",
    "    for j in range(n_iters):\n",
    "        key_idx = np.random.permutation(len(training_key_set))\n",
    "        # key_idx = np.random.choice(len(training_key_set),n_iters)\n",
    "        training_keys = []\n",
    "\n",
    "        for idx in key_idx:\n",
    "            training_keys.append(training_key_set[idx])\n",
    "\n",
    "            # criterion = custom_MultiLabelLoss_MSE()\n",
    "        # criterion = nn.MultiLabelSoftMarginLoss(size_average=False)\n",
    "        # criterion = nn.BCELoss()\n",
    "        weight_vector = []\n",
    "\n",
    "        for iter in range(1, len(training_key_set) + 1):\n",
    "            # training_pair = training_pairs[iter - 1]\n",
    "            # input_variable = training_pair[0]\n",
    "            # target_variable = training_pair[1]\n",
    "            input_variable = data_chunk[past_chunk][training_keys[iter - 1]]\n",
    "            target_variable = data_chunk[future_chunk][training_keys[iter - 1]]\n",
    "\n",
    "            loss = train(input_variable, target_variable, encoder,\n",
    "                         decoder, codes_inverse_freq, encoder_optimizer, decoder_optimizer, criterion, output_size,\n",
    "                         next_k_step)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            total_iter += 1\n",
    "\n",
    "        print_loss_avg = print_loss_total / len(training_key_set)\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.6f' % (timeSince(start, total_iter / (n_iters * len(training_key_set))), total_iter, total_iter / (n_iters * len(training_key_set)) * 100,print_loss_avg))\n",
    "\n",
    "        filepath = './models/encoder' + (model_id) + '_model_epoch' + str(int(j))\n",
    "        encoder_pathes.append(filepath)\n",
    "        torch.save(encoder, filepath)\n",
    "        filepath = './models/decoder' + (model_id) + '_model_epoch' + str(int(j))\n",
    "        decoder_pathes.append(filepath)\n",
    "        torch.save(decoder, filepath)\n",
    "        print('Finish epoch: ' + str(j))\n",
    "        print('Model is saved.')\n",
    "        sys.stdout.flush()\n",
    "    # showPlot(plot_losses)\n",
    "    # print('The loss: ' + str(print_loss_total))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Plotting results\n",
    "# ----------------\n",
    "#\n",
    "# Plotting is done with matplotlib, using the array of loss values\n",
    "# ``plot_losses`` saved while training.\n",
    "#\n",
    "\n",
    "cosine_sim = []\n",
    "pair_cosine_sim = []\n",
    "\n",
    "\n",
    "def decoding_next_k_step(encoder, decoder, input_variable, target_variable, output_size, k, activate_codes_num):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = len(input_variable)\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    history_record = np.zeros(output_size)\n",
    "    count = 0\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        for ele in input_variable[ei]:\n",
    "            history_record[ele] += 1\n",
    "        count += 1\n",
    "\n",
    "    history_record = history_record / count\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
    "\n",
    "        for ii in range(k):\n",
    "            vectorized_target = np.zeros(output_size)\n",
    "            for idx in target_variable[ii + 1]:\n",
    "                vectorized_target[idx] = 1\n",
    "\n",
    "            vectorized_input = np.zeros(output_size)\n",
    "            for idx in input_variable[ei]:\n",
    "                vectorized_input[idx] = 1\n",
    "\n",
    "    decoder_input = input_variable[input_length - 2]\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    last_hidden = decoder_hidden\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    num_str = 0\n",
    "    topk = 400\n",
    "    decoded_vectors = []\n",
    "    prob_vectors = []\n",
    "    cout = 0\n",
    "    for di in range(k):\n",
    "        if atten_decoder:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(topk)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        vectorized_target = np.zeros(output_size)\n",
    "        for idx in target_variable[di + 1]:\n",
    "            vectorized_target[idx] = 1\n",
    "\n",
    "        # target_topi = vectorized_target.argsort()[::-1][:topk]\n",
    "        # activation_bound\n",
    "\n",
    "        count = 0\n",
    "        start_idx = -1\n",
    "        end_idx = output_size\n",
    "        if activate_codes_num > 0:\n",
    "            pick_num = activate_codes_num\n",
    "        else:\n",
    "            pick_num = np.sum(vectorized_target)\n",
    "            # print(pick_num)\n",
    "\n",
    "        tmp = []\n",
    "        for ele in range(len(topi[0])):\n",
    "            if count >= pick_num:\n",
    "                break\n",
    "            tmp.append(topi[0][ele])\n",
    "            count += 1\n",
    "\n",
    "        decoded_vectors.append(tmp)\n",
    "        decoder_input = tmp\n",
    "        tmp = []\n",
    "\n",
    "        for i in range(topk):\n",
    "            tmp.append(topi[0][i])\n",
    "        prob_vectors.append(tmp)\n",
    "\n",
    "    return decoded_vectors, prob_vectors\n",
    "\n",
    "import bottleneck as bn\n",
    "\n",
    "\n",
    "def top_n_indexes(arr, n):\n",
    "    idx = bn.argpartition(arr, arr.size - n, axis=None)[-n:]\n",
    "    width = arr.shape[1]\n",
    "    return [divmod(i, width) for i in idx]\n",
    "\n",
    "\n",
    "def get_precision_recall_Fscore(groundtruth, pred):\n",
    "    a = groundtruth\n",
    "    b = pred\n",
    "    correct = 0\n",
    "    truth = 0\n",
    "    positive = 0\n",
    "\n",
    "    for idx in range(len(a)):\n",
    "        if a[idx] == 1:\n",
    "            truth += 1\n",
    "            if b[idx] == 1:\n",
    "                correct += 1\n",
    "        if b[idx] == 1:\n",
    "            positive += 1\n",
    "\n",
    "    flag = 0\n",
    "    if 0 == positive:\n",
    "        precision = 0\n",
    "        flag = 1\n",
    "        # print('postivie is 0')\n",
    "    else:\n",
    "        precision = correct / positive\n",
    "    if 0 == truth:\n",
    "        recall = 0\n",
    "        flag = 1\n",
    "        # print('recall is 0')\n",
    "    else:\n",
    "        recall = correct / truth\n",
    "\n",
    "    if flag == 0 and precision + recall > 0:\n",
    "        F = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        F = 0\n",
    "    return precision, recall, F, correct\n",
    "\n",
    "\n",
    "def get_F_score(prediction, test_Y):\n",
    "    jaccard_similarity = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "\n",
    "    count = 0\n",
    "    for idx in range(len(test_Y)):\n",
    "        pred = prediction[idx]\n",
    "        T = 0\n",
    "        P = 0\n",
    "        correct = 0\n",
    "        for id in range(len(pred)):\n",
    "            if test_Y[idx][id] == 1:\n",
    "                T = T + 1\n",
    "                if pred[id] == 1:\n",
    "                    correct = correct + 1\n",
    "            if pred[id] == 1:\n",
    "                P = P + 1\n",
    "\n",
    "        if P == 0 or T == 0:\n",
    "            continue\n",
    "        precision = correct / P\n",
    "        recall = correct / T\n",
    "        prec.append(precision)\n",
    "        rec.append(recall)\n",
    "        if correct == 0:\n",
    "            jaccard_similarity.append(0)\n",
    "        else:\n",
    "            jaccard_similarity.append(2 * precision * recall / (precision + recall))\n",
    "        count = count + 1\n",
    "\n",
    "    print(\n",
    "        'average precision: ' + str(np.mean(prec)))\n",
    "    print('average recall : ' + str(\n",
    "        np.mean(rec)))\n",
    "    print('average F score: ' + str(\n",
    "        np.mean(jaccard_similarity)))\n",
    "\n",
    "\n",
    "def get_DCG(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1) / math.log2(count + 1 + 1)\n",
    "        count += 1\n",
    "\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def get_NDCG(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1) / math.log2(count + 1 + 1)\n",
    "        count += 1\n",
    "    idcg = 0\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "#     num_item = int(min(num_real_item, k))\n",
    "    for i in range(num_item):\n",
    "        idcg += (1) / math.log2(i + 1 + 1)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def get_HT(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            return 1\n",
    "        count += 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_MAE(groundtruth, k):\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "    mae = abs(num_item - k)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def evaluate(data_chunk, encoder, decoder, output_size, test_key_set, next_k_step, activate_codes_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    F = []\n",
    "    prec1 = []\n",
    "    rec1 = []\n",
    "    F1 = []\n",
    "    prec2 = []\n",
    "    rec2 = []\n",
    "    F2 = []\n",
    "    prec3 = []\n",
    "    rec3 = []\n",
    "    F3 = []\n",
    "    length = np.zeros(3)\n",
    "\n",
    "    NDCG = []\n",
    "    MAE = []\n",
    "    n_hit = 0\n",
    "    count = 0\n",
    "\n",
    "    for iter in range(len(test_key_set)):\n",
    "        input_variable = data_chunk[past_chunk][test_key_set[iter]]\n",
    "        target_variable = data_chunk[future_chunk][test_key_set[iter]]\n",
    "\n",
    "        if len(target_variable) < 2 + next_k_step:\n",
    "            continue\n",
    "        count += 1\n",
    "        output_vectors, prob_vectors = decoding_next_k_step(encoder, decoder, input_variable, target_variable,\n",
    "                                                            output_size, next_k_step, activate_codes_num)\n",
    "\n",
    "        hit = 0\n",
    "        for idx in range(len(output_vectors)):\n",
    "            # for idx in [2]:\n",
    "            vectorized_target = np.zeros(output_size)\n",
    "            for ii in target_variable[1 + idx]:\n",
    "                vectorized_target[ii] = 1\n",
    "\n",
    "            vectorized_output = np.zeros(output_size)\n",
    "            for ii in output_vectors[idx]:\n",
    "                vectorized_output[ii] = 1\n",
    "\n",
    "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
    "            prec.append(precision)\n",
    "            rec.append(recall)\n",
    "            F.append(Fscore)\n",
    "            length[idx] += np.sum(target_variable[1 + idx])\n",
    "            target_topi = prob_vectors[idx]\n",
    "            ndcg = get_NDCG(vectorized_target, target_topi, activate_codes_num)\n",
    "            NDCG.append(ndcg)\n",
    "            mae = get_MAE(vectorized_target, activate_codes_num)\n",
    "            MAE.append(mae)\n",
    "            \n",
    "    print('average Recall: ' + str(np.mean(rec)))\n",
    "    print('average Precision: ' + str(np.mean(prec)))\n",
    "    print('average F score: ' + str(np.mean(F)))\n",
    "    print('average NDCG: ' + str(np.mean(NDCG)))\n",
    "    # print('average hit rate: ' + str(n_hit / len(test_key_set)))\n",
    "    return np.mean(rec), np.mean(prec), np.mean(F), np.mean(NDCG), np.mean(MAE)\n",
    "\n",
    "def split_data(full_list, ratio, shuffle=False):\n",
    "    n_total = len(full_list)\n",
    "    offset = int(n_total * ratio)\n",
    "    if n_total==0 or offset<1:\n",
    "        return [],full_list\n",
    "    if shuffle:\n",
    "        random.shuffle(full_list)\n",
    "    train = full_list[:offset]\n",
    "    test = full_list[offset:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def partition_the_data(data_chunk, key_set, next_k_step):\n",
    "    filtered_key_set = []\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[past_chunk][key]) <= 3:\n",
    "            continue\n",
    "        if len(data_chunk[future_chunk][key]) < 2 + next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set))]\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "    return training_key_set, test_key_set\n",
    "\n",
    "def partition_the_data_validate(data_chunk, key_set, next_k_step):\n",
    "    training_key_set, test_set = split_data(key_set, 0.8)\n",
    "    validation_key_set, test_key_set = split_data(test_set, 0.5)\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    print('Number of valid instances: ' + str(len(validation_key_set)))\n",
    "    print('Number of test instances: ' + str(len(test_key_set)))\n",
    "    return training_key_set, validation_key_set, test_key_set\n",
    "\n",
    "def get_codes_frequency_no_vector(X, num_dim, key_set):\n",
    "    result_vector = np.zeros(num_dim)\n",
    "    for pid in key_set:\n",
    "        for idx in X[pid]:\n",
    "            result_vector[idx] += 1\n",
    "    return result_vector\n",
    "\n",
    "\n",
    "# The first two parameters are the past records and future records, respectively.\n",
    "# The main function consists of two model which is decisded by the argv[5]. If training is 1, it is training mode. If\n",
    "# training is 0, it is test mode. model_version is the name of the model. next_k_step is the number of steps we predict.\n",
    "# model_epoch is the model generated by the model_epoch-th epoch.\n",
    "def main(argv):\n",
    "    # ['Sets2Sets.py', './data/TaFeng_history.csv', './data/TaFeng_future.csv', 'TaFeng', 1, 0]\n",
    "    # ['Sets2Sets.py', './data/Dunnhumby_history.csv', './data/Dunnhumby_future.csv', 'Dunnhumby', 1, 0] \n",
    "    argv = ['Sets2Sets.py', f'./data/{DATASET_NAME}_history.csv', f'./data/{DATASET_NAME}_future.csv', f'{DATASET_NAME}', 1, 0] \n",
    "\n",
    "    files = [argv[1],argv[2]]\n",
    "\n",
    "    model_version = argv[3]\n",
    "\n",
    "    next_k_step = int(argv[4])\n",
    "    training = int(argv[5])\n",
    "    path = './'\n",
    "    directory = './models/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    data_chunk, input_size, code_freq_at_first_claim = read_claim2vector_embedding_file_no_vector(path, files)\n",
    "    print(\"data_chunk;)\", type(data_chunk))\n",
    "    codes_freq = get_codes_frequency_no_vector(data_chunk[past_chunk], input_size, data_chunk[future_chunk].keys())\n",
    "    training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[future_chunk]), next_k_step)\n",
    "\n",
    "    weights = np.zeros(input_size)\n",
    "    max_freq = max(codes_freq)\n",
    "    for idx in range(len(codes_freq)):\n",
    "        if codes_freq[idx] > 0:\n",
    "            weights[idx] = max_freq / codes_freq[idx]\n",
    "        else:\n",
    "            weights[idx] = 0\n",
    "\n",
    "    encoder1 = EncoderRNN_new(input_size, hidden_size, num_layers)\n",
    "    attn_decoder1 = AttnDecoderRNN_new(hidden_size, input_size, num_layers, dropout_p=0.1)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder1 = encoder1.cuda()\n",
    "        attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "    if training == 1:\n",
    "        if atten_decoder:\n",
    "            trainIters(data_chunk, input_size, encoder1, attn_decoder1, model_version, training_key_set, weights,\n",
    "                       next_k_step, num_iter, print_every=print_val)\n",
    "\n",
    "    else:\n",
    "        result = \"./result/\"\n",
    "        if not os.path.exists(result):\n",
    "            os.mkdir(result)\n",
    "        header = [\"EPOCH\", \"Recall@5\", \"Recall@10\", \"Recall@15\", \"Recall@40\", \"Precision@5\", \"Precision@10\", \"Precision@15\", \"Precision@40\",\\\n",
    "                 \"F1-score@5\", \"F1-score@10\", \"F1-score@15\", \"F1-score@40\", \"NDCG@5\", \"NDCG@10\", \"NDCG@15\", \"NDCG@40\", \"MAE@5\", \"MAE@10\", \"MAE@15\", \"MAE@40\"]\n",
    "        with open(result+f'{model_version}.csv', 'w', newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "            for model_epoch in range(num_iter):\n",
    "                print('Epoch: ', model_epoch+1)\n",
    "                encoder_pathes = './models/encoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
    "                decoder_pathes = './models/decoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
    "                encoder_instance = torch.load(encoder_pathes)\n",
    "                decoder_instance = torch.load(decoder_pathes)\n",
    "                valid_recall = []\n",
    "                valid_precision = []\n",
    "                valid_f1 = []\n",
    "                valid_ndcg = []\n",
    "                valid_mae = []\n",
    "                recall_list = []\n",
    "                precision_list = []\n",
    "                f1_list = []\n",
    "                ndcg_list = []\n",
    "                mae_list = []\n",
    "\n",
    "                for i in [5, 10, 15, 40]:\n",
    "                    print('k = ' + str(i))\n",
    "\n",
    "                    # validation\n",
    "                    recall, precision, f1, ndcg, mae = evaluate(data_chunk, encoder_instance, decoder_instance, input_size, validation_key_set, next_k_step, i)\n",
    "                    valid_recall.append(recall)\n",
    "                    valid_precision.append(precision)\n",
    "                    valid_f1.append(f1)\n",
    "                    valid_ndcg.append(ndcg)\n",
    "                    valid_mae.append(mae)\n",
    "\n",
    "                    # test\n",
    "                    recall, precision, f1, ndcg, mae  = evaluate(data_chunk, encoder_instance, decoder_instance, input_size, test_key_set, next_k_step, i)\n",
    "                    recall_list.append(recall)\n",
    "                    precision_list.append(precision)\n",
    "                    f1_list.append(f1)\n",
    "                    ndcg_list.append(ndcg)\n",
    "                    mae_list.append(mae)\n",
    "                \n",
    "                row = []\n",
    "                row.append(model_epoch)\n",
    "                for recall in recall_list:\n",
    "                    row.append(recall)\n",
    "                for precision in precision_list:\n",
    "                    row.append(precision)\n",
    "                for f1 in f1_list:\n",
    "                    row.append(f1)\n",
    "                for ndcg in ndcg_list:\n",
    "                    row.append(ndcg)\n",
    "                for mae in mae_list:\n",
    "                    row.append(mae)\n",
    "                writer.writerow(row)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
