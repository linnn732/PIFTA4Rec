{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e298dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run merge_order_and_sort_by_date before this file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import gc\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a762c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"TaFeng\"\n",
    "# DATASET_NAME = \"Dunnhumby\"\n",
    "# DATASET_NAME = \"Instacart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ae017e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>TRANSACTION_DT</th>\n",
       "      <th>CART_ID</th>\n",
       "      <th>NEW_ITEM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1113</td>\n",
       "      <td>4902105011621</td>\n",
       "      <td>2000-11-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1113</td>\n",
       "      <td>7616100830794</td>\n",
       "      <td>2000-11-26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1113</td>\n",
       "      <td>4710892632017</td>\n",
       "      <td>2000-11-26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1113</td>\n",
       "      <td>4710905340113</td>\n",
       "      <td>2000-11-27</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1113</td>\n",
       "      <td>4717362901277</td>\n",
       "      <td>2000-11-27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533054</th>\n",
       "      <td>20002000</td>\n",
       "      <td>4710339772139</td>\n",
       "      <td>2001-01-20</td>\n",
       "      <td>62360</td>\n",
       "      <td>4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533055</th>\n",
       "      <td>20002000</td>\n",
       "      <td>20513184</td>\n",
       "      <td>2001-01-20</td>\n",
       "      <td>62360</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533056</th>\n",
       "      <td>20002000</td>\n",
       "      <td>4714800731229</td>\n",
       "      <td>2001-01-20</td>\n",
       "      <td>62360</td>\n",
       "      <td>2946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533057</th>\n",
       "      <td>20002000</td>\n",
       "      <td>4714541091071</td>\n",
       "      <td>2001-01-20</td>\n",
       "      <td>62360</td>\n",
       "      <td>7382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533058</th>\n",
       "      <td>20002000</td>\n",
       "      <td>4710018008634</td>\n",
       "      <td>2001-01-20</td>\n",
       "      <td>62360</td>\n",
       "      <td>2629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>533059 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUSTOMER_ID     PRODUCT_ID TRANSACTION_DT  CART_ID  NEW_ITEM_ID\n",
       "0              1113  4902105011621     2000-11-26        0            0\n",
       "1              1113  7616100830794     2000-11-26        0            1\n",
       "2              1113  4710892632017     2000-11-26        0            2\n",
       "3              1113  4710905340113     2000-11-27        1            3\n",
       "4              1113  4717362901277     2000-11-27        1            4\n",
       "...             ...            ...            ...      ...          ...\n",
       "533054     20002000  4710339772139     2001-01-20    62360         4546\n",
       "533055     20002000       20513184     2001-01-20    62360         1351\n",
       "533056     20002000  4714800731229     2001-01-20    62360         2946\n",
       "533057     20002000  4714541091071     2001-01-20    62360         7382\n",
       "533058     20002000  4710018008634     2001-01-20    62360         2629\n",
       "\n",
       "[533059 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\"../../cleaned_dataset\", DATASET_NAME+\"_clean.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db4531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1113,\n",
       "  [[0, 1, 2], [3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15]],\n",
       "  [3, 6, 7]),\n",
       " (5241,\n",
       "  [[16, 17, 18, 19, 20, 21],\n",
       "   [22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
       "   [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
       "   [48, 49, 50, 51, 52]],\n",
       "  [6, 10, 16, 5])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(f\"../../preprocessing-data/{DATASET_NAME}_user_cart_itemid_list.gz\", \"rb\") as fp:\n",
    "    user_cart_itemid_list = pickle.load(fp)\n",
    "user_cart_itemid_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a621e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8523\n",
      "2131\n"
     ]
    }
   ],
   "source": [
    "# 切分資料集\n",
    "train_set_size = int(len(user_cart_itemid_list) * 0.8)\n",
    "test_set_size = len(user_cart_itemid_list)-train_set_size\n",
    "train_set, test_set = random_split(user_cart_itemid_list, [train_set_size, test_set_size])\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_file = f\"data/{DATASET_NAME}_history.csv\"\n",
    "future_file = f\"data/{DATASET_NAME}_future.csv\"\n",
    "headers = ['CUSTOMER_ID','ORDER_NUMBER','MATERIAL_NUMBER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84643708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "\n",
    "    # path = '../Minnemudac/dunnhumby_50k/'\n",
    "    path = argv[1]\n",
    "    print('Preprocessing...')\n",
    "    files = os.listdir(path)\n",
    "    date_attr = 1\n",
    "    mat_attr = 6\n",
    "    user_attr = 11\n",
    "\n",
    "    pid_hash = {}\n",
    "    usr_oid_map = []\n",
    "    file_count = 0\n",
    "    usr_oid_record = {}\n",
    "    for fid in range(len(files)):\n",
    "        count = 0\n",
    "        with open(path + files[fid], 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                uid = row[user_attr]\n",
    "                mid = row[mat_attr]\n",
    "                str_date = row[date_attr]\n",
    "                date = int(str_date)\n",
    "                if mid not in pid_hash:\n",
    "                    pid_hash[mid] = 1\n",
    "                if uid not in usr_oid_record:\n",
    "                    usr_oid_record[uid] = {}\n",
    "                if date not in usr_oid_record[uid]:\n",
    "                    usr_oid_record[uid][date] = []\n",
    "                usr_oid_record[uid][date].append(mid)\n",
    "                count += 1\n",
    "        file_count += 1\n",
    "\n",
    "    average_records = 0\n",
    "    num_more_than_two_records = 0\n",
    "    num_more_than_three_records = 0\n",
    "    num = 0\n",
    "    for uid in usr_oid_record.keys():\n",
    "        num_records = len(usr_oid_record[uid].keys())\n",
    "\n",
    "        if num_records >= 2:\n",
    "            average_records += num_records\n",
    "            num_more_than_two_records += 1\n",
    "        if num_records >= 3:\n",
    "            num_more_than_three_records += 1\n",
    "\n",
    "    average_records = average_records / num_more_than_two_records\n",
    "\n",
    "\n",
    "\n",
    "    print('In the ' + str() + ' month:')\n",
    "    print('Total :' + str(len(usr_oid_record.keys())) + ' users')\n",
    "    print('Average records: ' + str(average_records))\n",
    "    print('More than one record: ' + str(num_more_than_two_records))\n",
    "    print('More than two records: ' + str(num_more_than_three_records))\n",
    "    usr_oid_map.append(usr_oid_record)\n",
    "\n",
    "    num_users = 50000\n",
    "    count = 0\n",
    "    print('Total '+str(len(pid_hash.keys()))+' items')\n",
    "\n",
    "    headers = ['CUSTOMER_ID','ORDER_NUMBER','MATERIAL_NUMBER']\n",
    "    path = './'\n",
    "\n",
    "    # history_file = 'Dunnhumby_history_order_original.csv'\n",
    "    # history_file = 'Dunnhumby_history_order_original_10_steps.csv'\n",
    "    history_file = 'Dunnhumby_history_order_original_10_steps_50kuser.csv'\n",
    "    with open(path + history_file, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "        for uid in usr_oid_record.keys():\n",
    "            if count > num_users:\n",
    "                break\n",
    "            if len(usr_oid_record[uid]) > 1:\n",
    "                dates = usr_oid_record[uid].keys()\n",
    "                sort_date = np.sort(list(dates))\n",
    "                if len(sort_date) >= 8:\n",
    "                    # for i in range(0,5):\n",
    "                    for i in range(0, 5):\n",
    "                        date = sort_date[i]\n",
    "                        for item in usr_oid_record[uid][date]:\n",
    "                            row = []\n",
    "                            row.append(uid)\n",
    "                            row.append(date)\n",
    "                            row.append(item)\n",
    "                            writer.writerow(row)\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    # future_file = 'Dunnhumby_future_order_original.csv'\n",
    "    # future_file = 'Dunnhumby_future_order_original_10_steps.csv'\n",
    "    future_file = 'Dunnhumby_future_order_original_10_steps_50kuser.csv'\n",
    "    with open(path + future_file, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "        for uid in usr_oid_record.keys():\n",
    "            if count > num_users:\n",
    "                break\n",
    "            if len(usr_oid_record[uid]) > 1:\n",
    "                dates = usr_oid_record[uid].keys()\n",
    "                sort_date = np.sort(list(dates))\n",
    "                if len(sort_date) >= 8:\n",
    "                    for i in range(5,8):\n",
    "                        date = sort_date[i]\n",
    "                        for item in usr_oid_record[uid][date]:\n",
    "                            row = []\n",
    "                            row.append(uid)\n",
    "                            row.append(date)\n",
    "                            row.append(item)\n",
    "                            writer.writerow(row)\n",
    "            count += 1\n",
    "\n",
    "    print('Partition the data...')\n",
    "    attributes_list = ['MATERIAL_NUMBER']\n",
    "    # files = ['BA_history_order_original.csv', 'BA_future_order_original.csv']\n",
    "    #files = ['BA_history_order_original_100k.csv','BA_future_order_original_100k.csv']\n",
    "    # files = ['BA_history_order_8kitem_200kuer.csv', 'BA_future_order_8kitem_200kuer.csv']\n",
    "    # files = ['Dunnhumby_history_order_original.csv', 'Dunnhumby_future_order_original.csv']\n",
    "    # files = ['Dunnhumby_history_order_original_10_steps.csv', 'Dunnhumby_future_order_original_10_steps.csv']\n",
    "    files = ['Dunnhumby_history_order_original_10_steps_50kuser.csv', 'Dunnhumby_future_order_original_10_steps_50kuser.csv']\n",
    "\n",
    "    # print('start dictionary generation...')\n",
    "    # dictionary_table, num_dim, counter_table = GDF.generate_dictionary_BA(files,attributes_list)\n",
    "    # print('finish dictionary generation*****')\n",
    "\n",
    "\n",
    "\n",
    "    total_num = 0\n",
    "    item_map = {}\n",
    "    #data_chunk, input_size, code_freq_at_first_claim = BasketAnalysis_claim2vector.read_claim2vector_embedding_file(files)\n",
    "    for file in files:\n",
    "        with open(path + file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                total_num += 1\n",
    "                if row[attributes_list[0]] not in item_map:\n",
    "                    item_map[row[attributes_list[0]]] = 1\n",
    "                else:\n",
    "                    item_map[row[attributes_list[0]]] += 1\n",
    "\n",
    "    import operator\n",
    "    sorted_x = sorted(item_map.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    topk = 6000\n",
    "\n",
    "    topk_num = 0\n",
    "    count = 0\n",
    "    topk_dictionary = {}\n",
    "    for idx in range(len(sorted_x)):\n",
    "        if idx >= topk:\n",
    "            break\n",
    "        topk_dictionary[sorted_x[-1-idx][0]] = 1\n",
    "        topk_num += sorted_x[-1-idx][1]\n",
    "    print('Percentage of the top '+str(topk)+' items: ' + str(topk_num/total_num))\n",
    "\n",
    "    history = []\n",
    "    future = []\n",
    "    history_keys = {}\n",
    "    future_keys = {}\n",
    "\n",
    "    cus_attr = 'CUSTOMER_ID'\n",
    "    with open(path + files[0], 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row[attributes_list[0]] in topk_dictionary:\n",
    "                instance = []\n",
    "                for key in row.keys():\n",
    "                    instance.append(row[key])\n",
    "                history.append(instance)\n",
    "                history_keys[row[cus_attr]] = 1\n",
    "\n",
    "    with open(path + files[1], 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row[attributes_list[0]] in topk_dictionary:\n",
    "                instance = []\n",
    "                for key in row.keys():\n",
    "                    instance.append(row[key])\n",
    "                future.append(instance)\n",
    "                future_keys[row[cus_attr]] = 1\n",
    "\n",
    "    # files = ['Dunnhumby_history_order.csv', 'Dunnhumby_future_order.csv']\n",
    "    # files = ['Dunnhumby_history_order_10_steps.csv', 'Dunnhumby_future_order_10_steps.csv']\n",
    "    # files = ['Dunnhumby_history_order_10_steps_50kuser.csv', 'Dunnhumby_future_order_10_steps_50kuser.csv']\n",
    "    files = [argv[2],argv[3]]\n",
    "    headers = ['CUSTOMER_ID','ORDER_NUMBER','MATERIAL_NUMBER']\n",
    "    with open(path + files[0], 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "        for row in history:\n",
    "            if row[0] in  history_keys and  row[0] in  future_keys:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "    with open(path + files[1], 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "        for row in future:\n",
    "            if row[0] in  history_keys and  row[0] in  future_keys:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print('DONE!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
