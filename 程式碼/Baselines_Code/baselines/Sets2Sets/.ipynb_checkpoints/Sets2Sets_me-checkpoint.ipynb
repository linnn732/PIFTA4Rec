{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835eb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = \"TaFeng\"\n",
    "DATASET_NAME = \"Dunnhumby\"\n",
    "# DATASET_NAME = \"Instacart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648e4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dictionary generation...\n",
      "{'MATERIAL_NUMBER': 250580}\n",
      "# dimensions of final vector: 250580 | 12826\n",
      "finish dictionary generation*****\n",
      "num of vectors having entries more than 1: 237753\n",
      "num of vectors having entries more than 1: 12825\n",
      "data_chunk;) <class 'list'>\n",
      "Number of training instances: 10260\n",
      "Number of valid instances: 1283\n",
      "Number of test instances: 1283\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 251165354896 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1032\u001b[0m\n\u001b[0;32m   1029\u001b[0m                 writer\u001b[38;5;241m.\u001b[39mwriterow(row)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1032\u001b[0m     main(sys\u001b[38;5;241m.\u001b[39margv)\n",
      "Cell \u001b[1;32mIn[2], line 960\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m         weights[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m encoder1 \u001b[38;5;241m=\u001b[39m EncoderRNN_new(input_size, hidden_size, num_layers)\n\u001b[0;32m    961\u001b[0m attn_decoder1 \u001b[38;5;241m=\u001b[39m AttnDecoderRNN_new(hidden_size, input_size, num_layers, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cuda:\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mEncoderRNN_new.__init__\u001b[1;34m(self, input_size, hidden_size, num_layers)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(input_size, hidden_size)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(input_size, hidden_size)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_weight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_size, input_size)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_embedding \u001b[38;5;129;01mor\u001b[39;00m use_linear_reduction:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGRU(hidden_size, hidden_size, num_layers)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\baselines\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 251165354896 bytes."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_iter = 20\n",
    "\n",
    "past_chunk = 0\n",
    "future_chunk = 1\n",
    "hidden_size = 4   \n",
    "num_layers = 1    # 1\n",
    "\n",
    "# only one can be set 1\n",
    "use_embedding = 1         \n",
    "use_linear_reduction = 0  \n",
    "###\n",
    "atten_decoder = 1\n",
    "use_dropout = 0\n",
    "use_average_embedding = 1\n",
    "\n",
    "weight = 10\n",
    "labmda = 0\n",
    "topk_labels = 3\n",
    "\n",
    "# It should be the same as the reductioned input in decoder's cat function\n",
    "\n",
    "teacher_forcing_ratio = 0\n",
    "MAX_LENGTH = 1000\n",
    "learning_rate = 0.0001\n",
    "optimizer_option = 2\n",
    "print_val = 3000\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class EncoderRNN_new(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(EncoderRNN_new, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reduction = nn.Linear(input_size, hidden_size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.time_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.time_weight = nn.Linear(input_size, input_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        else:\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        if use_embedding:\n",
    "            list = Variable(torch.LongTensor(input).view(-1, 1))\n",
    "            if use_cuda:\n",
    "                list = list.cuda()\n",
    "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
    "            # sum_embedding = Variable(torch.zeros(hidden_size)).view(1,1,-1)\n",
    "            vectorized_input = Variable(torch.zeros(self.input_size)).view(-1)\n",
    "            if use_cuda:\n",
    "                average_embedding = average_embedding.cuda()\n",
    "                # sum_embedding = sum_embedding.cuda()\n",
    "                vectorized_input = vectorized_input.cuda()\n",
    "\n",
    "            for ele in list:\n",
    "                embedded = self.embedding(ele).view(1, 1, -1)\n",
    "                tmp = average_embedding.clone()\n",
    "                average_embedding = tmp + embedded\n",
    "                # embedded = self.time_embedding(ele).view(1, 1, -1)\n",
    "                # tmp = sum_embedding.clone()\n",
    "                # sum_embedding = tmp + embedded\n",
    "                vectorized_input[ele] = 1\n",
    "\n",
    "            # normalize_length = Variable(torch.LongTensor(len(idx_list)))\n",
    "            # if use_cuda:\n",
    "            #     normalize_length = normalize_length.cuda()\n",
    "            if use_average_embedding:\n",
    "                tmp = [1] * hidden_size\n",
    "                length = Variable(torch.FloatTensor(tmp))\n",
    "                if use_cuda:\n",
    "                    length = length.cuda()\n",
    "                # for idx in range(hidden_size):\n",
    "                real_ave = average_embedding.view(-1) / length\n",
    "                average_embedding = real_ave.view(1, 1, -1)\n",
    "\n",
    "            embedding = average_embedding\n",
    "        else:\n",
    "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
    "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            if use_linear_reduction == 1:\n",
    "                reduced_input = self.reduction(inputs)\n",
    "            else:\n",
    "                reduced_input = inputs\n",
    "\n",
    "            embedding = torch.unsqueeze(reduced_input, 0)\n",
    "\n",
    "        output, hidden = self.gru(embedding, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "class AttnDecoderRNN_new(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout_p=0.2, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN_new, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "            self.attn1 = nn.Linear(self.hidden_size + output_size, self.hidden_size)\n",
    "        else:\n",
    "            self.attn = nn.Linear(self.hidden_size + self.output_size, self.output_size)\n",
    "\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.attn_combine3 = nn.Linear(self.hidden_size * 2 + output_size, self.hidden_size)\n",
    "        else:\n",
    "            self.attn_combine = nn.Linear(self.hidden_size + self.output_size, self.hidden_size)\n",
    "        self.attn_combine5 = nn.Linear(self.output_size, self.output_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.reduction = nn.Linear(self.output_size, self.hidden_size)\n",
    "        if use_embedding or use_linear_reduction:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        else:\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, history_record, last_hidden):\n",
    "        if use_embedding:\n",
    "            list = Variable(torch.LongTensor(input).view(-1, 1))\n",
    "            if use_cuda:\n",
    "                list = list.cuda()\n",
    "            average_embedding = Variable(torch.zeros(hidden_size)).view(1, 1, -1)\n",
    "            if use_cuda:\n",
    "                average_embedding = average_embedding.cuda()\n",
    "\n",
    "            for ele in list:\n",
    "                embedded = self.embedding(ele).view(1, 1, -1)\n",
    "                tmp = average_embedding.clone()\n",
    "                average_embedding = tmp + embedded\n",
    "\n",
    "            if use_average_embedding:\n",
    "                tmp = [1] * hidden_size\n",
    "                length = Variable(torch.FloatTensor(tmp))\n",
    "                if use_cuda:\n",
    "                    length = length.cuda()\n",
    "                # for idx in range(hidden_size):\n",
    "                real_ave = average_embedding.view(-1) / length\n",
    "                average_embedding = real_ave.view(1, 1, -1)\n",
    "\n",
    "            embedding = average_embedding\n",
    "        else:\n",
    "            tensorized_input = torch.from_numpy(input).clone().type(torch.FloatTensor)\n",
    "            inputs = Variable(torch.unsqueeze(tensorized_input, 0).view(1, -1))\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            if use_linear_reduction == 1:\n",
    "                reduced_input = self.reduction(inputs)\n",
    "            else:\n",
    "                reduced_input = inputs\n",
    "\n",
    "            embedding = torch.unsqueeze(reduced_input, 0)\n",
    "\n",
    "        if use_dropout:\n",
    "            droped_ave_embedded = self.dropout(embedding)\n",
    "        else:\n",
    "            droped_ave_embedded = embedding\n",
    "\n",
    "        history_context = Variable(torch.FloatTensor(history_record).view(1, -1))\n",
    "        if use_cuda:\n",
    "            history_context = history_context.cuda()\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((droped_ave_embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        element_attn_weights = F.softmax(\n",
    "            self.attn1(torch.cat((history_context, hidden[0]), 1)), dim=1)\n",
    "\n",
    "        # attn_applied = torch.bmm(element_attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # attn_embedd = element_attn_weights * droped_ave_embedded[0]\n",
    "\n",
    "        output = torch.cat((droped_ave_embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # output = torch.cat((droped_ave_embedded[0], attn_applied[0], time_coef.unsqueeze(0)), 1)\n",
    "        # output = self.attn_combine3(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        linear_output = self.out(output[0])\n",
    "        # output_user_item = F.softmax(linear_output)\n",
    "\n",
    "        value = torch.sigmoid(self.attn_combine5(history_context).unsqueeze(0))\n",
    "\n",
    "        one_vec = Variable(torch.ones(self.output_size).view(1, -1))\n",
    "        if use_cuda:\n",
    "            one_vec = one_vec.cuda()\n",
    "\n",
    "        # ones_set = torch.index_select(value[0,0], 1, ones_idx_set[:, 1])\n",
    "        res = history_context.clone()\n",
    "        res[history_context != 0] = 1\n",
    "\n",
    "        output = F.softmax(linear_output * (one_vec - res * value[0]) + history_context * value[0], dim=1)\n",
    "\n",
    "        return output.view(1, -1), hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(num_layers, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "class custom_MultiLabelLoss_torch(nn.modules.loss._Loss):\n",
    "    def __init__(self):\n",
    "        super(custom_MultiLabelLoss_torch, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, weights):\n",
    "        mseloss = torch.sum(weights * torch.pow((pred - target), 2))\n",
    "        pred = pred.data\n",
    "        target = target.data\n",
    "        #\n",
    "        ones_idx_set = (target == 1).nonzero()\n",
    "        zeros_idx_set = (target == 0).nonzero()\n",
    "        # zeros_idx_set = (target == -1).nonzero()\n",
    "        \n",
    "        ones_set = torch.index_select(pred, 1, ones_idx_set[:, 1])\n",
    "        zeros_set = torch.index_select(pred, 1, zeros_idx_set[:, 1])\n",
    "        \n",
    "        repeat_ones = ones_set.repeat(1, zeros_set.shape[1])\n",
    "        repeat_zeros_set = torch.transpose(zeros_set.repeat(ones_set.shape[1], 1), 0, 1).clone()\n",
    "        repeat_zeros = repeat_zeros_set.contiguous().view(1, -1)\n",
    "        difference_val = -(repeat_ones - repeat_zeros)\n",
    "        exp_val = torch.exp(difference_val)\n",
    "        exp_loss = torch.sum(exp_val)\n",
    "        normalized_loss = exp_loss / (zeros_set.shape[1] * ones_set.shape[1])\n",
    "        set_loss = Variable(torch.FloatTensor([labmda * normalized_loss]), requires_grad=True)\n",
    "        if use_cuda:\n",
    "            set_loss = set_loss.cuda()\n",
    "        loss = mseloss + set_loss\n",
    "        #loss = mseloss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def generate_dictionary_BA(path, files, attributes_list):\n",
    "    # path = '../Minnemudac/'\n",
    "    # files = ['Coborn_history_order.csv','Coborn_future_order.csv']\n",
    "    # files = ['BA_history_order.csv', 'BA_future_order.csv']\n",
    "    # attributes_list = ['MATERIAL_NUMBER']\n",
    "    dictionary_table = {}\n",
    "    counter_table = {}\n",
    "    for attr in attributes_list:\n",
    "        dictionary = {}\n",
    "        dictionary_table[attr] = dictionary\n",
    "        counter_table[attr] = 0\n",
    "\n",
    "    #csv.field_size_limit(sys.maxsize)\n",
    "    csv.field_size_limit(2**31-1)\n",
    "    for filename in files:\n",
    "        count = 0\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                key = attributes_list[0]\n",
    "                if row[2] not in dictionary_table[key]:\n",
    "                    dictionary_table[key][row[2]] = counter_table[key]\n",
    "                    counter_table[key] = counter_table[key] + 1\n",
    "                    count += 1\n",
    "\n",
    "    print(counter_table)\n",
    "\n",
    "    total = 0\n",
    "    for key in counter_table.keys():\n",
    "        total = total + counter_table[key]\n",
    "\n",
    "    print('# dimensions of final vector: ' + str(total) + ' | ' + str(count - 1))\n",
    "\n",
    "    return dictionary_table, total, counter_table\n",
    "\n",
    "\n",
    "def read_claim2vector_embedding_file_no_vector(path, files):\n",
    "    # attributes_list = ['DRG', 'PROVCAT ', 'RVNU_CD', 'DIAG', 'PROC']\n",
    "    attributes_list = ['MATERIAL_NUMBER']\n",
    "    # path = '../Minnemudac/'\n",
    "    print('start dictionary generation...')\n",
    "    dictionary_table, num_dim, counter_table = generate_dictionary_BA(path, files, attributes_list)\n",
    "    print('finish dictionary generation*****')\n",
    "    usr_attr = 'CUSTOMER_ID'\n",
    "    ord_attr = 'ORDER_NUMBER'\n",
    "\n",
    "    # dictionary_table, num_dim, counter_table = GDF.generate_dictionary(attributes_list)\n",
    "\n",
    "    freq_max = 200\n",
    "    ## all the follow three ways array. First index is patient, second index is the time step, third is the feature vector\n",
    "    data_chunk = []\n",
    "    day_gap_counter = []\n",
    "    claims_counter = 0\n",
    "    num_claim = 0\n",
    "    code_freq_at_first_claim = np.zeros(num_dim + 2)\n",
    "\n",
    "    for file_id in range(len(files)):\n",
    "\n",
    "        count = 0\n",
    "        data_chunk.append({})\n",
    "        filename = files[file_id]\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            # gap_within_one_year = np.zeros(365)\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            last_pid_date = '*'\n",
    "            last_pid = '-1'\n",
    "            last_days = -1\n",
    "            # 2 more elements in the end for start and end states\n",
    "            feature_vector = []\n",
    "            for row in reader:\n",
    "                cur_pid_date = row[usr_attr] + '_' + row[ord_attr]\n",
    "                cur_pid = row[usr_attr]\n",
    "                # cur_days = int(row[ord_attr])\n",
    "\n",
    "                if cur_pid != last_pid:\n",
    "                    # start state\n",
    "                    tmp = [-1]\n",
    "                    data_chunk[file_id][cur_pid] = []\n",
    "                    data_chunk[file_id][cur_pid].append(tmp)\n",
    "                    num_claim = 0\n",
    "\n",
    "                if cur_pid_date not in last_pid_date:\n",
    "                    if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                        sorted_feature_vector = np.sort(feature_vector)\n",
    "                        data_chunk[file_id][last_pid].append(sorted_feature_vector)\n",
    "                        if len(sorted_feature_vector) > 0:\n",
    "                            count = count + 1\n",
    "                        # data_chunk[file_id][last_pid].append(feature_vector)\n",
    "                    feature_vector = []\n",
    "\n",
    "                    claims_counter = 0\n",
    "                if cur_pid != last_pid:\n",
    "                    # end state\n",
    "                    if last_pid not in '-1':\n",
    "                        tmp = [-1]\n",
    "                        data_chunk[file_id][last_pid].append(tmp)\n",
    "\n",
    "                key = attributes_list[0]\n",
    "\n",
    "                within_idx = dictionary_table[key][row[key]]\n",
    "                previous_idx = 0\n",
    "\n",
    "                for j in range(attributes_list.index(key)):\n",
    "                    previous_idx = previous_idx + counter_table[attributes_list[j]]\n",
    "                idx = within_idx + previous_idx\n",
    "\n",
    "                # set corresponding dimention to 1\n",
    "                if idx not in feature_vector:\n",
    "                    feature_vector.append(idx)\n",
    "\n",
    "                last_pid_date = cur_pid_date\n",
    "                last_pid = cur_pid\n",
    "                # last_days = cur_days\n",
    "                if file_id == 1:\n",
    "                    claims_counter = claims_counter + 1\n",
    "\n",
    "            if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                data_chunk[file_id][last_pid].append(np.sort(feature_vector))\n",
    "        print('num of vectors having entries more than 1: ' + str(count))\n",
    "\n",
    "    return data_chunk, num_dim + 2, code_freq_at_first_claim\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, codes_inverse_freq, encoder_optimizer, decoder_optimizer,\n",
    "          criterion, output_size, next_k_step, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(input_variable)\n",
    "    target_length = len(target_variable)\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    history_record = np.zeros(output_size)\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        for ele in input_variable[ei]:\n",
    "            history_record[ele] += 1 / (input_length - 2)\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
    "\n",
    "    last_input = input_variable[input_length - 2]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    last_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    num_str = 0\n",
    "    topk = 1\n",
    "    max_len = 5\n",
    "\n",
    "    if next_k_step > 0:\n",
    "        if next_k_step <= target_length - 2:\n",
    "            max_step = next_k_step\n",
    "        else:\n",
    "            max_step = target_length - 2\n",
    "    else:\n",
    "        max_step = target_length - 1\n",
    "        max_step = min(target_length - 2, max_len)\n",
    "    decoder_input = last_input\n",
    "\n",
    "    for di in range(max_step):\n",
    "\n",
    "        if atten_decoder:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(topk)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        # activation_bound\n",
    "        # topk_labels\n",
    "        # target_neg = zero2neg(target_variable[di])\n",
    "\n",
    "        vectorized_target = np.zeros(output_size)\n",
    "        for idx in target_variable[di + 1]:\n",
    "            vectorized_target[idx] = 1\n",
    "\n",
    "        target = Variable(torch.FloatTensor(vectorized_target).view(1, -1))\n",
    "        if use_cuda:\n",
    "            target = target.cuda()\n",
    "        weights = Variable(torch.FloatTensor(codes_inverse_freq).view(1, -1))\n",
    "        if use_cuda:\n",
    "            weights = weights.cuda()\n",
    "\n",
    "        tt = criterion(decoder_output, target, weights)\n",
    "        # tt = torch.sum(weights*torch.pow((decoder_output - target),2))\n",
    "        loss += tt\n",
    "\n",
    "        decoder_input = target_variable[di + 1]\n",
    "        # loss += multilable_loss(decoder_output, target)\n",
    "\n",
    "    # encoder_optimizer.zero_grad()\n",
    "    # decoder_optimizer.zero_grad()\n",
    "    loss = torch.tensor(0.0, requires_grad=True) if not loss else loss\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "#\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def trainIters(data_chunk, output_size, encoder, decoder, model_id, training_key_set, codes_inverse_freq, next_k_step,\n",
    "               n_iters, print_every=300):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    encoder_pathes = []\n",
    "    decoder_pathes = []\n",
    "    # elem_wise_connection.initWeight()\n",
    "\n",
    "    # sum_history = add_history(data_chunk[past_chunk],training_key_set,output_size)\n",
    "    # KNN_history = KNN_history_record1(sum_history, output_size, num_nearest_neighbors)\n",
    "    KNN_history = []\n",
    "\n",
    "    if optimizer_option == 1:\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    elif optimizer_option == 2:\n",
    "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-09, weight_decay=0)\n",
    "        # encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.88, 0.95), eps=1e-08, weight_decay=0)\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
    "                                             weight_decay=0)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-11,\n",
    "                                             weight_decay=0)\n",
    "    elif optimizer_option == 3:\n",
    "        encoder_optimizer = torch.optim.RMSprop(encoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
    "                                                weight_decay=0, momentum=0, centered=False)\n",
    "        decoder_optimizer = torch.optim.RMSprop(decoder.parameters(), lr=learning_rate, alpha=0.99, eps=1e-08,\n",
    "                                                weight_decay=0, momentum=0, centered=False)\n",
    "    elif optimizer_option == 4:\n",
    "        encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
    "                                                 weight_decay=0)\n",
    "        decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate, rho=0.9, eps=1e-06,\n",
    "                                                 weight_decay=0)\n",
    "\n",
    "    # training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "    #                  for i in range(n_iters)]\n",
    "    # criterion = nn.NLLLoss()\n",
    "    total_iter = 0\n",
    "    criterion = custom_MultiLabelLoss_torch()\n",
    "    for j in range(n_iters):\n",
    "        key_idx = np.random.permutation(len(training_key_set))\n",
    "        # key_idx = np.random.choice(len(training_key_set),n_iters)\n",
    "        training_keys = []\n",
    "\n",
    "        for idx in key_idx:\n",
    "            training_keys.append(training_key_set[idx])\n",
    "\n",
    "            # criterion = custom_MultiLabelLoss_MSE()\n",
    "        # criterion = nn.MultiLabelSoftMarginLoss(size_average=False)\n",
    "        # criterion = nn.BCELoss()\n",
    "        weight_vector = []\n",
    "\n",
    "        for iter in range(1, len(training_key_set) + 1):\n",
    "            # training_pair = training_pairs[iter - 1]\n",
    "            # input_variable = training_pair[0]\n",
    "            # target_variable = training_pair[1]\n",
    "            input_variable = data_chunk[past_chunk][training_keys[iter - 1]]\n",
    "            target_variable = data_chunk[future_chunk][training_keys[iter - 1]]\n",
    "\n",
    "            loss = train(input_variable, target_variable, encoder,\n",
    "                         decoder, codes_inverse_freq, encoder_optimizer, decoder_optimizer, criterion, output_size,\n",
    "                         next_k_step)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            total_iter += 1\n",
    "\n",
    "        print_loss_avg = print_loss_total / len(training_key_set)\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.6f' % (timeSince(start, total_iter / (n_iters * len(training_key_set))), total_iter, total_iter / (n_iters * len(training_key_set)) * 100,print_loss_avg))\n",
    "\n",
    "        filepath = './models/encoder' + (model_id) + '_model_epoch' + str(int(j))\n",
    "        encoder_pathes.append(filepath)\n",
    "        torch.save(encoder, filepath)\n",
    "        filepath = './models/decoder' + (model_id) + '_model_epoch' + str(int(j))\n",
    "        decoder_pathes.append(filepath)\n",
    "        torch.save(decoder, filepath)\n",
    "        print('Finish epoch: ' + str(j))\n",
    "        print('Model is saved.')\n",
    "        sys.stdout.flush()\n",
    "    # showPlot(plot_losses)\n",
    "    # print('The loss: ' + str(print_loss_total))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Plotting results\n",
    "# ----------------\n",
    "#\n",
    "# Plotting is done with matplotlib, using the array of loss values\n",
    "# ``plot_losses`` saved while training.\n",
    "#\n",
    "\n",
    "cosine_sim = []\n",
    "pair_cosine_sim = []\n",
    "\n",
    "\n",
    "def decoding_next_k_step(encoder, decoder, input_variable, target_variable, output_size, k, activate_codes_num):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = len(input_variable)\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    history_record = np.zeros(output_size)\n",
    "    count = 0\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        for ele in input_variable[ei]:\n",
    "            history_record[ele] += 1\n",
    "        count += 1\n",
    "\n",
    "    history_record = history_record / count\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        if ei == 0:\n",
    "            continue\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei - 1] = encoder_output[0][0]\n",
    "\n",
    "        for ii in range(k):\n",
    "            vectorized_target = np.zeros(output_size)\n",
    "            for idx in target_variable[ii + 1]:\n",
    "                vectorized_target[idx] = 1\n",
    "\n",
    "            vectorized_input = np.zeros(output_size)\n",
    "            for idx in input_variable[ei]:\n",
    "                vectorized_input[idx] = 1\n",
    "\n",
    "    decoder_input = input_variable[input_length - 2]\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    last_hidden = decoder_hidden\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    num_str = 0\n",
    "    topk = 400\n",
    "    decoded_vectors = []\n",
    "    prob_vectors = []\n",
    "    cout = 0\n",
    "    for di in range(k):\n",
    "        if atten_decoder:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, history_record, last_hidden)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(topk)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        vectorized_target = np.zeros(output_size)\n",
    "        for idx in target_variable[di + 1]:\n",
    "            vectorized_target[idx] = 1\n",
    "\n",
    "        # target_topi = vectorized_target.argsort()[::-1][:topk]\n",
    "        # activation_bound\n",
    "\n",
    "        count = 0\n",
    "        start_idx = -1\n",
    "        end_idx = output_size\n",
    "        if activate_codes_num > 0:\n",
    "            pick_num = activate_codes_num\n",
    "        else:\n",
    "            pick_num = np.sum(vectorized_target)\n",
    "            # print(pick_num)\n",
    "\n",
    "        tmp = []\n",
    "        for ele in range(len(topi[0])):\n",
    "            if count >= pick_num:\n",
    "                break\n",
    "            tmp.append(topi[0][ele])\n",
    "            count += 1\n",
    "\n",
    "        decoded_vectors.append(tmp)\n",
    "        decoder_input = tmp\n",
    "        tmp = []\n",
    "\n",
    "        for i in range(topk):\n",
    "            tmp.append(topi[0][i])\n",
    "        prob_vectors.append(tmp)\n",
    "\n",
    "    return decoded_vectors, prob_vectors\n",
    "\n",
    "import bottleneck as bn\n",
    "\n",
    "\n",
    "def top_n_indexes(arr, n):\n",
    "    idx = bn.argpartition(arr, arr.size - n, axis=None)[-n:]\n",
    "    width = arr.shape[1]\n",
    "    return [divmod(i, width) for i in idx]\n",
    "\n",
    "\n",
    "def get_precision_recall_Fscore(groundtruth, pred):\n",
    "    a = groundtruth\n",
    "    b = pred\n",
    "    correct = 0\n",
    "    truth = 0\n",
    "    positive = 0\n",
    "\n",
    "    for idx in range(len(a)):\n",
    "        if a[idx] == 1:\n",
    "            truth += 1\n",
    "            if b[idx] == 1:\n",
    "                correct += 1\n",
    "        if b[idx] == 1:\n",
    "            positive += 1\n",
    "\n",
    "    flag = 0\n",
    "    if 0 == positive:\n",
    "        precision = 0\n",
    "        flag = 1\n",
    "        # print('postivie is 0')\n",
    "    else:\n",
    "        precision = correct / positive\n",
    "    if 0 == truth:\n",
    "        recall = 0\n",
    "        flag = 1\n",
    "        # print('recall is 0')\n",
    "    else:\n",
    "        recall = correct / truth\n",
    "\n",
    "    if flag == 0 and precision + recall > 0:\n",
    "        F = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        F = 0\n",
    "    return precision, recall, F, correct\n",
    "\n",
    "\n",
    "def get_F_score(prediction, test_Y):\n",
    "    jaccard_similarity = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "\n",
    "    count = 0\n",
    "    for idx in range(len(test_Y)):\n",
    "        pred = prediction[idx]\n",
    "        T = 0\n",
    "        P = 0\n",
    "        correct = 0\n",
    "        for id in range(len(pred)):\n",
    "            if test_Y[idx][id] == 1:\n",
    "                T = T + 1\n",
    "                if pred[id] == 1:\n",
    "                    correct = correct + 1\n",
    "            if pred[id] == 1:\n",
    "                P = P + 1\n",
    "\n",
    "        if P == 0 or T == 0:\n",
    "            continue\n",
    "        precision = correct / P\n",
    "        recall = correct / T\n",
    "        prec.append(precision)\n",
    "        rec.append(recall)\n",
    "        if correct == 0:\n",
    "            jaccard_similarity.append(0)\n",
    "        else:\n",
    "            jaccard_similarity.append(2 * precision * recall / (precision + recall))\n",
    "        count = count + 1\n",
    "\n",
    "    print(\n",
    "        'average precision: ' + str(np.mean(prec)))\n",
    "    print('average recall : ' + str(\n",
    "        np.mean(rec)))\n",
    "    print('average F score: ' + str(\n",
    "        np.mean(jaccard_similarity)))\n",
    "\n",
    "\n",
    "def get_DCG(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1) / math.log2(count + 1 + 1)\n",
    "        count += 1\n",
    "\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def get_NDCG(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1) / math.log2(count + 1 + 1)\n",
    "        count += 1\n",
    "    idcg = 0\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "#     num_item = int(min(num_real_item, k))\n",
    "    for i in range(num_item):\n",
    "        idcg += (1) / math.log2(i + 1 + 1)\n",
    "    ndcg = (dcg / idcg) * (num_item / (num_item + abs(num_item - k)))\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def get_HT(groundtruth, pred_rank_list, k):\n",
    "    count = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            return 1\n",
    "        count += 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_MAE(groundtruth, k):\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "    mae = abs(num_item - k)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def evaluate(data_chunk, encoder, decoder, output_size, test_key_set, next_k_step, activate_codes_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    F = []\n",
    "    prec1 = []\n",
    "    rec1 = []\n",
    "    F1 = []\n",
    "    prec2 = []\n",
    "    rec2 = []\n",
    "    F2 = []\n",
    "    prec3 = []\n",
    "    rec3 = []\n",
    "    F3 = []\n",
    "    length = np.zeros(3)\n",
    "\n",
    "    NDCG = []\n",
    "    MAE = []\n",
    "    n_hit = 0\n",
    "    count = 0\n",
    "\n",
    "    for iter in range(len(test_key_set)):\n",
    "        input_variable = data_chunk[past_chunk][test_key_set[iter]]\n",
    "        target_variable = data_chunk[future_chunk][test_key_set[iter]]\n",
    "\n",
    "        if len(target_variable) < 2 + next_k_step:\n",
    "            continue\n",
    "        count += 1\n",
    "        output_vectors, prob_vectors = decoding_next_k_step(encoder, decoder, input_variable, target_variable,\n",
    "                                                            output_size, next_k_step, activate_codes_num)\n",
    "\n",
    "        hit = 0\n",
    "        for idx in range(len(output_vectors)):\n",
    "            # for idx in [2]:\n",
    "            vectorized_target = np.zeros(output_size)\n",
    "            for ii in target_variable[1 + idx]:\n",
    "                vectorized_target[ii] = 1\n",
    "\n",
    "            vectorized_output = np.zeros(output_size)\n",
    "            for ii in output_vectors[idx]:\n",
    "                vectorized_output[ii] = 1\n",
    "\n",
    "            precision, recall, Fscore, correct = get_precision_recall_Fscore(vectorized_target, vectorized_output)\n",
    "            prec.append(precision)\n",
    "            rec.append(recall)\n",
    "            F.append(Fscore)\n",
    "            length[idx] += np.sum(target_variable[1 + idx])\n",
    "            target_topi = prob_vectors[idx]\n",
    "            ndcg = get_NDCG(vectorized_target, target_topi, activate_codes_num)\n",
    "            NDCG.append(ndcg)\n",
    "            mae = get_MAE(vectorized_target, activate_codes_num)\n",
    "            MAE.append(mae)\n",
    "            \n",
    "    print('average Recall: ' + str(np.mean(rec)))\n",
    "    print('average Precision: ' + str(np.mean(prec)))\n",
    "    print('average F score: ' + str(np.mean(F)))\n",
    "    print('average NDCG: ' + str(np.mean(NDCG)))\n",
    "    # print('average hit rate: ' + str(n_hit / len(test_key_set)))\n",
    "    return np.mean(rec), np.mean(prec), np.mean(F), np.mean(NDCG), np.mean(MAE)\n",
    "\n",
    "def split_data(full_list, ratio, shuffle=False):\n",
    "    n_total = len(full_list)\n",
    "    offset = int(n_total * ratio)\n",
    "    if n_total==0 or offset<1:\n",
    "        return [],full_list\n",
    "    if shuffle:\n",
    "        random.shuffle(full_list)\n",
    "    train = full_list[:offset]\n",
    "    test = full_list[offset:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def partition_the_data(data_chunk, key_set, next_k_step):\n",
    "    filtered_key_set = []\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[past_chunk][key]) <= 3:\n",
    "            continue\n",
    "        if len(data_chunk[future_chunk][key]) < 2 + next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set))]\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "    return training_key_set, test_key_set\n",
    "\n",
    "def partition_the_data_validate(data_chunk, key_set, next_k_step):\n",
    "    training_key_set, test_set = split_data(key_set, 0.8)\n",
    "    validation_key_set, test_key_set = split_data(test_set, 0.5)\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    print('Number of valid instances: ' + str(len(validation_key_set)))\n",
    "    print('Number of test instances: ' + str(len(test_key_set)))\n",
    "    return training_key_set, validation_key_set, test_key_set\n",
    "\n",
    "def get_codes_frequency_no_vector(X, num_dim, key_set):\n",
    "    result_vector = np.zeros(num_dim)\n",
    "    for pid in key_set:\n",
    "        for idx in X[pid]:\n",
    "            result_vector[idx] += 1\n",
    "    return result_vector\n",
    "\n",
    "\n",
    "# The first two parameters are the past records and future records, respectively.\n",
    "# The main function consists of two model which is decisded by the argv[5]. If training is 1, it is training mode. If\n",
    "# training is 0, it is test mode. model_version is the name of the model. next_k_step is the number of steps we predict.\n",
    "# model_epoch is the model generated by the model_epoch-th epoch.\n",
    "def main(argv):\n",
    "    argv = ['Sets2Sets.py', f'./data/{DATASET_NAME}_history.csv', f'./data/{DATASET_NAME}_future.csv', f'{DATASET_NAME}', 1, 1] \n",
    "\n",
    "    files = [argv[1],argv[2]]\n",
    "\n",
    "    model_version = argv[3]\n",
    "\n",
    "    next_k_step = int(argv[4])\n",
    "    training = int(argv[5])\n",
    "    path = './'\n",
    "    directory = './models/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    data_chunk, input_size, code_freq_at_first_claim = read_claim2vector_embedding_file_no_vector(path, files)\n",
    "    print(\"data_chunk;)\", type(data_chunk))\n",
    "    codes_freq = get_codes_frequency_no_vector(data_chunk[past_chunk], input_size, data_chunk[future_chunk].keys())\n",
    "    training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[future_chunk]), next_k_step)\n",
    "\n",
    "    weights = np.zeros(input_size)\n",
    "    max_freq = max(codes_freq)\n",
    "    for idx in range(len(codes_freq)):\n",
    "        if codes_freq[idx] > 0:\n",
    "            weights[idx] = max_freq / codes_freq[idx]\n",
    "        else:\n",
    "            weights[idx] = 0\n",
    "\n",
    "    encoder1 = EncoderRNN_new(input_size, hidden_size, num_layers)\n",
    "    attn_decoder1 = AttnDecoderRNN_new(hidden_size, input_size, num_layers, dropout_p=0.1)\n",
    "\n",
    "    if use_cuda:\n",
    "        encoder1 = encoder1.cuda()\n",
    "        attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "    if training == 1:\n",
    "        if atten_decoder:\n",
    "            trainIters(data_chunk, input_size, encoder1, attn_decoder1, model_version, training_key_set, weights,\n",
    "                       next_k_step, num_iter, print_every=print_val)\n",
    "\n",
    "    else:\n",
    "        result = \"./result/\"\n",
    "        if not os.path.exists(result):\n",
    "            os.mkdir(result)\n",
    "        header = [\"EPOCH\", \"Recall@5\", \"Recall@10\", \"Recall@20\", \"Recall@40\", \"Precision@5\", \"Precision@10\", \"Precision@20\", \"Precision@40\",\\\n",
    "                 \"F1-score@5\", \"F1-score@10\", \"F1-score@20\", \"F1-score@40\", \"NDCG@5\", \"NDCG@10\", \"NDCG@20\", \"NDCG@40\", \"MAE@5\", \"MAE@10\", \"MAE@20\", \"MAE@40\"]\n",
    "        with open(result+f'{model_version}.csv', 'w', newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "            for model_epoch in range(num_iter):\n",
    "                print('Epoch: ', model_epoch+1)\n",
    "                encoder_pathes = './models/encoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
    "                decoder_pathes = './models/decoder' + str(model_version) + '_model_epoch' + str(model_epoch)\n",
    "                encoder_instance = torch.load(encoder_pathes)\n",
    "                decoder_instance = torch.load(decoder_pathes)\n",
    "                valid_recall = []\n",
    "                valid_precision = []\n",
    "                valid_f1 = []\n",
    "                valid_ndcg = []\n",
    "                valid_mae = []\n",
    "                recall_list = []\n",
    "                precision_list = []\n",
    "                f1_list = []\n",
    "                ndcg_list = []\n",
    "                mae_list = []\n",
    "\n",
    "                for i in [5, 10, 20, 40]:\n",
    "                    print('k = ' + str(i))\n",
    "\n",
    "                    # validation\n",
    "                    recall, precision, f1, ndcg, mae = evaluate(data_chunk, encoder_instance, decoder_instance, input_size, validation_key_set, next_k_step, i)\n",
    "                    valid_recall.append(recall)\n",
    "                    valid_precision.append(precision)\n",
    "                    valid_f1.append(f1)\n",
    "                    valid_ndcg.append(ndcg)\n",
    "                    valid_mae.append(mae)\n",
    "\n",
    "                    # test\n",
    "                    recall, precision, f1, ndcg, mae  = evaluate(data_chunk, encoder_instance, decoder_instance, input_size, test_key_set, next_k_step, i)\n",
    "                    recall_list.append(recall)\n",
    "                    precision_list.append(precision)\n",
    "                    f1_list.append(f1)\n",
    "                    ndcg_list.append(ndcg)\n",
    "                    mae_list.append(mae)\n",
    "                \n",
    "                row = []\n",
    "                row.append(model_epoch)\n",
    "                for recall in recall_list:\n",
    "                    row.append(recall)\n",
    "                for precision in precision_list:\n",
    "                    row.append(precision)\n",
    "                for f1 in f1_list:\n",
    "                    row.append(f1)\n",
    "                for ndcg in ndcg_list:\n",
    "                    row.append(ndcg)\n",
    "                for mae in mae_list:\n",
    "                    row.append(mae)\n",
    "                writer.writerow(row)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d1f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
